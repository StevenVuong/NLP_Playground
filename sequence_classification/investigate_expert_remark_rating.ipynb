{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17cf774-2fda-4bdc-a78b-c4859786fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import csv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed5d4d8-7c2b-4827-b345-743240614515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display all pandas columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a567397-8551-4e50-9137-a45cc3284175",
   "metadata": {},
   "source": [
    "### Open and Examine Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045311d5-569f-4cc2-891e-0ba045c2f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = './input_ai_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e370ceae-be43-4add-9143-b996eaee23a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_input_csv_to_dataframe(input_csv_path: str) -> pd.DataFrame:\n",
    "    # load all lines into list\n",
    "    all_lines = []\n",
    "\n",
    "    with open(input_csv_path, \"r\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        for line in tqdm(reader):\n",
    "            all_lines.append(line[:30])\n",
    "\n",
    "    # make into dataframe\n",
    "    df = pd.DataFrame(all_lines)\n",
    "    df.columns = df.iloc[0]\n",
    "    df = df[1:]\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = load_input_csv_to_dataframe(input_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d1702-97b7-478e-93bb-4698fedb0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns relevent to remark rating\n",
    "df = df[['Category ID', 'Player Level', 'User Note', 'Occurrence Remarks', 'Expert Remark Rating', 'Expert Level',]]\n",
    "df = df.drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236b496d-c0ef-4a8b-8d56-f1165070f02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all before taxon tree (what was added post processing)\n",
    "df['User Note'] = df['User Note'].apply(lambda x: x.split('Taxon Tree')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edecdf7-2931-406a-867c-cb1c52391afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process user note; join together all users' comments\n",
    "df['Joint User Note'] = df['User Note'] + df['Occurrence Remarks']\n",
    "df = df.drop(['User Note', 'Occurrence Remarks'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11542ed-6875-4911-aebb-0d486848df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all are valid\n",
    "df = df[df['Joint User Note'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737262b4-1cb5-4216-afd3-a1529fda7942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where nan expert remark rating\n",
    "df = df[df['Expert Remark Rating'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d35dcf-1e72-4614-88af-c72feffbf7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Expert Remark Rating', data=df)\n",
    "plt.title('Expert Remark Distribution');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e661b382-c6c6-4c61-b462-b77b4c558115",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b99dfcb-1f90-4a30-9387-33ea208c23a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn expert remark rating into an int\n",
    "df['Expert Remark Rating'] = df['Expert Remark Rating'].apply(lambda x: int(float(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c664ca0a-4a69-40d8-b12a-b58b1e23233f",
   "metadata": {},
   "source": [
    "### Preprocess text\n",
    "https://machinelearningknowledge.ai/11-techniques-of-text-preprocessing-using-nltk-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa2c25-6f48-48b2-aeec-bf6c567cf588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336abd85-e448-44ec-b66f-f933bea484a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376fef8-e545-410e-bb5a-d954b7d2e176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def remove_whitespace(text):\n",
    "    return  \" \".join(text.split())\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    result = [token for token in text if token not in en_stopwords]\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    lst=tokenizer.tokenize(' '.join(text))\n",
    "    return lst\n",
    "\n",
    "\n",
    "def lemmatization(text):\n",
    "    result=[]\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for token,tag in pos_tag(text):\n",
    "        pos=tag[0].lower()\n",
    "        \n",
    "        if pos not in ['a', 'r', 'n', 'v']:\n",
    "            pos='n'\n",
    "            \n",
    "        result.append(wordnet.lemmatize(token,pos))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_tag(text):\n",
    "    text=' '.join(text)\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    return html_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1290ba42-cf11-4ccc-9de6-74293e892010",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = df.sample(10_000).copy() # take subset of 10000 first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39c7aa-b2c0-4c90-bd0f-2b2fd511bf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "print('Removing Whitespace')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(remove_whitespace)\n",
    "\n",
    "print('Tokenizing words')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(word_tokenize) \n",
    "\n",
    "print('Removing stopwords')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(lambda x: remove_stopwords(x)) \n",
    "\n",
    "print('Removing punctuation')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(lambda x: \" \".join(remove_punct(x)))\n",
    "\n",
    "print('Removing URLs')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(remove_urls) \n",
    "\n",
    "print('Lemmatizing')\n",
    "subset_df['Joint User Note'] = subset_df['Joint User Note'].progress_apply(lambda x: \" \".join(lemmatization(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4255c7e8-b119-4f79-aefb-3c1863282bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save text file\n",
    "with open('./remarks.txt', 'w+') as wf:\n",
    "    for el, jun in subset_df[['Expert Remark Rating', 'Joint User Note']].values:\n",
    "        string = f\"__label__{el} {jun}\"\n",
    "        wf.write(string)\n",
    "        wf.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde9b060-b292-4298-ad7a-60a0f73e54a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for total of 10000 data points\n",
    "!head -n 9000 remarks.txt > remarks.train\n",
    "!tail -n 1000 remarks.txt > remarks.valid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0327ca-b4ce-4e3d-8ac0-a4fd3bd30bfb",
   "metadata": {},
   "source": [
    "### Train Using FastText\n",
    "https://fasttext.cc/docs/en/supervised-tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663a7814-a336-4138-bb3c-bbf017cf0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b0c9f-ab62-49e8-96b5-f3aea8faf847",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"remarks.train\", lr=1, epoch=100, wordNgrams=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a015eef4-787e-4ef9-a896-01dc3d538e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_model(\"model_remarks.bin\") #load_model does the inverse\n",
    "\n",
    "# model.test(\"remarks.valid\") # precision, recall at 1; how often highest ranked document contains correct answer\n",
    "model.test(\"remarks.valid\", k=5) # precision, recall at 5. want > 0.2 precision, otherwise no better than random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6291f3-38f2-4ca2-bc41-621bc85b1179",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(\"what an amazing kangaroo\", k=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84dde389-8449-49f2-a7b6-02204147a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.test(\"remarks.valid\", k=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7981d13-6d54-4ed8-a653-d8c471821cb5",
   "metadata": {},
   "source": [
    "### Results are not that great; let's try BERT\n",
    "https://colab.research.google.com/github/rap12391/transformers_multilabel_toxic/blob/master/toxic_multilabel.ipynb#scrollTo=0DF3ddjej5vd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1c210-eacd-482d-b31b-49993bded6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "import pickle\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8df4aa-a8c1-4beb-b860-725730a1ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0f50ee-a174-4054-8305-8155187bc2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b7070f-92d1-4482-8bb1-7e6284b6110d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a9b75-9070-4258-838c-b481c6256b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(subset_df['Expert Remark Rating'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe773424-3579-4766-8838-89c63ca83c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of labels and comments\n",
    "labels = list(enc.transform(subset_df['Expert Remark Rating'].values.reshape(-1,1)).toarray())\n",
    "comments = list(subset_df['Joint User Note'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cee246-62ed-4f2b-aa93-cecd3c97b152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # tokenizer\n",
    "encodings = tokenizer.batch_encode_plus(comments,max_length=max_length,pad_to_max_length=True) # tokenizer's encoding method\n",
    "print('tokenizer outputs: ', encodings.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88850e45-ddd1-46f2-a3c3-d6383ba94b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = encodings['input_ids'] # tokenized and encoded sentences\n",
    "token_type_ids = encodings['token_type_ids'] # token type ids\n",
    "attention_masks = encodings['attention_mask'] # attention masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f858cf7e-ddff-49c1-854a-041f9af711cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_token_types, validation_token_types, train_masks, validation_masks = train_test_split(input_ids, labels, token_type_ids,attention_masks,\n",
    "                                                            random_state=2020, test_size=0.10, stratify = labels)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "train_token_types = torch.tensor(train_token_types)\n",
    "\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "validation_token_types = torch.tensor(validation_token_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a1ad83-9572-4f92-8813-65cafe5da4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
    "batch_size = 32\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels, train_token_types)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels, validation_token_types)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37eedfa-d67f-4ab3-8311-9cb3fd7c7053",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# Load model, the pretrained model will include a single linear classification layer on top for classification. \n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=6)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe1fb25-9d44-4bc9-9e8f-f3c0306f6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting custom optimization parameters. You may implement a scheduler here as well.\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f430df-8f89-4fa4-af14-07253bf55161",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(optimizer_grouped_parameters,lr=2e-5,correct_bias=True)\n",
    "# optimizer = AdamW(model.parameters(),lr=2e-5)  # Default optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac690c8-df57-47ea-8878-76ff94852a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels=6\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs (authors recommend between 2 and 4)\n",
    "epochs = 3\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "  # Training\n",
    "  \n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss = 0 #running loss\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # # Forward pass for multiclass classification\n",
    "    # outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "    # loss = outputs[0]\n",
    "    # logits = outputs[1]\n",
    "\n",
    "    # Forward pass for multilabel classification\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    logits = outputs[0]\n",
    "    loss_func = BCEWithLogitsLoss() \n",
    "    loss = loss_func(\n",
    "        logits.view(-1,num_labels),\n",
    "        b_labels.type_as(logits).view(-1,num_labels)\n",
    "    ) #convert labels to float for calculation\n",
    "    # loss_func = BCELoss() \n",
    "    # loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "    train_loss_set.append(loss.item())    \n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "    # scheduler.step()\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Variables to gather full output\n",
    "  logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "  # Predict\n",
    "  for i, batch in enumerate(validation_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "    with torch.no_grad():\n",
    "      # Forward pass\n",
    "      outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "      b_logit_pred = outs[0]\n",
    "      pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "      b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "      pred_label = pred_label.to('cpu').numpy()\n",
    "      b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tokenized_texts.append(b_input_ids)\n",
    "    logit_preds.append(b_logit_pred)\n",
    "    true_labels.append(b_labels)\n",
    "    pred_labels.append(pred_label)\n",
    "\n",
    "  # Flatten outputs\n",
    "  pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "  true_labels = [item for sublist in true_labels for item in sublist]\n",
    "\n",
    "  # Calculate Accuracy\n",
    "  threshold = 0.50\n",
    "  pred_bools = [pl>threshold for pl in pred_labels]\n",
    "  true_bools = [tl==1 for tl in true_labels]\n",
    "  val_f1_accuracy = f1_score(true_bools,pred_bools,average='micro')*100\n",
    "  val_flat_accuracy = accuracy_score(true_bools, pred_bools)*100\n",
    "\n",
    "  print('F1 Validation Accuracy: ', val_f1_accuracy)\n",
    "  print('Flat Validation Accuracy: ', val_flat_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95ffa8b-109d-48bd-a571-e877ea09d55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df.sample(10_000).copy() # take subset of 10000 to test\n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(remove_whitespace)\n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(word_tokenize) \n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(lambda x: remove_stopwords(x)) \n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(lambda x: \" \".join(remove_punct(x)))\n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(remove_urls) \n",
    "test_df['Joint User Note'] = test_df['Joint User Note'].progress_apply(lambda x: \" \".join(lemmatization(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a8712-7866-49f6-868d-a004e96ba586",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = list(enc.transform(test_df['Expert Remark Rating'].values.reshape(-1,1)).toarray())\n",
    "test_comments = list(test_df['Joint User Note'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa7e158-2e13-4cfd-8c64-059c64dc727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding input data\n",
    "test_encodings = tokenizer.batch_encode_plus(test_comments,max_length=max_length,pad_to_max_length=True)\n",
    "test_input_ids = test_encodings['input_ids']\n",
    "test_token_type_ids = test_encodings['token_type_ids']\n",
    "test_attention_masks = test_encodings['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0c7c43-f65e-4950-a79e-ad14f2034425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make tensors out of data\n",
    "test_inputs = torch.tensor(test_input_ids)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "test_masks = torch.tensor(test_attention_masks)\n",
    "test_token_types = torch.tensor(test_token_type_ids)\n",
    "# Create test dataloader\n",
    "test_data = TensorDataset(test_inputs, test_masks, test_labels, test_token_types)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "# Save test dataloader\n",
    "#torch.save(test_dataloader,'test_data_loader')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89fa7f-796c-40d4-bd4c-8d3fa7fbb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "# Put model in evaluation mode to evaluate loss on the validation set\n",
    "model.eval()\n",
    "\n",
    "#track variables\n",
    "logit_preds,true_labels,pred_labels,tokenized_texts = [],[],[],[]\n",
    "\n",
    "# Predict\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "  batch = tuple(t.to(device) for t in batch)\n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels, b_token_types = batch\n",
    "  with torch.no_grad():\n",
    "    # Forward pass\n",
    "    outs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "    b_logit_pred = outs[0]\n",
    "    pred_label = torch.sigmoid(b_logit_pred)\n",
    "\n",
    "    b_logit_pred = b_logit_pred.detach().cpu().numpy()\n",
    "    pred_label = pred_label.to('cpu').numpy()\n",
    "    b_labels = b_labels.to('cpu').numpy()\n",
    "\n",
    "  tokenized_texts.append(b_input_ids)\n",
    "  logit_preds.append(b_logit_pred)\n",
    "  true_labels.append(b_labels)\n",
    "  pred_labels.append(pred_label)\n",
    "\n",
    "# Flatten outputs\n",
    "tokenized_texts = [item for sublist in tokenized_texts for item in sublist]\n",
    "pred_labels = [item for sublist in pred_labels for item in sublist]\n",
    "true_labels = [item for sublist in true_labels for item in sublist]\n",
    "# Converting flattened binary values to boolean values\n",
    "true_bools = [tl==1 for tl in true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4abfd7e-9a3e-49c4-adf2-f4dbe46e102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_target_labels = [str(e) for e in enc.categories_[0]]\n",
    "pred_bools = [pl>0.50 for pl in pred_labels] #boolean output after thresholding\n",
    "\n",
    "# Print and save classification report\n",
    "print('Test F1 Accuracy: ', f1_score(true_bools, pred_bools,average='micro'))\n",
    "print('Test Flat Accuracy: ', accuracy_score(true_bools, pred_bools),'\\n')\n",
    "clf_report = classification_report(true_bools,pred_bools,target_names=str_target_labels)\n",
    "pickle.dump(clf_report, open('classification_report.txt','wb')) #save report\n",
    "print(clf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ce1996-a805-44d9-8e53-4837ddf9a027",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2label = dict(zip(range(6),str_target_labels))\n",
    "print(idx2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fb6ac8-e957-4fb9-b2d9-8acbcc23825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting indices of where boolean one hot vector true_bools is True so we can use idx2label to gather label names\n",
    "true_label_idxs, pred_label_idxs=[],[]\n",
    "for vals in true_bools:\n",
    "  true_label_idxs.append(np.where(vals)[0].flatten().tolist())\n",
    "for vals in pred_bools:\n",
    "  pred_label_idxs.append(np.where(vals)[0].flatten().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ace8b-d8cf-4315-879d-3ee71d7cc1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering vectors of label names using idx2label\n",
    "true_label_texts, pred_label_texts = [], []\n",
    "for vals in true_label_idxs:\n",
    "  if vals:\n",
    "    true_label_texts.append([idx2label[val] for val in vals])\n",
    "  else:\n",
    "    true_label_texts.append(vals)\n",
    "\n",
    "for vals in pred_label_idxs:\n",
    "  if vals:\n",
    "    pred_label_texts.append([idx2label[val] for val in vals])\n",
    "  else:\n",
    "    pred_label_texts.append(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8a8dac-de82-496d-9d5b-dd0d5537b0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding input ids to comment text\n",
    "comment_texts = [tokenizer.decode(text,skip_special_tokens=True,clean_up_tokenization_spaces=False) for text in tokenized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda295ea-ee95-4c77-9a9e-960519cf9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting lists to df\n",
    "comparisons_df = pd.DataFrame({'comment_text': comment_texts, 'true_labels': true_label_texts, 'pred_labels':pred_label_texts})\n",
    "comparisons_df.to_csv('comparisons.csv')\n",
    "comparisons_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b31eddf-a605-4e40-9503-646fb55376b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
