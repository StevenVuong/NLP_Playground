{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# We want to build a QR Information Retrieval System with a Language Model on Wikipedia Data\n\nLink to [article](https://qa.fastforwardlabs.com/pytorch/hugging%20face/wikipedia/bert/transformers/2020/05/19/Getting_Started_with_QA.html)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"## Setting up Environment","metadata":{}},{"cell_type":"code","source":"!pip install torch  torchvision -f https://download.pytorch.org/whl/torch_stable.html\n!pip install transformers==2.5.1\n!pip install wikipedia==1.4.0","metadata":{"execution":{"iopub.status.busy":"2022-10-23T12:42:39.899561Z","iopub.execute_input":"2022-10-23T12:42:39.900031Z","iopub.status.idle":"2022-10-23T12:43:21.059985Z","shell.execute_reply.started":"2022-10-23T12:42:39.899966Z","shell.execute_reply":"2022-10-23T12:43:21.058806Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Looking in links: https://download.pytorch.org/whl/torch_stable.html\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.1.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transformers==2.5.1\n  Downloading transformers-2.5.1-py3-none-any.whl (499 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m711.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (2.28.1)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (0.0.53)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (0.1.97)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (2021.11.10)\nCollecting tokenizers==0.5.2\n  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (3.7.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (1.21.6)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from transformers==2.5.1) (1.24.93)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.5.1) (0.6.0)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.5.1) (1.0.1)\nRequirement already satisfied: botocore<1.28.0,>=1.27.93 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.5.1) (1.27.93)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.5.1) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.5.1) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.5.1) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.5.1) (3.3)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.5.1) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.5.1) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.5.1) (1.15.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.93->boto3->transformers==2.5.1) (2.8.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers==2.5.1) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.5.1) (4.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.5.1) (3.8.0)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.5.2 transformers-2.5.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting wikipedia==1.4.0\n  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (from wikipedia==1.4.0) (4.11.1)\nRequirement already satisfied: requests<3.0.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wikipedia==1.4.0) (2.28.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.3)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.1.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4->wikipedia==1.4.0) (2.3.1)\nBuilding wheels for collected packages: wikipedia\n  Building wheel for wikipedia (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11695 sha256=31c3bcda69c33793ce719273606e86608a0c9eafbd819cb173c5156e18db1f05\n  Stored in directory: /root/.cache/pip/wheels/15/93/6d/5b2c68b8a64c7a7a04947b4ed6d89fb557dcc6bc27d1d7f3ba\nSuccessfully built wikipedia\nInstalling collected packages: wikipedia\nSuccessfully installed wikipedia-1.4.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"Consider alos the model, BERT does not do machine-translation and GPT does not do QA. So there are some limitations with each LM. We will go ahead with BERT.\n\n## QA Dataset\n\nFor QA dataset; SQuAD is a canonical dataset for QA and in SQuAD 1.1, all questions have the answer in the passage; whereas SQuAD2.0 has questions that cannot be answered by the provided passage.","metadata":{}},{"cell_type":"code","source":"# code to download the specified version of squad\n\n# set path with magic\n%env DATA_DIR=./data/squad \n\n# download the data\ndef download_squad(version=1):\n    if version == 1:\n        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n    else:\n        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n        !wget -P $DATA_DIR https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n            \ndownload_squad(version=2)","metadata":{"execution":{"iopub.status.busy":"2022-10-23T11:05:09.205712Z","iopub.execute_input":"2022-10-23T11:05:09.207439Z","iopub.status.idle":"2022-10-23T11:05:12.335442Z","shell.execute_reply.started":"2022-10-23T11:05:09.207389Z","shell.execute_reply":"2022-10-23T11:05:12.334079Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"env: DATA_DIR=./data/squad\n--2022-10-23 11:05:10--  https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.109.153, 185.199.108.153, 185.199.110.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.109.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 42123633 (40M) [application/json]\nSaving to: ‘./data/squad/train-v2.0.json’\n\ntrain-v2.0.json     100%[===================>]  40.17M   183MB/s    in 0.2s    \n\n2022-10-23 11:05:10 (183 MB/s) - ‘./data/squad/train-v2.0.json’ saved [42123633/42123633]\n\n--2022-10-23 11:05:11--  https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\nResolving rajpurkar.github.io (rajpurkar.github.io)... 185.199.110.153, 185.199.109.153, 185.199.111.153, ...\nConnecting to rajpurkar.github.io (rajpurkar.github.io)|185.199.110.153|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 4370528 (4.2M) [application/json]\nSaving to: ‘./data/squad/dev-v2.0.json’\n\ndev-v2.0.json       100%[===================>]   4.17M  --.-KB/s    in 0.08s   \n\n2022-10-23 11:05:12 (52.1 MB/s) - ‘./data/squad/dev-v2.0.json’ saved [4370528/4370528]\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fine-Tuning Script","metadata":{}},{"cell_type":"code","source":"# grab run_squad.py training script\n!curl -L -O https://github.com/huggingface/transformers/blob/b90745c5901809faef3136ed09a689e7d733526c/examples/run_squad.py","metadata":{"execution":{"iopub.status.busy":"2022-10-23T11:05:12.338970Z","iopub.execute_input":"2022-10-23T11:05:12.339734Z","iopub.status.idle":"2022-10-23T11:05:14.074352Z","shell.execute_reply.started":"2022-10-23T11:05:12.339699Z","shell.execute_reply":"2022-10-23T11:05:14.073181Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  462k    0  462k    0     0   859k      0 --:--:-- --:--:-- --:--:--  859k\n","output_type":"stream"}]},{"cell_type":"code","source":"# fine-tuning your own model for QA using HF's `run_squad.py`\n# turn flags on and off according to the model you're training\n\ncmd = [\n    'python', \n#    '-m torch.distributed.launch --nproc_per_node 2', # use this to perform distributed training over multiple GPUs\n    'run_squad.py', \n    \n    '--model_type', 'bert',                            # model type (one of the list under \"Pick a Model\" above)\n    '--model_name_or_path', 'bert-base-uncased',       # specific model name of the given model type (shown, a list is here: https://huggingface.co/transformers/pretrained_models.html) \n                                                       # on first execution this initiates a download of pre-trained model weights;\n                                                       # can also be a local path to a directory with model weights\n    \n    '--output_dir', './models/bert/bbu_squad2',        # directory for model checkpoints and predictions\n    \n#    '--overwrite_output_dir',                         # use when adding output to a directory that is non-empty --\n                                                       # for instance, when training crashes midway through and you need to restart it\n    \n    '--do_train',                                      # execute the training method \n    '--train_file', '$DATA_DIR/train-v2.0.json',       # provide the training data\n    '--version_2_with_negative',                       # ** MUST use this flag if training on SQuAD 2.0! DO NOT use if training on SQuAD 1.1\n    '--do_lower_case',                                 # ** set this flag if using an uncased model; don't use for Cased Models\n    '--do_eval',                                       # execute the evaluation method on the dev set -- note: \n                                                       # if coupled with --do_train, evaluation runs after fine-tuning \n    \n    '--predict_file', '$DATA_DIR/dev-v2.0.json',       # provide evaluation data (dev set)\n    '--eval_all_checkpoints',                          # evaluate the model on the dev set at each checkpoint\n    '--per_gpu_eval_batch_size', '12',                 # evaluation batch size for each gpu\n    '--per_gpu_train_batch_size', '12',                # training batch size for each gpu\n    '--save_steps', '5000',                            # how often checkpoints (complete model snapshot) are saved \n    '--threads', '8',                                  # num of CPU threads to use for converting SQuAD examples to model features\n    \n    # --- Model and Feature Hyperparameters --- \n    '--num_train_epochs', '3',                         # number of training epochs - usually 2-3 for SQuAD \n    '--learning_rate', '3e-5',                         # learning rate for the default optimizer (Adam in this case)\n    '--max_seq_length', '384',                         # maximum length allowed for the full input sequence \n    '--doc_stride', '128'                              # used for long documents that must be chunked into multiple features -- \n                                                       # this \"sliding window\" controls the amount of stride between chunks\n]","metadata":{"execution":{"iopub.status.busy":"2022-10-23T11:05:14.077573Z","iopub.execute_input":"2022-10-23T11:05:14.078062Z","iopub.status.idle":"2022-10-23T11:05:14.086053Z","shell.execute_reply.started":"2022-10-23T11:05:14.078017Z","shell.execute_reply":"2022-10-23T11:05:14.085037Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"When executing `run_squad.py` for the first time:\n1. Download pre-trained model weights for specified model type (bert-base-uncased)\n2. Convert SQuAD training examples into features (15-30 mins)\n3. Training features are saved to a cache file\n4. `--do_train` for as many epochs as we specify; savng model every ``save_steps`` until trainng finishes\n5. Final model weights and peripheral files are saved to ``output_dir``\n6. If ``--do_eval``; SQuAD dev examples are converted to features\n7. Dev features are also saved to a cache\n8. Evaluation commences and outputs an assortment of performance scores","metadata":{}},{"cell_type":"code","source":"!python run_squad.py  \\\n    --model_type bert   \\\n    --model_name_or_path bert-base-uncased  \\\n    --output_dir models/bert/ \\\n    --data_dir data/squad   \\\n    --overwrite_output_dir \\\n    --overwrite_cache \\\n    --do_train  \\\n    --train_file train-v2.0.json   \\\n    --version_2_with_negative \\\n    --do_lower_case  \\\n    --do_eval   \\\n    --predict_file dev-v2.0.json   \\\n    --per_gpu_train_batch_size 2   \\\n    --learning_rate 3e-5   \\\n    --num_train_epochs 2.0   \\\n    --max_seq_length 384   \\\n    --doc_stride 128   \\\n    --threads 10   \\\n    --save_steps 5000 ","metadata":{"execution":{"iopub.status.busy":"2022-10-23T11:05:14.087478Z","iopub.execute_input":"2022-10-23T11:05:14.088034Z","iopub.status.idle":"2022-10-23T11:05:15.193569Z","shell.execute_reply.started":"2022-10-23T11:05:14.087995Z","shell.execute_reply":"2022-10-23T11:05:15.192353Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"  File \"run_squad.py\", line 8\n    <!DOCTYPE html>\n    ^\nSyntaxError: invalid syntax\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Training Output\n\nWe'll find model's tokenizer:\n- `tokenizer_config.json`\n- `vocab.txt`\n- `special_tokens_map.json`\n\nModel Files:\n- `pytorch_model.bin`: Actual model weights (can be several GB for some models)\n- `config.json`: details of model architecture\n\nBinary representation of command line args to train the model: `training_args.bin`\n\nAnd if we included `--do_eval`:\n- `predictions_.json`: Official best answer for each example\n- `nbest_predictions.json` top n best answers for each example","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# Load the fine-tuned model\ntokenizer = AutoTokenizer.from_pretrained(\"./models/bert/bbu_squad2\")\nmodel = AutoModelForQuestionAnswering.from_pretrained(\"./models/bert/bbu_squad2\")","metadata":{"execution":{"iopub.status.busy":"2022-10-23T11:11:25.072532Z","iopub.execute_input":"2022-10-23T11:11:25.073006Z","iopub.status.idle":"2022-10-23T11:11:26.678766Z","shell.execute_reply.started":"2022-10-23T11:11:25.072933Z","shell.execute_reply":"2022-10-23T11:11:26.676331Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1688220072.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the fine-tuned model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/bert/bbu_squad2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForQuestionAnswering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./models/bert/bbu_squad2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mtokenizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tokenizer_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizer_class\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0mtokenizer_auto_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m     )\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresolved_config_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         )\n\u001b[1;32m    294\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                     raise ValueError(\n\u001b[0;32m--> 555\u001b[0;31m                         \u001b[0;34m\"Connection error, and we cannot find the requested files in the cached path.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m                         \u001b[0;34m\" Please try again or make sure your Internet connection is on.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     )\n","\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."],"ename":"ValueError","evalue":"Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.","output_type":"error"}]},{"cell_type":"markdown","source":"If we don't have time to train; we can just load a model for HuggingFace! There are several..","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\n# executing these commands for the first time initiates a download of the \n# model weights to ~/.cache/torch/transformers/\ntokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\") \nmodel = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")","metadata":{"execution":{"iopub.status.busy":"2022-10-23T12:47:42.957529Z","iopub.execute_input":"2022-10-23T12:47:42.957898Z","iopub.status.idle":"2022-10-23T12:48:39.815705Z","shell.execute_reply.started":"2022-10-23T12:47:42.957865Z","shell.execute_reply":"2022-10-23T12:48:39.814543Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/508 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c340f1c6614624b50a43a9a1badbf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a550a1dfd6a4e65a6e2bc72f3573d50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3b49ac8ca6e4b4ca869f17e578485e0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/152 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc7c256a2e05418b85d045e7acbabea5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/433M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f388258112c64027826a9b48b2cc5088"}},"metadata":{}}]},{"cell_type":"markdown","source":"### Try Our Model\n\nThree setps to QA:\n1. Tokenise input\n2. Obtain model scores\n3. Get the answer span","metadata":{}},{"cell_type":"code","source":"question = \"Who ruled Macedonia\"\n\ncontext = \"\"\"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, \nand later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled \nby the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient \nMacedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th \ncentury BC, it was a small kingdom outside of the area dominated by the city-states of Athens, \nSparta and Thebes, and briefly subordinate to Achaemenid Persia.\"\"\"\n\n\n# 1. TOKENIZE THE INPUT\n# note: if you don't include return_tensors='pt' you'll get a list of lists which is easier for \n# exploration but you cannot feed that into a model. \ninputs = tokenizer.encode_plus(question, context, return_tensors=\"pt\") \n\n# 2. OBTAIN MODEL SCORES\n# the AutoModelForQuestionAnswering class includes a span predictor on top of the model. \n# the model returns answer start and end scores for each word in the text\nanswer_start_scores, answer_end_scores = model(**inputs)\nanswer_start = torch.argmax(answer_start_scores)  # get the most likely beginning of answer with the argmax of the score\nanswer_end = torch.argmax(answer_end_scores) + 1  # get the most likely end of answer with the argmax of the score\n\n# 3. GET THE ANSWER SPAN\n# once we have the most likely start and end tokens, we grab all the tokens between them\n# and convert tokens back to words!\ntokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))","metadata":{"execution":{"iopub.status.busy":"2022-10-23T12:48:43.956808Z","iopub.execute_input":"2022-10-23T12:48:43.957324Z","iopub.status.idle":"2022-10-23T12:48:44.481501Z","shell.execute_reply.started":"2022-10-23T12:48:43.957281Z","shell.execute_reply":"2022-10-23T12:48:44.479470Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"'the Argead dynasty'"},"metadata":{}}]},{"cell_type":"markdown","source":"## QA On Wikipedia Pages","metadata":{}},{"cell_type":"code","source":"# pull up a wikipedia page\nimport wikipedia as wiki\nimport pprint as pp\n\nquestion = \"What is the wingspan of an albatross?\"\n\nresults = wiki.search(question)\nprint(\"Wikipedia seach results for our question: \\n\")\npp.pprint(results)\n\npage = wiki.page(results[0])\ntext = page.content\nprint(f\"\\nThe {results[0]} Wikipedia article contains {len(text)} characters.\")","metadata":{"execution":{"iopub.status.busy":"2022-10-23T12:52:32.595037Z","iopub.execute_input":"2022-10-23T12:52:32.596057Z","iopub.status.idle":"2022-10-23T12:52:38.892197Z","shell.execute_reply.started":"2022-10-23T12:52:32.596009Z","shell.execute_reply":"2022-10-23T12:52:38.890942Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Wikipedia seach results for our question: \n\n['Wandering albatross',\n 'Pelagornis sandersi',\n 'List of largest birds',\n 'Black-browed albatross',\n 'Argentavis',\n 'List of birds by flight speed',\n 'Mollymawk',\n 'Largest body part',\n 'Aspect ratio (aeronautics)',\n 'Early flying machines']\n\nThe Wandering albatross Wikipedia article contains 10427 characters.\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\nprint(f\"This translates into {len(inputs['input_ids'][0])} tokens.\")","metadata":{"execution":{"iopub.status.busy":"2022-10-23T12:53:21.176748Z","iopub.execute_input":"2022-10-23T12:53:21.177196Z","iopub.status.idle":"2022-10-23T12:53:21.228242Z","shell.execute_reply.started":"2022-10-23T12:53:21.177122Z","shell.execute_reply":"2022-10-23T12:53:21.227211Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"This translates into 2536 tokens.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note: Always use tokenizer specific to model, as tokens themselves will be different.\n\n2536 execeds number of tokens we can feed into at once; most BERT-esque models can only accept 512 tokens at once, so need to split into chunks and each chunk must not exceed 512 tokens in total.\n\nWe must follow the format: [CLS] question tokens [SEP] context tokens [SEP].\n\nSo we must prepend the original question, followed by the next \"chunk\" of article tokens.","metadata":{}},{"cell_type":"code","source":"from collections import OrderedDict\n\n# identify question tokens (token_type_ids = 0)\nqmask = inputs[\"token_type_ids\"].lt(1) # make a mast of token type ids=0; represents question. Make mask\n\nqt = torch.masked_select(inputs['input_ids'], qmask) # question tokens\nprint(f\"The question consists of {qt.size()[0]} tokens.\")\n\n# 1 accounts for having to add a [SEP] token to the end of each chunk\nchunk_size = model.config.max_position_embeddings - qt.size()[0] - 1 # max chunk size=512 - len(question_tokens) - 1\nprint(f\"Each chunk will contain {chunk_size - 2} tokens of the Wikipedia article.\")\n\n# create a dict of dicts; each sub-dict mimics the structure of pre-chunked model inputs\nchunked_input = OrderedDict()\n\n# loop through input_ids, token_type_ids, attention mask\nfor k, v in inputs.items():\n    q = torch.masked_select(v, qmask) \n    c = torch.masked_select(v, ~qmask)\n    chunks = torch.split(c, chunk_size) # chunk remaining tokens (non-question)\n    \n    # loop through chunks\n    for i, chunk in enumerate(chunks): \n        \n        # make key first time around\n        if i not in chunked_input:\n            chunked_input[i] = {}\n        \n        # append question to chunk\n        thing = torch.cat((q, chunk))\n        if i != len(chunks)-1:\n            # append 102 SEP for end of question\n            if k == \"input_ids\":\n                thing = torch.cat((thing, torch.tensor([102])))\n            # else add 1\n            else:\n                thing = torch.cat((thing, torch.tensor([1])))\n                \n        chunked_input[i][k] = torch.unsqueeze(thing, dim=0)","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:00:58.761545Z","iopub.execute_input":"2022-10-23T13:00:58.761907Z","iopub.status.idle":"2022-10-23T13:00:58.773341Z","shell.execute_reply.started":"2022-10-23T13:00:58.761875Z","shell.execute_reply":"2022-10-23T13:00:58.772283Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"The question consists of 12 tokens.\nEach chunk will contain 497 tokens of the Wikipedia article.\n","output_type":"stream"}]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:08:37.784092Z","iopub.execute_input":"2022-10-23T13:08:37.784918Z","iopub.status.idle":"2022-10-23T13:08:37.794115Z","shell.execute_reply.started":"2022-10-23T13:08:37.784877Z","shell.execute_reply":"2022-10-23T13:08:37.792954Z"},"trusted":true},"execution_count":34,"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  1327,  1110,  ..., 19021,  1233,   102]]),\n 'token_type_ids': tensor([[0, 0, 0,  ..., 1, 1, 1]]),\n 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"code","source":"for i in range(len(chunked_input.keys())):\n    print(f\"Number of tokens in chunk {i}: {len(chunked_input[i]['input_ids'].tolist()[0])}\")","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:03:40.449231Z","iopub.execute_input":"2022-10-23T13:03:40.449601Z","iopub.status.idle":"2022-10-23T13:03:40.456057Z","shell.execute_reply.started":"2022-10-23T13:03:40.449569Z","shell.execute_reply":"2022-10-23T13:03:40.454952Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Number of tokens in chunk 0: 512\nNumber of tokens in chunk 1: 512\nNumber of tokens in chunk 2: 512\nNumber of tokens in chunk 3: 512\nNumber of tokens in chunk 4: 512\nNumber of tokens in chunk 5: 41\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Can now be fed to model without indexing errors. We'll get an \"answer\" for each chunk, however, not all answers are useful since not every part of Wikipedia article is informative for our question. The model will return a [CLS] token when it determines the context does not contain an answer to the question.","metadata":{}},{"cell_type":"code","source":"def convert_ids_to_string(tokenizer, input_ids):\n    return tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids))\n\nanswer = \"\"\n\n# iterate over chunks, look for best answer from each chunk\nfor _, chunk in chunked_input.items():\n    answer_start_scores, answer_end_scores = model(**chunk)\n    \n    answer_start = torch.argmax(answer_start_scores)\n    answer_end = torch.argmax(answer_end_scores) + 1\n    \n    ans = convert_ids_to_string(tokenizer, chunk['input_ids'][0][answer_start:answer_end])\n    \n    # if ans = [CLS] then the model did not find a real answer in this chunk\n    if ans != '[CLS]':\n        answer += ans + \" / \"\n    \nprint(answer)","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:27:56.203074Z","iopub.execute_input":"2022-10-23T13:27:56.204002Z","iopub.status.idle":"2022-10-23T13:28:01.603356Z","shell.execute_reply.started":"2022-10-23T13:27:56.203964Z","shell.execute_reply":"2022-10-23T13:28:01.602349Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Putting it all together","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\nclass DocumentReader:\n    def __init__(self, pretrained_model_name_or_path=\"bert-large-uncased\"):\n        self.READER_PATH = pretrained_model_name_or_path\n        self.tokenizer = AutoTokenizer.from_pretrained(self.READER_PATH)\n        self.model = AutoModelForQuestionAnswering.from_pretrained(self.READER_PATH)\n        self.max_len = self.model.config.max_position_embeddings\n        self.chunked = False\n        \n    def tokenize(self, question, text):\n        self.inputs = self.tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n        self.input_ids = self.inputs[\"input_ids\"].tolist()[0]\n        \n        if len(self.input_ids) > self.max_len:\n            self.inputs = self.chunkify()\n            self.chunked = True\n            \n    def chunkify(self):\n        \"\"\"\n        Break up a long article into chunks that fit within the max token\n        requirement for that Transformer model.\n        \n        Calls to BERT / RoBERTa / ALBERT require the following format:\n        [CLS] question tokens [SEP] context tokens [SEP].\n        \"\"\"\n        # create a question mask based on token_type_ids\n        # value is 0 for question tokens, 1 for context tokens\n        qmask = self.inputs[\"token_type_ids\"].lt(1)\n        qt = torch.masked_select(self.inputs[\"input_ids\"], qmask)\n        chunk_size = self.max_len - qt.size()[0] - 1 # 1 accounts for having to add [SEP] to the end\n        \n        # create dict of dicts; each sub-dict mimics structure of pre-chunked models inputs\n        chunked_input = OrderedDict()\n        for k, v in self.inputs.items():\n            q = torch.masked_select(v, qmask)\n            c = torch.masked_select(v, ~qmask)\n            chunks = torch.split(c, chunk_size)\n            \n            for i, chunk in enumerate(chunks):\n                if i not in chunked_input:\n                    chunked_input[i] = {}\n                    \n                thing = torch.cat((q, chunk))\n                if i != len(chunks) - 1:\n                    if k == 'input_ids':\n                        thing = torch.cat((thing, torch.tensor([102])))\n                    else:\n                        thing = torch.cat((thing, torch.tensor([1])))\n                        \n                chunked_input[i][k] = torch.unsqueeze(thing, dim=0)\n        return chunked_input\n    \n    \n    def get_answer(self):\n        if self.chunked:\n            answer = \"\"\n            for k, chunk in self.inputs.items():\n                answer_start_scores, answer_end_scores = self.model(**chunk)\n                \n                answer_start = torch.argmax(answer_start_scores)\n                answer_end = torch.argmax(answer_end_scores) + 1\n                \n                ans = self.convert_ids_to_string(chunk['input_ids'][0][answer_start:answer_end])\n                if ans != '[CLS]':\n                    answer += ans + \" / \"\n            return answer\n        else:\n            answer_start_scores, answer_end_scores = self.model(**self.inputs)\n            \n            answer_start = torch.argmax(answer_start_scores) # most likely beginning of answer\n            answer_end = torch.argmax(answer_end_scores) + 1\n            \n            return self.convert_ids_to_string(self.inputs['input_ids'][0][answer_start:answer_end])\n        \n    def convert_ids_to_string(self, input_ids):\n        return self.tokenizer.convert_tokens_to_string(self.tokenizer.convert_ids_to_tokens(input_ids))","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:30:09.830826Z","iopub.execute_input":"2022-10-23T13:30:09.831226Z","iopub.status.idle":"2022-10-23T13:30:09.848191Z","shell.execute_reply.started":"2022-10-23T13:30:09.831191Z","shell.execute_reply":"2022-10-23T13:30:09.847175Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"questions = [\n    'When was Barack Obama born?',\n    'Why is the sky blue?',\n    'How many sides does a pentagon have?'\n]\n\nreader = DocumentReader(\"deepset/bert-base-cased-squad2\") \n\n# if you trained your own model using the training cell earlier, you can access it with this:\n#reader = DocumentReader(\"./models/bert/bbu_squad2\")\n\nfor question in questions:\n    print(f\"Question: {question}\")\n    results = wiki.search(question)\n\n    page = wiki.page(results[0])\n    print(f\"Top wiki result: {page}\")\n\n    text = page.content\n\n    reader.tokenize(question, text)\n    print(f\"Answer: {reader.get_answer()}\")\n    print()","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:30:17.249977Z","iopub.execute_input":"2022-10-23T13:30:17.251110Z","iopub.status.idle":"2022-10-23T13:31:17.080649Z","shell.execute_reply.started":"2022-10-23T13:30:17.251063Z","shell.execute_reply":"2022-10-23T13:31:17.079490Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"Question: When was Barack Obama born?\nTop wiki result: <WikipediaPage 'Family of Barack Obama'>\nAnswer: January 17 , 1964 / circa 1829 / 1895 / 1934 / c . 1940 / [CLS] When was Barack Obama born ? [SEP] Siaya in 2013 . His campaign slogan was \" Obama here , Obama there \" in reference to his half - brother who was serving his second term as the president of the United States . Malik garnered a meager 2 , 792 votes , about 140 , 000 votes behind the eventual winner . Prior to the 2016 United States presidential election , he stated that he supported Donald Trump , the candidate for the Republican Party . He attended the third presidential debate as one of Trump ' s guests . = = = Auma Obama = = = Barack Obama ' s half - sister , born c . 1960 / August 24 , 1912 / \n\nQuestion: Why is the sky blue?\nTop wiki result: <WikipediaPage 'Diffuse sky radiation'>\nAnswer: Rayleigh scattering / its intrinsic nature , can illuminate under - canopy leaves permitting more efficient total whole - plant photosynthesis / \n\nQuestion: How many sides does a pentagon have?\nTop wiki result: <WikipediaPage 'The Pentagon'>\nAnswer: five / five / \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Incorrect answer is due to content more oft than the actual model! So also requires a search engine (document retriever) as good as the document reader.","metadata":{}},{"cell_type":"code","source":"page.content","metadata":{"execution":{"iopub.status.busy":"2022-10-23T13:32:41.381818Z","iopub.execute_input":"2022-10-23T13:32:41.382111Z","iopub.status.idle":"2022-10-23T13:32:41.390244Z","shell.execute_reply.started":"2022-10-23T13:32:41.382085Z","shell.execute_reply":"2022-10-23T13:32:41.388991Z"},"trusted":true},"execution_count":42,"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'The Pentagon is the headquarters building of the United States Department of Defense. It was constructed on an accelerated schedule during World War II. As a symbol of the U.S. military, the phrase The Pentagon is often used as a metonym for the Department of Defense and its leadership.\\nLocated in Arlington County, Virginia, across the Potomac River from Washington, D.C., the building was designed by American architect George Bergstrom and built by contractor John McShain. Ground was broken on 11 September 1941, and the building was dedicated on 15 January 1943. General Brehon Somervell provided the major impetus to gain Congressional approval for the project; Colonel Leslie Groves was responsible for overseeing the project for the U.S. Army Corps of Engineers, which supervised it.\\nThe Pentagon is the world\\'s largest office building, with about 6.5 million square feet (150 acres; 60 ha) of floor space, of which 3.7 million square feet (85 acres; 34 ha) are used as offices. Some 23,000 military and civilian employees, and another 3,000 non-defense support personnel, work in the Pentagon. It has five sides, five floors above ground, two basement levels, and five ring corridors per floor with a total of 17.5 mi (28.2 km) of corridors. The central five-acre (2.0 ha) pentagonal plaza is nicknamed \"ground zero\" on the presumption that it would be a prime target in a nuclear war.In 2001, the Pentagon was damaged during the September 11 attacks. Five al-Qaeda hijackers flew American Airlines Flight 77 into the western side of the building, killing themselves and 184 others: 59 on the airplane and 125 in the Pentagon. It was the first significant foreign attack on Washington\\'s governmental facilities since the city was burned by the British during the War of 1812. Following the attacks, the western side of the building was repaired, with a small indoor memorial and chapel added at the point of impact. An outdoor memorial dedicated to the Pentagon victims of 9/11 opened in 2008.\\n\\n\\n== Layout and facilities ==\\nThe Pentagon building spans 28.7 acres (116,000 m2), and includes an additional 5.1 acres (21,000 m2) as a central courtyard.Starting with the north side and moving clockwise, its five façade entrances are the Mall Terrace, the River Terrace, the Concourse (or Metro Station), the South Parking, and the Heliport. On the north side of the building, the Mall Entrance, which also features a portico, leads out to a 600 ft-long (180 m) terrace that is used for ceremonies. The River Entrance, which features a portico projecting out twenty ft (6 m), is on the northeast side, overlooking the lagoon and facing Washington. A stepped terrace on the River Entrance leads down to the lagoon; and a landing dock was used until the late 1960s to ferry personnel between Bolling Air Force Base and the Pentagon. The main entrance for visitors is on the southeast side, as are the Pentagon Metro station and the bus station.\\nThere is also a concourse on the southeast side of the second floor of the building, which contains a mini-shopping mall. The south parking lot adjoins the southwest façade, and the west side of the Pentagon faces Washington Boulevard.\\nThe concentric rings are designated from the center out as \"A\" through \"E\" (with additional \"F\" and \"G\" rings in the basement). \"E\" Ring offices are the only ones with outside views and are generally occupied by senior officials. Office numbers go clockwise around each of the rings, and have two parts: a nearest-corridor number (1 to 10), followed by a bay number (00 to 99), so office numbers range from 100 to 1099. These corridors radiate out from the central courtyard, with corridor 1 beginning with the Concourse\\'s south end. Each numbered radial corridor intersects with the corresponding numbered group of offices (for example, corridor 5 divides the 500 series office block). There are a number of historical displays in the building, particularly in the \"A\" and \"E\" rings.Subterranean floors in the Pentagon are lettered \"B\" for Basement and \"M\" for Mezzanine. The concourse is on the second floor at the Metro entrance. Above-ground floors are numbered 1 to 5. Room numbers are given as the floor, concentric ring, and office number (which is in turn the nearest corridor number followed by the bay number). Thus, office 2B315 is on the second floor, B ring, and nearest to corridor 3 (between corridors 2 and 3). One way to get to this office would be to go to the second floor, get to the A (innermost) ring, go to and take corridor 3, and then turn left on ring B to get to bay 15.It is possible for a person to walk between any two points in the Pentagon in less than ten minutes, though the most optimal route may involve a brisk walk, routing through the open-air central courtyard, or both. The complex includes eating and exercise facilities, and meditation and prayer rooms.\\nJust south of the Pentagon are Pentagon City and Crystal City, extensive shopping, business, and high-density residential districts in Arlington. Arlington National Cemetery is to the north. The Pentagon is surrounded by the relatively complex Pentagon road network.The Pentagon has six Washington, DC, ZIP Codes (despite its location in Virginia): The Secretary of Defense, the Joint Chiefs of Staff, and the four service branches each have their own ZIP Code.\\n\\n\\n== History ==\\n\\n\\n=== Background ===\\n\\nBefore the Pentagon was built, the United States Department of War was headquartered in the Munitions Building, a temporary structure erected during World War I along Constitution Avenue on the National Mall. The War Department, which was a civilian agency created to administer the U.S. Army, was spread out in additional temporary buildings on the National Mall, as well as dozens of other buildings in Washington, D.C., Maryland and Virginia. In the late 1930s, during the Great Depression and federal construction program, a new War Department Building was constructed at 21st and C Streets in Foggy Bottom but, upon completion, the new building did not solve the department\\'s space problem. It became the headquarters of the Department of State.When World War II broke out in Europe in 1939, the War Department rapidly expanded to deal with current issues and in anticipation that the United States would be drawn into the conflict. Secretary of War Henry L. Stimson found the situation unacceptable, with the Munitions Building overcrowded and department offices spread out in additional sites.Stimson told U.S. President Franklin D. Roosevelt in May 1941 that the War Department needed additional space. On 17 July 1941, a congressional hearing took place, organized by Congressman Clifton Woodrum (D-VA), regarding proposals for new War Department buildings. Woodrum pressed Brigadier General Eugene Reybold, who was representing the War Department at the hearing, for an \"overall solution\" to the department\\'s \"space problem\", rather than building yet more temporary buildings. Reybold agreed to report back to the congressman within five days. The War Department called upon its construction chief, General Brehon Somervell, to come up with a plan.\\n\\n\\n=== Planning ===\\n\\nGovernment officials agreed that the War Department building, officially designated Federal Office Building No 1, should be constructed across the Potomac River, in Arlington County, Virginia. Requirements for the new building were that it be no more than four stories tall, and that it use a minimal amount of steel to reserve that resource for war needs. The requirements meant that, instead of rising vertically, the building would be sprawling over a large area. Possible sites for the building included the Department of Agriculture\\'s Arlington Experimental Farm, adjacent to Arlington National Cemetery, and the obsolete Hoover Field site.The site originally chosen was Arlington Farms, which had an asymmetric, roughly pentagonal shape, so the building was planned accordingly as an irregular pentagon. Concerned that the new building could obstruct the view of Washington, D.C., from Arlington Cemetery, President Roosevelt ended up selecting the Hoover Airport site instead. The building retained the pentagonal layout because Roosevelt liked it and a major redesign at that stage would have been costly. Freed of the constraints of the Arlington Farms site, the building was modified as a regular pentagon. It resembled star forts constructed during the gunpowder age.On 28 July, Congress authorized funding for a new Department of War building in Arlington, which would house the entire department under one roof. President Roosevelt officially approved of the Hoover Airport site on 2 September. While the project went through the approval process in late July 1941, Somervell selected the contractors, including John McShain, Inc. of Philadelphia, which had built Washington National Airport in Arlington, the Jefferson Memorial in Washington, and the National Naval Medical Center in Bethesda, Maryland, along with Wise Contracting Company, Inc. and Doyle and Russell, both from Virginia. In addition to the Hoover Airport site and other government-owned land, construction of the Pentagon required an additional 287 acres (1.16 km2), which were acquired at a cost of $2.2 million (equivalent to $30.3 million in 2020). The Hell\\'s Bottom neighborhood, consisting of numerous pawnshops, factories, approximately 150 homes, and other buildings around Columbia Pike, was cleared to make way for the Pentagon. Later, 300 acres (1.2 km2) of land were transferred to Arlington National Cemetery and to Fort Myer, leaving 280 acres (1.1 km2) for the Pentagon.\\n\\n\\n=== Construction ===\\n\\nContracts totaling $31,100,000 (equivalent to $428 million in 2020) were finalized with McShain and the other contractors on 11 September 1941, and ground was broken for the Pentagon the same day. Among the design requirements, Somervell required the structural design to accommodate floor loads of up to 150 psi (1,000 kPa), which was done in case the building became a records storage facility at some time after the end of the current war. A minimal amount of steel was used as it was in short supply during World War II. Instead, the Pentagon was built as a reinforced concrete structure, using 680,000 tons of sand dredged from the Potomac River, and a lagoon was created beneath the Pentagon\\'s river entrance. To minimize steel usage, concrete ramps were built rather than installing elevators. Indiana limestone was used for the building\\'s façade.Architectural and structural design work for the Pentagon proceeded simultaneously with construction, with initial drawings provided in early October 1941, and most of the design work completed by 1 June 1942. At times the construction work got ahead of the design, with different materials used than those specified in the plans. Pressure to speed up design and construction intensified after the attack on Pearl Harbor on 7 December 1941, with Somervell demanding that 1 million sq ft (9.3 ha) of space at the Pentagon be available for occupation by 1 April 1943. David J. Witmer replaced Bergstrom as chief architect on 11 April after Bergstrom resigned. Unrelated to the Pentagon project, he was charged with improper conduct while having served as president of the American Institute of Architects. Construction was completed 15 January 1943.Soil conditions of the site – on the Potomac River floodplain – presented challenges, as did the varying elevations across the site, which ranged from ten to forty ft (3 to 12 m) above sea level. Two retaining walls were built to compensate for the elevation variations, and cast-in-place piles were used to deal with the soil conditions. Construction of the Pentagon was completed in approximately 16 months at a total cost of $83 million (equivalent to $1.14 billion in 2020). The building\\'s approximate height is 77 ft (23 m), and each of the five sides is 921 ft (281 m) in length.The building was built wedge by wedge; each wedge was occupied as soon as it was completed, even as construction continued on the remaining wedges.The Pentagon was designed in accordance with the racial segregation laws in force in the state of Virginia at the time, with separate eating and lavatory accommodations for white and black persons. While the sets of lavatories were side by side, the dining areas for blacks were located in the basement. When Roosevelt visited the facility before its dedication, he ordered removal of the \"Whites Only\" signs in segregated areas. When the Governor of Virginia protested, Roosevelt\\'s administration responded that the Pentagon, although on Virginia land, was under Federal jurisdiction. In addition, its military and civilian Federal employees were going to comply with the President\\'s policies. As a result, the Pentagon was the only building in Virginia where racial segregation laws were not enforced (these laws were not overturned until 1965). The side-by-side sets of restrooms still exist, but have been integrated in practice since the building was occupied.\\n\\n\\n=== Hall of Heroes ===\\n\\nOn the building\\'s main concourse is the Hall of Heroes, opened 1968 and dedicated to the more than 3,460 recipients of the Medal of Honor, the United States\\' highest military decoration. The three versions of the Medal of Honor – Army, Sea Service (for the Marine Corps, Navy, and Coast Guard), and Air Force (for the Air Force and Space Force) – are on display along with the names of recipients.The Hall is also used for promotions, retirements, and other ceremonies.\\n\\n\\n=== Renovation ===\\n\\nFrom 1998 to 2011, the Pentagon was completely gutted and reconstructed in phases to bring it up to modern standards and improve security and efficiency. Asbestos was removed and all office windows were sealed.As originally built, most Pentagon office space consisted of open bays which spanned an entire ring. These offices used cross-ventilation from operable windows instead of air conditioning for cooling. Gradually, bays were subdivided into private offices with many using window air conditioning units. With renovations now complete, the new space includes a return to open office bays, and a new Universal Space Plan of standardized office furniture and partitions developed by Studios Architecture.Pentagon tours were suspended during the COVID-19 pandemic.\\n\\n\\n== Incidents ==\\n\\n\\n=== Protests ===\\n\\nDuring the late 1960s, the Pentagon became a focal point for protests against the Vietnam War. A group of 2,500 women, organized by Women Strike for Peace, demonstrated outside Secretary of Defense Robert S. McNamara\\'s office at the Pentagon on 15 February 1967. In May 1967, a group of 20 demonstrators held a sit-in outside the Joint Chiefs of Staff\\'s office, which lasted four days before they were arrested. In one of the better known incidents, on 21 October 1967, some 35,000 anti-war protesters organized by the National Mobilization Committee to End the War in Vietnam, gathered for a demonstration at the Defense Department (the \"March on the Pentagon\"). They were confronted by some 2,500 armed soldiers. During the protest, a famous picture was taken, where George Harris placed carnations into the soldiers\\' gun barrels. The march concluded with an attempt to \"exorcise\" the building.On 19 May 1972, the Weather Underground Organization bombed a fourth-floor women\\'s restroom, in \"retaliation\" for the Nixon administration\\'s bombing of Hanoi in the final stages of the Vietnam War.On 17 March 2007, 4,000 to 15,000 people (estimates vary significantly) protested the Iraq War by marching from the Lincoln Memorial to the Pentagon\\'s north parking lot.\\n\\n\\n=== September 11, 2001 attacks ===\\n\\nOn September 11, 2001, coincidentally the 60th anniversary of the Pentagon\\'s groundbreaking, five al-Qaeda affiliated hijackers took control of American Airlines Flight 77, en route from Washington Dulles International Airport to Los Angeles International Airport, and deliberately crashed the Boeing 757 airliner into the western side of the Pentagon at 9:37 am EDT as part of the September 11 attacks. The impact of the plane severely damaged the outer ring of one wing of the building and caused its partial collapse. At the time of the attacks, the Pentagon was under renovation and many offices were unoccupied, resulting in fewer casualties. Only 800 of 4,500 people who would have been in the area were there because of the work. Furthermore, the area hit, on the side of the Heliport façade, was the section best prepared for such an attack. The renovation there, improvements which resulted from the Oklahoma City bombing, had nearly been completed.It was the only area of the Pentagon with a sprinkler system, and it had been reconstructed with a web of steel columns and bars to withstand bomb blasts. The steel reinforcement, bolted together to form a continuous structure through all of the Pentagon\\'s five floors, kept that section of the building from collapsing for 30 minutes—enough time for hundreds of people to crawl out to safety. The area struck by the plane also had blast-resistant windows—2 inches (5 cm) thick and 2,500 pounds (1,100 kg) each—that stayed intact during the crash and fire. It had fire doors that opened automatically and newly built exits that allowed people to get out.\\n\\nContractors already involved with the renovation were given the added task of rebuilding the sections damaged in the attacks. This additional project was named the \"Phoenix Project\" and was charged with having the outermost offices of the damaged section occupied by 11 September 2002.When the damaged section of the Pentagon was repaired, a small indoor memorial and chapel were added at the point of impact. For the fifth anniversary of the September 11 attacks, a memorial of 184 beams of light shone up from the center courtyard of the Pentagon, one light for each victim of the attack. In addition, an American flag is hung each year on the side of the Pentagon damaged in the attacks, and the side of the building is illuminated at night with blue lights. After the attacks, plans were developed for an outdoor memorial, with construction underway in 2006. This Pentagon Memorial consists of a park on 2 acres (8,100 m2) of land, containing 184 benches, one dedicated to each victim. The benches are aligned along the line of Flight 77 according to the victims\\' ages, from 3 to 71. The park opened to the public on 11 September 2008.\\n\\n\\n== Gallery ==\\n\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\t\\t\\n\\n\\n== See also ==\\n\\nList of National Historic Landmarks in Virginia\\nList of United States military bases\\nNational Register of Historic Places listings in Arlington County, Virginia\\nThe Octagon, headquarters of the Egyptian Ministry of Defense in the New Administrative Capital\\n\\n\\n== Notes ==\\n\\n\\n== References ==\\n\\n\\n=== Citations ===\\n\\n\\n=== Sources ===\\n\\n\\n== External links ==\\n\\nThe Pentagon website (archived version)\\nPopular Mechanics, March 1943, \"Army\\'s Giant Five-by-Five\" one of earliest World War II articles on the Pentagon\\nPentagon Force Protection Agency\\nHow the Pentagon Got Its Shape – The Washington Post, 26 May 2007\\nU.S. Geological Survey Geographic Names Information System: The Pentagon'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}