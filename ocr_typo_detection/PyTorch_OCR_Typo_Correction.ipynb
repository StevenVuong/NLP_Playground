{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch OCR Typo Correction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Original Post](https://towardsdatascience.com/ocr-typo-detection-9dd6e396ecac)"
      ],
      "metadata": {
        "id": "1T8M13T-nF1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xue-gEuamtvo"
      },
      "outputs": [],
      "source": [
        "# download dataset\n",
        "!wget https://zenodo.org/record/3515403/files/ICDAR2019-POCR-ground-truth.zip?download=1\n",
        "!mv 'ICDAR2019-POCR-ground-truth.zip?download=1' IDCAR2019-POCR-ground-truth.zip''\n",
        "!unzip IDCAR2019-POCR-ground-truth.zip\n",
        "!rm IDCAR2019-POCR-ground-truth.zip "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import all our dependencies\n",
        "!pip install transformers\n",
        "!pip install pyyaml==5.4.1 # reverting to this for plotly express to work; 6.0.0 is not compatbile at the moment\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import plotly.express as px\n",
        "import pickle\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForTokenClassification\n",
        "import torch\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "import glob\n",
        "import os\n",
        "from os import path\n",
        "import editdistance\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import transformers\n",
        "from transformers import BertTokenizer, BertConfig, BertModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "torch.__version__\n",
        "transformers.__version__\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "rFJIs4gSm1m3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Each of the files contains OCR'd text, aligned OCR'd text and aligned GS (gold standard) text on the new line.\n",
        "Aligned OCR'd and GS texts always have the same length, and missing characters are defined by '@' sign.\n",
        "\n",
        "Folder structure:\n",
        "    - training_18M_without_Finnish\n",
        "        - Languages: EN/DE, etc..\n",
        "            - EN1, DE1 etc..\n",
        "                - 0.txt, 1.txt, etc..\n",
        "                    - shape (3, len_each_tect) in order below: \n",
        "\n",
        "OCR_toInput: Raw OCR output\n",
        "OCR_aligned: OCR output with @ inplace of where length is changed, so is the same length\n",
        "GS_aligned: Ground truth\n",
        "\"\"\"\n",
        "# load the dataset\n",
        "file = path.join(r\"./training_18M_without_Finnish/EN\", \"EN1\", \"0.txt\")\n",
        "\n",
        "with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.readlines()\n",
        "\n",
        "[s[:100] for s in raw_text]"
      ],
      "metadata": {
        "id": "vR9468hqnDK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use NLTK package for sentence tokenization (english tokenizer)\n",
        "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "metadata": {
        "id": "mevHnXBAoj4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Split sentences into subtokens in places where space sign matches in both\n",
        "ocr and GS sentences. So avoid situations where incorrectly split OCR'd word \n",
        "splits also the GS word\n",
        "\"\"\"\n",
        "\n",
        "def get_common_space_ids(sentence, gs_sentence):\n",
        "    \"\"\"Return indices of the spaces in the text which match in the both OCR and GS sentences.\"\"\"\n",
        "    \n",
        "    # get indexes of all the spaces in ocr and gs sentences\n",
        "    ocr_space_ids = [match.span()[0] for match in re.finditer(\" \", sentence)]\n",
        "    gs_space_ids = [match.span()[0] for match in re.finditer(\" \", gs_sentence)]\n",
        "\n",
        "    gs_cursor = 0\n",
        "    new_ocr_space_ids = []\n",
        "    # loop through ocr space ids\n",
        "    for space_id in ocr_space_ids:\n",
        "        \n",
        "        # while counter less than total gs len and len less than space id; add 1 to catch up to\n",
        "        while (gs_cursor < len(gs_space_ids)) and (gs_space_ids[gs_cursor] < space_id):\n",
        "            gs_cursor += 1\n",
        "\n",
        "        # if gs cursor less than len, and the two match; then it is the same spacing; append\n",
        "        # only if the spacing id's are equal for the given word; then we append\n",
        "        if (gs_cursor < len(gs_space_ids)) and (gs_space_ids[gs_cursor] == space_id):\n",
        "            new_ocr_space_ids.append(space_id)\n",
        "    \n",
        "    # append length of sentence as the last element in list\n",
        "    new_ocr_space_ids.append(len(sentence))\n",
        "    return new_ocr_space_ids"
      ],
      "metadata": {
        "id": "X3TKnK3Sp-so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_sentence = [s[:100] for s in raw_text]\n",
        "sample_ocr = sample_sentence[1] # ocr with @\n",
        "sample_gt = sample_sentence[2] # gs\n",
        "get_common_space_ids(sample_ocr, sample_gt)"
      ],
      "metadata": {
        "id": "vetRg96Xrazd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create ground truth by traversing all files, and collecting split sentences into\n",
        "subtokens (variable words). Every subtoken receives the corresponding binary labels,\n",
        "where 1 means that  it contains a typo (label).\n",
        "\n",
        "Also collect algined OCR'd words and GS words for further analysis,\n",
        "aligned_words, aligned_gs_words\n",
        "\"\"\"\n",
        "def extract_dataset(files, ocr_last_breakline_remove=False):\n",
        "\n",
        "    # initialise variables\n",
        "    words = [] # subtokens\n",
        "    labels = [] # labels; 1 means has a type, 0 means no typo\n",
        "    aligned_words = [] \n",
        "    aligned_gs_words = []\n",
        "\n",
        "    print(f'There are {len(files):,} files in total')\n",
        "\n",
        "    # loop through all the files that we have\n",
        "    for file in files:\n",
        "        print(file)\n",
        "\n",
        "        # open file and parse raw text\n",
        "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
        "            raw_text = f.readlines()\n",
        "\n",
        "        assert len(raw_text) == 3\n",
        "                    \n",
        "        # Omit first 14 symbols, which contain structure defintion [OCR_toInput]\n",
        "        # optional arg to remove last breakline\n",
        "        if ocr_last_breakline_remove:\n",
        "            aligned_ocr = raw_text[1][14:-1]\n",
        "        else:\n",
        "            aligned_ocr = raw_text[1][14:]\n",
        "        aligned_gs = raw_text[2][14:]\n",
        "\n",
        "        # length of the aligned texts should be the same\n",
        "        assert len(aligned_ocr) == len(aligned_gs)\n",
        "\n",
        "        # initialise variables that we keep on a file level\n",
        "        # we will aggregate on a file level then add to the larger list\n",
        "        file_aligned_words = []\n",
        "        file_words = []\n",
        "        file_aligned_gs_words = []\n",
        "        file_labels = []\n",
        "        \n",
        "        # Getting sentence spans from the aligned OCRed text\n",
        "        sentence_spans = sentence_tokenizer.span_tokenize(aligned_ocr)\n",
        "\n",
        "        # loop through the sentence spans; has a (start_index, end_index) for each         \n",
        "        for sentence_start, sentence_end in sentence_spans:\n",
        "\n",
        "            # get corresponding sentences in aligned ocr and gs\n",
        "            sentence = aligned_ocr[sentence_start: sentence_end]\n",
        "            gs_sentence = aligned_gs[sentence_start: sentence_end]\n",
        "            # these should be the same length\n",
        "            assert len(gs_sentence) == len(sentence)\n",
        "            \n",
        "            # initialise variables for a sentence level; will aggregate these to the file level\n",
        "            sentence_aligned_words = []\n",
        "            sentence_words = []\n",
        "            sentence_aligned_gs_words = []\n",
        "            sentence_labels = []\n",
        "            \n",
        "            # Getting space indices between the tokens, which are used to get the \n",
        "            # word spans. Here a word may contain more than one token, if the \n",
        "            # GS on this span does not have the same split words\n",
        "            new_ocr_space_ids = get_common_space_ids(sentence, gs_sentence)\n",
        "            \n",
        "            word_start = 0\n",
        "            # Traversing through the space ids (indexes of where common spaces are)\n",
        "            for space_id in new_ocr_space_ids:\n",
        "                word = sentence[word_start: space_id] # get word up to space id\n",
        "\n",
        "    #           If there was a sequence of spaces, the resulting word is empty. We omit it\n",
        "                if len(word) == 0:\n",
        "                    word_start = space_id + 1 \n",
        "                    continue\n",
        "                \n",
        "                # trim the word; remove '@' symbol\n",
        "                trimmed_word = word.replace(\"@\", \"\")\n",
        "                gs_word = gs_sentence[word_start: space_id] # get corresponding gs word\n",
        "                label = int(word != gs_word) # see if the ocr word is infact the word we are looking for\n",
        "                \n",
        "                # append word to sentence \n",
        "                sentence_labels.append(label)\n",
        "                sentence_aligned_words.append(word)\n",
        "                sentence_words.append(trimmed_word)\n",
        "                sentence_aligned_gs_words.append(gs_word) \n",
        "                \n",
        "                word_start = space_id + 1\n",
        "\n",
        "            # append sentences to file  \n",
        "            file_aligned_words.append(sentence_aligned_words)\n",
        "            file_words.append(sentence_words)\n",
        "            file_aligned_gs_words.append(sentence_aligned_gs_words)\n",
        "            file_labels.append(sentence_labels)\n",
        "            \n",
        "        # extend file to global; several sentences of individual files\n",
        "        aligned_words.extend(file_aligned_words)\n",
        "        words.extend(file_words)\n",
        "        aligned_gs_words.extend(file_aligned_gs_words)\n",
        "        labels.extend(file_labels)\n",
        "\n",
        "    return aligned_words, words, aligned_gs_words, labels\n",
        "\n"
      ],
      "metadata": {
        "id": "3t9lV5Vnr_Ul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_files = glob.glob(path.join(r\"./training_18M_without_Finnish/*\", \"*\", \"*.txt\"))\n",
        "aligned_words, words, aligned_gs_words, labels = extract_dataset(train_files, False)"
      ],
      "metadata": {
        "id": "8Dvxbd_AGlrw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "After some exploration; we realize some pieces of text are too noisy for learning.\n",
        "Contain too many typos, unrecognized words or false alarms. Need to eliminate.\n",
        "In order to be suitable, enough t omeasure edit distance between OCR'd word and\n",
        "GS sentences. If very large, then sentence is a bad material to learn.\n",
        "\"\"\"\n",
        "\n",
        "sent_stat = pd.DataFrame({\"ocr_sentence\": aligned_words, \"gs_sentence\": aligned_gs_words})\n",
        "sent_stat.head()"
      ],
      "metadata": {
        "id": "hLmpEBg0HJZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_sent_edit_distance(x):\n",
        "    '''Compute sentence edit distance normalized by the length of the sentence.'''\n",
        "    ocr_sent = \"\".join(x['ocr_sentence'])\n",
        "    gs_sent = \"\".join(x['gs_sentence'])\n",
        "    return editdistance.distance(ocr_sent, gs_sent) / max(len(ocr_sent), len(gs_sent))\n",
        "\n",
        "sent_stat[\"sent_edit_distance\"] = sent_stat.apply(compute_sent_edit_distance, axis=1)"
      ],
      "metadata": {
        "id": "zOoqfLQlMBCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# majority have edit distance less than 0.4\n",
        "sent_stat[\"sent_edit_distance\"].hist()"
      ],
      "metadata": {
        "id": "H2xY0sjnMIQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see which proportion of sentences fall into good and bad with threshold 0.4\n",
        "MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE = 0.4\n",
        "total_sent = sent_stat.shape[0]\n",
        "good_sent = (sent_stat[\"sent_edit_distance\"] <= MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE).sum()\n",
        "good_sent_ratio = good_sent / total_sent\n",
        "print(\"good sentences: %s\\ntotal sentences: %s\\ngood sentences ratio: %s\" % (good_sent, total_sent, good_sent_ratio))"
      ],
      "metadata": {
        "id": "GL3hrjv8Lt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# some examples of good sentences\n",
        "good_sentences_stat = sent_stat[sent_stat[\"sent_edit_distance\"] <= MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE]\n",
        "i = random.randint(0, good_sentences_stat.shape[0] - 1)\n",
        "\n",
        "good_sentences_stat[i: i + 5].sort_values(\"sent_edit_distance\", ascending=False)"
      ],
      "metadata": {
        "id": "UyAb02xMNO7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# look at bad sentences\n",
        "noisy_sentences_stat = sent_stat[sent_stat[\"sent_edit_distance\"] > MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE]\n",
        "i = random.randint(0, noisy_sentences_stat.shape[0] - 1)\n",
        "\n",
        "noisy_sentences_stat[i: i + 5].sort_values(\"sent_edit_distance\", ascending=False)"
      ],
      "metadata": {
        "id": "yk1sJg5FQ2CP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numpy array the results; we do not save to pickle file as we will use directly below\n",
        "words = np.array(words, dtype=object)[good_sentences_stat.index.tolist()].tolist()\n",
        "labels = np.array(labels, dtype=object)[good_sentences_stat.index.tolist()].tolist()\n",
        "# pickle.dump(words, open(\"train_ed_filtered_words.pickle\", \"wb\"))\n",
        "# pickle.dump(labels, open(\"train_ed_filtered_labels.pickle\", \"wb\"))\n",
        "\n",
        "# do the same for test set\n",
        "test_files = glob.glob(path.join(r\"./evaluation_4M_without_Finnish/*\", \"*\", \"*.txt\"))\n",
        "test_aligned_words, test_words, test_aligned_gs_words, test_labels = extract_dataset(test_files, True)\n",
        "\n",
        "test_sent_stat = pd.DataFrame({\"ocr_sentence\": test_aligned_words, \"gs_sentence\": test_aligned_gs_words})\n",
        "test_sent_stat[\"sent_edit_distance\"] = test_sent_stat.apply(compute_sent_edit_distance, axis=1)\n",
        "test_sent_stat[\"sent_edit_distance\"].hist()\n",
        "print(\"good sentences: %s\\ntotal sentences: %s\\ngood sentences ratio: %s\" % ((test_sent_stat[\"sent_edit_distance\"] <= MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE).sum(), test_sent_stat.shape[0], (test_sent_stat[\"sent_edit_distance\"] < MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE).sum() / test_sent_stat.shape[0]))\n",
        "test_good_sentences_stat = test_sent_stat[test_sent_stat[\"sent_edit_distance\"] <= MAXIMUM_AVERAGE_EDIT_DISTANCE_RATE]\n",
        "\n",
        "eval_words = np.array(test_words, dtype=object)[test_good_sentences_stat.index.tolist()].tolist()\n",
        "eval_labels = np.array(test_labels, dtype=object)[test_good_sentences_stat.index.tolist()].tolist()\n",
        "# pickle.dump(eval_words, open(\"test_ed_filtered_words.pickle\", \"wb\"))\n",
        "# pickle.dump(eval_labels, open(\"test_ed_filtered_labels.pickle\", \"wb\"))"
      ],
      "metadata": {
        "id": "yLGqxyN0RhbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Training the Model!\n",
        "Utilise pretrained multilingual BERT; BERT output of each sub-token is plugged into \n",
        "convolutional layers and fully-connected layers to be classified. Model predictions\n",
        "of sub-tokens are merged into token-level predictions. If more than one sub-token of a \n",
        "token is predicted to be erroneous, then the toke nis erroneous.\n",
        "\"\"\"\n",
        "# check how many typos in the training dataset; approximately 50%. A balanced dataset\n",
        "# which is good for training a model\n",
        "pos_labels_count = sum(sum(sent_labels) for sent_labels in labels)\n",
        "total_labels_count = sum(len(sent_labels) for sent_labels in labels)\n",
        "print(f\"typo words: {pos_labels_count}, total: {total_labels_count}, typo_rate: {pos_labels_count / total_labels_count}\")\n"
      ],
      "metadata": {
        "id": "K5jDBiX5WgsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We have 10 languages in our dataset; s othe most suitable BERT tokenizer and\n",
        "pretrained model is bert-base-multilingual-cased.\n",
        "\"\"\"\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")"
      ],
      "metadata": {
        "id": "O637fqNjsUxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_preserve_labels(sentence, text_labels):\n",
        "    \"\"\" Tokenize sentence into BERT subtokens. As well as the text label\"\"\"\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    for word, label in zip(sentence, text_labels):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    # Adding CLS and SEP tokens\n",
        "    tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"]\n",
        "    labels = [0] + labels + [0] # add 0 on either side\n",
        "    return tokenized_sentence, labels"
      ],
      "metadata": {
        "id": "SufaRlbtuCRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Convert data into tensor format; must adjust length of sequences \n",
        "\"\"\"\n",
        "def truncate_or_pad(arr, max_sequence_length):\n",
        "    \"\"\" Truncate or pad the `arr` according the maximum sequence length\"\"\"\n",
        "    return arr[:max_sequence_length] + [tokenizer.pad_token_id] * (max_sequence_length - len(arr))\n"
      ],
      "metadata": {
        "id": "K42wAH7bwQel"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Prepare dataset by:\n",
        "1. Tokenize words using word piece tokenization from BERT\n",
        "2. Convert Tokens into ids\n",
        "3. align sequence lengths to max sequence length\n",
        "4. Create attention masks, where 0 means padding\n",
        "5. Convert token ids, labels and attention masks into tensors\n",
        "\"\"\"\n",
        "def prepare_dataset(words, labels, max_sequence_length=100):\n",
        "    \"\"\"Extract inputs, tags and masks tensors from the dataset\"\"\"\n",
        "\n",
        "    # tokenize words and labels; add CLS to beginning and SEP to end\n",
        "    tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(words, labels)]\n",
        "    \n",
        "    # separate to tokenized texts and tokenized labels\n",
        "    tokenized_texts, tokenized_labels = zip(*tokenized_texts_and_labels)\n",
        "\n",
        "    # convert tokens to ids and truncate/pad if needed (pads with 0's)\n",
        "    input_ids = np.array([truncate_or_pad(tokenizer.convert_tokens_to_ids(txt), max_sequence_length) for txt in tokenized_texts], dtype='long')\n",
        "    subtoken_labels = np.array([truncate_or_pad(sentence_labels, max_sequence_length) for sentence_labels in tokenized_labels], dtype='long')\n",
        "    \n",
        "    # create attention mask; what to focus on. 0 if 0's, 1 otherwise\n",
        "    attention_masks = [[int(i != 0) for i in ii] for ii in input_ids]\n",
        "\n",
        "    # convert token ids, labels, masks into tensors\n",
        "    inputs = torch.tensor(input_ids, dtype=torch.long)\n",
        "    tags = torch.tensor(subtoken_labels, dtype=torch.long)\n",
        "    masks = torch.tensor(attention_masks, dtype=torch.long)\n",
        "\n",
        "    return inputs, tags, masks\n"
      ],
      "metadata": {
        "id": "2qNli7KOwfyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQUENCE_LENGTH = 100\n",
        "tr_inputs, tr_tags, tr_masks = prepare_dataset(words, labels, MAX_SEQUENCE_LENGTH)\n",
        "val_inputs, val_tags, val_masks = prepare_dataset(eval_words, eval_labels, MAX_SEQUENCE_LENGTH)"
      ],
      "metadata": {
        "id": "CzdLV1gevHHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In our model, we use pretrained BERT embeddings model bert-base-multilingual-cased.\n",
        "BERT embeddings are supplied to conv layers with 4 kernel sizes (2,3,4,5) each\n",
        "with 32 filters. Then followed by max pooling, both conv and max pooling have stride=1.\n",
        "Which has effect of information exchange within n-grams.\n",
        "Finally, concatenated each kernel and fed to linear layer to produce final logits for binary \n",
        "classification. Equivalent technique to image segmentation.\n",
        "\"\"\"\n",
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, hidden_dropout_prob):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\n",
        "        self.dropout = nn.Dropout(hidden_dropout_prob)\n",
        "        self.kernel_1 = 2\n",
        "        self.kernel_2 = 3\n",
        "        self.kernel_3 = 4\n",
        "        self.kernel_4 = 5\n",
        "        self.embedding_dim = 768\n",
        "        self.out_size = 32\n",
        "        self.num_labels = 2\n",
        "\n",
        "        # Different paddings are used in order to preserve the original sequence lengths.\n",
        "        self.conv_1 = nn.Conv1d(self.embedding_dim, self.out_size, self.kernel_1, stride=1, padding=1)\n",
        "        self.pool_1 = nn.MaxPool1d(kernel_size=self.kernel_1, stride=1)\n",
        "\n",
        "        self.conv_2 = nn.Conv1d(self.embedding_dim, self.out_size, self.kernel_2, stride=1, padding=1)\n",
        "        self.pool_2 = nn.MaxPool1d(kernel_size=self.kernel_2, stride=1, padding=1)\n",
        "\n",
        "        self.conv_3 = nn.Conv1d(self.embedding_dim, self.out_size, self.kernel_3, stride=1, padding=2)\n",
        "        self.pool_3 = nn.MaxPool1d(kernel_size=self.kernel_3, stride=1, padding=1)\n",
        "\n",
        "        self.conv_4 = nn.Conv1d(self.embedding_dim, self.out_size, self.kernel_4, stride=1, padding=2)\n",
        "        self.pool_4 = nn.MaxPool1d(kernel_size=self.kernel_4, stride=1, padding=2)\n",
        "        \n",
        "        self.classifier = nn.Linear(self.out_size * 4, self.num_labels)\n",
        "    \n",
        "    def forward(self, input, attention_mask):\n",
        "        outputs = self.bert(input, attention_mask)\n",
        "        sequence_output = outputs[0] # batch_size, sequence_length, hidden_size\n",
        "        conv_input = sequence_output.permute(0, 2, 1) # batch_size, hidden_size, sequence_length\n",
        "        conv_output_1 = self.conv_1(conv_input)\n",
        "        conv_output_1 = torch.relu(conv_output_1)\n",
        "        pool_output_1 = self.pool_1(conv_output_1)\n",
        "        conv_output_2 = self.conv_2(conv_input)\n",
        "        conv_output_2 = torch.relu(conv_output_2)\n",
        "        pool_output_2 = self.pool_2(conv_output_2)\n",
        "        conv_output_3 = self.conv_3(conv_input)\n",
        "        conv_output_3 = torch.relu(conv_output_3)\n",
        "        pool_output_3 = self.pool_3(conv_output_3)\n",
        "        conv_output_4 = self.conv_4(conv_input)\n",
        "        conv_output_4 = torch.relu(conv_output_4)\n",
        "        pool_output_4 = self.pool_4(conv_output_4)\n",
        "        conc_pool_output = torch.cat((pool_output_1, pool_output_2, pool_output_3, pool_output_4), 1)  # batch_size, out_size * 4, sequence_length\n",
        "        conc_pool_output = conc_pool_output.permute(0, 2, 1)  # batch_size, sequence_length, out_size * 4\n",
        "        sequence_output = self.dropout(conc_pool_output)\n",
        "        logits = self.classifier(sequence_output)  # batch_size, sequence_length, 2\n",
        "        return logits"
      ],
      "metadata": {
        "id": "FggEWgFxxNjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activate GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "cUB_IvZj0Egb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare data loader for training & evaluation\n",
        "BATCH_SIZE = 32\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, pin_memory=True, num_workers=2, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "NLxigE_myA68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize model and sent to available device\n",
        "model = CNNModel(0.1)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "2NK8it0GyJws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Use AdamW optimizer with weight decay 0.01 for non-bias parameters.\n",
        "Because biases are not influenced by input data, so does not make sense\n",
        "to apply regulation to them.\n",
        "\"\"\"\n",
        "param_optimizer = list(model.named_parameters())\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if \"bias\" in n], 'weight_decay_rate': 0.01}, \n",
        "    {'params': [p for n, p in param_optimizer if \"bias\" not in n], 'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n",
        "optimizer = AdamW(\n",
        "    optimizer_grouped_parameters,\n",
        "    lr=3e-5,\n",
        "    eps=1e-8\n",
        ")"
      ],
      "metadata": {
        "id": "mNsJiIuEyQeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The BERT authors suggested having no more than 3 epochs for training, and the linear\n",
        "decay of learning rate. As our model always overfits on 3rd epoch.\n",
        "Also use clipping to prevent gradient explosion\n",
        "\"\"\"\n",
        "epochs = 3\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the linear learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "## Store the average loss and accuracy after each epoch so we can plot them.\n",
        "loss_values, validation_loss_values = [], []\n",
        "training_acc, val_acc = [], []\n",
        "best_valid_loss = float('inf')\n",
        "best_valid_acc = 0\n",
        "\n",
        "best_acc_output_dir = \"cnnmodel_best_acc_all\"\n",
        "best_loss_output_dir = \"cnnmodel_best_loss_all\"\n",
        "\n",
        "# Create output directory if needed\n",
        "os.makedirs(best_acc_output_dir, exist_ok=True)\n",
        "os.makedirs(best_loss_output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Best validation accuracy model location: %s\" % best_acc_output_dir)\n",
        "print(\"Best validation loss model location: %s\" % best_loss_output_dir)\n",
        "all_predictions = []\n",
        "all_true_labels = []\n",
        "all_masks = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch:{epoch}\")\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    # Put the model into training mode.\n",
        "    model.train()\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Training loop\n",
        "    true_labels = []\n",
        "    logits_list = []\n",
        "    label_ids_list = []\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, position=0, leave=True)):\n",
        "        # for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a backward pass.\n",
        "        model.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        # This will return the loss (rather than the model output)\n",
        "        # because we have provided the `labels`.\n",
        "        logits = model(b_input_ids, attention_mask=b_input_mask)\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Only keep active parts of the loss\n",
        "        active_loss = b_input_mask.view(-1) == 1\n",
        "        active_logits = logits.view(-1, model.num_labels)\n",
        "        active_labels = torch.where(\n",
        "            active_loss, b_labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(b_labels)\n",
        "        )\n",
        "        loss = loss_fct(active_logits, active_labels)\n",
        "\n",
        "        logits = logits.detach()\n",
        "        logits_list.append(logits)\n",
        "        true_labels.extend(b_labels)\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # track train loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Clip the norm of the gradient\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    \n",
        "    predictions = [list(p) for logits in logits_list for p in np.argmax(logits.to(\"cpu\").numpy(), axis=2)]\n",
        "    true_labels = [tl.to(\"cpu\").numpy() for tl in true_labels]\n",
        "    \n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    print(\"\\nTraining loss: {}\".format(avg_train_loss))\n",
        "                                  \n",
        "    b_input_mask_list = b_input_mask.cpu().numpy().tolist()\n",
        "    pred_tags = [p_i for p, l, a in zip(predictions, true_labels, b_input_mask_list)\n",
        "                                 for p_i, l_i, a_i in zip(p, l, a) if a_i]\n",
        "    valid_tags = [l_i for l, a in zip(true_labels, b_input_mask_list)\n",
        "                                  for l_i, a_i in zip(l, a) if a_i]\n",
        "\n",
        "    acc = accuracy_score(pred_tags, valid_tags)\n",
        "    print(\"Training Accuracy: {}\".format(acc))\n",
        "    training_acc.append(acc)\n",
        "\n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    model.eval()\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "    predictions, true_labels = [], []\n",
        "    for batch in valid_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_input_mask)\n",
        "        # Move logits and labels to CPU\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss()\n",
        "        # Only keep active parts of the loss\n",
        "        active_loss = b_input_mask.view(-1) == 1\n",
        "        active_logits = logits.view(-1, model.num_labels)\n",
        "        active_labels = torch.where(\n",
        "            active_loss, b_labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(b_labels)\n",
        "        )\n",
        "        loss = loss_fct(active_logits, active_labels).item()\n",
        "\n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        eval_loss += loss\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "        true_labels.extend(label_ids)\n",
        "\n",
        "    eval_loss = eval_loss / len(valid_dataloader)\n",
        "    validation_loss_values.append(eval_loss)\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "    b_input_mask_list = b_input_mask.cpu().numpy().tolist()\n",
        "    pred_tags = [p_i for p, l, a in zip(predictions, true_labels, b_input_mask_list)\n",
        "                                 for p_i, l_i, a_i in zip(p, l, a) if a_i]\n",
        "    valid_tags = [l_i for l, a in zip(true_labels, b_input_mask_list)\n",
        "                                  for l_i, a_i in zip(l, a) if a_i]\n",
        "    acc = accuracy_score(pred_tags, valid_tags)\n",
        "    all_predictions.append(predictions)\n",
        "    all_true_labels.append(true_labels)\n",
        "    all_masks.append(b_input_mask_list)\n",
        "    \n",
        "    print(\"Validation Accuracy: {}\".format(acc))\n",
        "    val_acc.append(acc)\n",
        "    \n",
        "    \n",
        "    if eval_loss < best_valid_loss:\n",
        "        best_valid_loss = eval_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, path.join(best_loss_output_dir, \"model\"))\n",
        "        tokenizer.save_pretrained(best_loss_output_dir)\n",
        "    \n",
        "    if acc > best_valid_acc:\n",
        "        best_valid_acc = acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            }, path.join(best_acc_output_dir, \"model\"))\n",
        "        tokenizer.save_pretrained(best_acc_output_dir)"
      ],
      "metadata": {
        "id": "xoPlh0eiyjTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o', label=\"training loss\")\n",
        "plt.plot(validation_loss_values, 'r-o', label=\"validation loss\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Learning curve\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bcYsaGrQ1S6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test Inference\n",
        "\"\"\"\n",
        "output_dir = \"cnnmodel_best_loss_all\"\n",
        "tokenizer1 = BertTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "model1 = CNNModel(0.1)\n",
        "checkpoint = torch.load(path.join(output_dir, \"model\"))\n",
        "model1.load_state_dict(checkpoint['model_state_dict'])\n",
        "# Copy the model to the GPU.\n",
        "model1.to(device)\n",
        "model1.eval()"
      ],
      "metadata": {
        "id": "y3ABrYnR1__q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"\"\"\n",
        "Eroxena, Does my soul make an impreillon upon my ryes ?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "E6OjQS_m3Lt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(test_sentence):\n",
        "\n",
        "    # prepare sentence; tokenize and create attention masks\n",
        "    tokenized_sentence = tokenizer1.encode(test_sentence)\n",
        "    attention_mask = np.zeros(100)\n",
        "    attention_mask[:len(tokenized_sentence)] = 1\n",
        "    tokenized_sentence += [0] * (100 - len(tokenized_sentence)) # zero pad end of tokenized sentence\n",
        "\n",
        "    # turn to tensors and load to GPU\n",
        "    input_ids = torch.tensor([tokenized_sentence]).cuda()\n",
        "    attention_mask = torch.tensor([attention_mask]).cuda()\n",
        "\n",
        "    # get model output\n",
        "    with torch.no_grad():\n",
        "        logits = model1(input_ids, attention_mask)\n",
        "    label_indices = np.argmax(logits.to('cpu').numpy(), axis=2)\n",
        "\n",
        "    # model predicts ids, convert these back to tokens\n",
        "    tokens = tokenizer1.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
        "\n",
        "    # join split tokens\n",
        "    new_tokens, new_labels = [], []\n",
        "    for token, label_idx in zip(tokens, label_indices[0]):\n",
        "\n",
        "        # if starts with #, then belongs to past word; append to previous; and skip two hashtag\n",
        "        if token.startswith(\"##\"):\n",
        "            new_tokens[-1] = new_tokens[-1] + token[2:] # add to previous; skip two hashtag\n",
        "            if not new_labels[-1] and label_idx:\n",
        "                new_labels[-1] = label_idx # set label accordingly\n",
        "        # not unknown; add to label and token index\n",
        "        else:\n",
        "            new_labels.append(label_idx)\n",
        "            new_tokens.append(token)\n",
        "\n",
        "    # print these out; stop at PAD\n",
        "    for token, label in zip(new_tokens, new_labels):\n",
        "        if token == \"[PAD]\":\n",
        "            break\n",
        "        print(\"{}\\t{}\".format(label, token))\n",
        "\n",
        "inference(test_sentence)"
      ],
      "metadata": {
        "id": "oIjYBvwk3NEg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xv7JezKGqOed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}