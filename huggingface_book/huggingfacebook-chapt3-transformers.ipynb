{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install bertviz","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:17.982969Z","iopub.execute_input":"2022-08-19T02:07:17.984628Z","iopub.status.idle":"2022-08-19T02:07:29.982460Z","shell.execute_reply.started":"2022-08-19T02:07:17.984532Z","shell.execute_reply":"2022-08-19T02:07:29.980941Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nfrom bertviz.transformers_neuron_view import BertModel\nfrom bertviz.neuron_view import show\n\nmodel_ckpt = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = BertModel.from_pretrained(model_ckpt)\n\ntext = \"time flies like an arrow\"\nshow(model, \"bert\", tokenizer, text, display_mode=\"light\", layer=0, head=8)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-19T02:07:29.984868Z","iopub.execute_input":"2022-08-19T02:07:29.985435Z","iopub.status.idle":"2022-08-19T02:07:37.057285Z","shell.execute_reply.started":"2022-08-19T02:07:29.985380Z","shell.execute_reply":"2022-08-19T02:07:37.056060Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# tokenise the text\ninputs = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False) \n# exclude [CLS] and [SEP] to keep things simple (add_special_tokens=False)\ninputs.input_ids","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.059730Z","iopub.execute_input":"2022-08-19T02:07:37.060532Z","iopub.status.idle":"2022-08-19T02:07:37.071411Z","shell.execute_reply.started":"2022-08-19T02:07:37.060493Z","shell.execute_reply":"2022-08-19T02:07:37.070070Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from torch import nn\nfrom transformers import AutoConfig\n\nconfig = AutoConfig.from_pretrained(model_ckpt)\n# create dense embeddings; all contain a non-zero value\ntoken_emb = nn.Embedding(config.vocab_size, config.hidden_size) \ntoken_emb","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.073375Z","iopub.execute_input":"2022-08-19T02:07:37.075004Z","iopub.status.idle":"2022-08-19T02:07:37.685495Z","shell.execute_reply.started":"2022-08-19T02:07:37.074916Z","shell.execute_reply":"2022-08-19T02:07:37.684402Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Each input ID will be mapped to one of 30,522 embedding vectors in nn.Embedding, with size 768. \n\nThe AutoConfig also stores additional metadata, such as label names used to format the model's predictions.\n\nNote: Token embeddings at this point are independent of their context. Subsequent attention layers will mix these token embeddings to disambiguate and inform the representation of each token with the content of its context.","metadata":{}},{"cell_type":"code","source":"# generate embeddings by feeding in input ids\ninputs_embeds = token_emb(inputs.input_ids)\ninputs_embeds.size() # [batch_size, seq_len, hidden_dim]","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.686884Z","iopub.execute_input":"2022-08-19T02:07:37.687366Z","iopub.status.idle":"2022-08-19T02:07:37.699180Z","shell.execute_reply.started":"2022-08-19T02:07:37.687325Z","shell.execute_reply":"2022-08-19T02:07:37.696414Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# create query, key and value vectors and calculate attention scores using dot product as similarity fn\nimport torch\nfrom math import sqrt\n\nquery = key = value = inputs_embeds\ndim_k = key.size(-1) # 768\nprint(dim_k)\n\n# transpose returns a transposed version; swap dimensions 1 and 2\n# perform batch matrix mul; require transposing key tensor to have [hidden_dim,seq_len]\n# then use matrix prduct to collect dot products in [seq_len, seq_len] matrix. bmm takes two batches and multiplies each matrix from first batch with corresponding\n# matrix in second batch\nscores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k) \n\nscores.size() # 5 x 5 attention scores per sample in the batch","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.700901Z","iopub.execute_input":"2022-08-19T02:07:37.701428Z","iopub.status.idle":"2022-08-19T02:07:37.716362Z","shell.execute_reply.started":"2022-08-19T02:07:37.701389Z","shell.execute_reply":"2022-08-19T02:07:37.714679Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Then we get query, key and value vectors by applying independent weight matrices $W_q$, $W_k$,$W_v$ to the embeddings. \n\nIn scaled dot-product attention, the dot products are scaled by the size of the embedding vectors so that we don't get too many large numbers during training that can cause the softmax we apply next to saturate.","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# apply softmax\nweights = F.softmax(scores, dim=-1)\nweights.sum(dim=-1) # should add upto 1","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.718855Z","iopub.execute_input":"2022-08-19T02:07:37.719372Z","iopub.status.idle":"2022-08-19T02:07:37.733729Z","shell.execute_reply.started":"2022-08-19T02:07:37.719329Z","shell.execute_reply":"2022-08-19T02:07:37.732515Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# finally, multiply attention weights by values\nattn_outputs = torch.bmm(weights, value)\nattn_outputs.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.735898Z","iopub.execute_input":"2022-08-19T02:07:37.737294Z","iopub.status.idle":"2022-08-19T02:07:37.746286Z","shell.execute_reply.started":"2022-08-19T02:07:37.737242Z","shell.execute_reply":"2022-08-19T02:07:37.744904Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"def scaled_dot_product_attention(query, key, value):\n    dim_k = query.size(-1) # num hidden states\n    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n    weights = F.softmax(scores, dim=-1)\n    return torch.bmm(weights, value)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.748523Z","iopub.execute_input":"2022-08-19T02:07:37.749470Z","iopub.status.idle":"2022-08-19T02:07:37.756444Z","shell.execute_reply.started":"2022-08-19T02:07:37.749418Z","shell.execute_reply":"2022-08-19T02:07:37.755124Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"If query and key are equal then a very large score will be assigned to identical words. In practice however, the meaning of a word is better informed by complementary words in the context than by identical words.. How to promote this behaviour?\n\nAllow the model to create different sets of vectors or query, key and value of a token by using three different linear projections to project our initial token vector into three different spaces.\n\n**Multi-headed attention**\n\nIn practice, self-attention applies three independent linear transformations to each embedding to generate query, key and value vectors. These project the embeddings and each projection carries its own set of learnable parameters, allowing the self-attention layer to focus on different semantic aspects of the sequence.\n\nThese multiple sets of linear projections, each an *attention head*. So we have *multi-headed attention layer*. Softmax of one head tends to focus on one aspect of similarity, so multiple heads allow the model to focus on several aspects at once, e.g. subject-verb interaction, or finding adjectives. So the model learns these relationships from the data. Resemblance to filters in convolutional neural networks, where a filter can be resposible for detecting faces and another one finding wheels of cards.","metadata":{}},{"cell_type":"code","source":"# code up a single attention head\n\nclass AttentionHead(nn.Module):\n    def __init__(self, embed_dim, head_dim):\n        super().__init__()\n        # three independent linear layers\n        self.q = nn.Linear(embed_dim, head_dim)\n        self.k = nn.Linear(embed_dim, head_dim)\n        self.v = nn.Linear(embed_dim, head_dim)\n    \n    def forward(self, hidden_state):\n        # each apply matmul to embedding vectors to produce tensors [batch_size, seq_len, head_dim]\n        # where head_dim is the number of dimensions we're projecting into\n        # in practice, head_dim is chosen as a multiple of embed_dim so the computation across each\n        # head is constant. E.g. BERT has 12 heads, so dimension of each head is 768/12=64\n        attn_outputs = scaled_dot_product_attention(\n            self.q(hidden_state), self.k(hidden_state), self.v(hidden_state)\n        )\n        return attn_outputs","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.758602Z","iopub.execute_input":"2022-08-19T02:07:37.759611Z","iopub.status.idle":"2022-08-19T02:07:37.771689Z","shell.execute_reply.started":"2022-08-19T02:07:37.759540Z","shell.execute_reply":"2022-08-19T02:07:37.770237Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# concatenate outputs of each attention head to get full multi-head attention layer\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        embed_dim = config.hidden_size\n        num_heads = config.num_attention_heads\n        head_dim = embed_dim // num_heads\n        self.heads = nn.ModuleList(\n            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n        )\n        self.output_linear = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, hidden_state):\n        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n        # final linear layer\n        # output tensor of shape [batch_size, seq_len, hidden_dim] suitable for feed-forward network downstream\n        x = self.output_linear(x) \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.773791Z","iopub.execute_input":"2022-08-19T02:07:37.774635Z","iopub.status.idle":"2022-08-19T02:07:37.792769Z","shell.execute_reply.started":"2022-08-19T02:07:37.774583Z","shell.execute_reply":"2022-08-19T02:07:37.790775Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# test with pre-loaded BERT config from prior\nmultihead_attn = MultiHeadAttention(config)\nattn_output = multihead_attn(inputs_embeds)\nattn_output.size() # works!","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.795351Z","iopub.execute_input":"2022-08-19T02:07:37.797324Z","iopub.status.idle":"2022-08-19T02:07:37.844980Z","shell.execute_reply.started":"2022-08-19T02:07:37.797254Z","shell.execute_reply":"2022-08-19T02:07:37.842757Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# use BertViz again to visalise attention for two different uses of word \"flies\"\nfrom bertviz import head_view\nfrom transformers import AutoModel\n\nmodel = AutoModel.from_pretrained(model_ckpt, output_attentions=True)\n\nsentence_a = \"time flies like an arrow\"\nsentence_b = \"fruit flies like a banana\"\n\nviz_inputs = tokenizer(sentence_a, sentence_b, return_tensors=\"pt\")\nattention = model(**viz_inputs).attentions\nsentence_b_start = (viz_inputs.token_type_ids == 0).sum(dim=1)\ntokens = tokenizer.convert_ids_to_tokens(viz_inputs.input_ids[0])\n\nhead_view(attention, tokens, sentence_b_start, heads=[8])","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:37.855001Z","iopub.execute_input":"2022-08-19T02:07:37.855512Z","iopub.status.idle":"2022-08-19T02:07:41.030140Z","shell.execute_reply.started":"2022-08-19T02:07:37.855473Z","shell.execute_reply":"2022-08-19T02:07:41.028338Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"visaulisation shows the token whose embedding gets updated (left) with every word attended to (right). Line intensity indicates strength of attention weights, with dark lines close to 1 and faint lines close to 0.\n\nOne thing we see is that visualisation weights are strongest between words that belong to the same sentence. We can also see how flies associates with arrow and time for sentence A and flies associates to fruit and banana for sentence b, showing how our model is able to distinguis the use of flies as a verb or noun depending n the context!!\n\n**Feed-Forward Layer**\n\nTwo-layer fully connected NN. Processes each embedding independently instead of the whole sequence as a single-vector, so often referred to as position-wise feed-forward layer, also one-dimensional convolution with kernel size of one. Rule of thumb from literature is for hidden size of first layer to be four times the size of the embeddings, and GELU activation function is most commonly used.","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n        self.gelu = nn.GELU()\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        \n    def forward(self, x):\n        x = self.linear_1(x)\n        x = self.gelu(x)\n        x = self.linear_2(x)\n        x = self.dropout(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.031801Z","iopub.execute_input":"2022-08-19T02:07:41.032293Z","iopub.status.idle":"2022-08-19T02:07:41.043765Z","shell.execute_reply.started":"2022-08-19T02:07:41.032252Z","shell.execute_reply":"2022-08-19T02:07:41.042372Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# note: FF layer such as nn.linear is usually applied to tensor of shape (batch_size, input_dim)\n# where it acts on each element of the batch dimension independently\n# true for any dimension except for last one; so we pass tensor of shape (batch_size, seq_len, hidden_dim)\n# layer applied to all token embeddings of the batch and sequence independently\nfeed_forward = FeedForward(config)\nff_outputs = feed_forward(attn_outputs)\nff_outputs.size() # have all ingredients to make a transformer encoder layer!","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.045406Z","iopub.execute_input":"2022-08-19T02:07:41.045955Z","iopub.status.idle":"2022-08-19T02:07:41.099620Z","shell.execute_reply.started":"2022-08-19T02:07:41.045905Z","shell.execute_reply":"2022-08-19T02:07:41.098272Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"**Layer Normalisation**\n\nNormalises each input in batch to have zero mean and unity variance. Skip connections pass a tensor to the next layer of the model without processing and add it to the processed layer. For normalisation, there are two main choices from literature:\n\n-  *Post layer normalisation*:\n    Places layer normalisation in between skip connections (after multi-headed attn). This is tricky to train from scratch as gradients can diverge, so often a concept known as learning rate warm-up is implemented, where the learning rate is gradually increased from small to some max value during training. \n-  *Pre layer normalisation*:\n    Most common arrangement; places layer normalisation within span of skip connection (includes inside skip connection before attn), tends to be more stable during training and usually does not require any learning rate warm up.\n    \nSkip connect the layer before attn and afterward.","metadata":{}},{"cell_type":"code","source":"# use second arrangement\nclass TransformerEncodingLayer(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.layer_norm_1 = nn.LayerNorm(config.hidden_size)\n        self.layer_norm_2 = nn.LayerNorm(config.hidden_size)\n        self.attention = MultiHeadAttention(config)\n        self.feed_forward = FeedForward(config)\n        \n    def forward(self, x):\n        # Apply layer norm, then copy input into query, key, value\n        hidden_state = self.layer_norm_1(x)\n        # Apply attention with skip-connection\n        x = x + self.attention(hidden_state)\n        # Apply feed-forward layer with skip connection\n        x = x + self.feed_forward(self.layer_norm_2(x))\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.101729Z","iopub.execute_input":"2022-08-19T02:07:41.103163Z","iopub.status.idle":"2022-08-19T02:07:41.113745Z","shell.execute_reply.started":"2022-08-19T02:07:41.103097Z","shell.execute_reply":"2022-08-19T02:07:41.112179Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# test with input embeddings\nencoder_layer = TransformerEncodingLayer(config)\ninputs_embeds.shape, encoder_layer(inputs_embeds).size()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.116552Z","iopub.execute_input":"2022-08-19T02:07:41.117407Z","iopub.status.idle":"2022-08-19T02:07:41.205890Z","shell.execute_reply.started":"2022-08-19T02:07:41.117114Z","shell.execute_reply":"2022-08-19T02:07:41.204510Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"markdown","source":"Woohoo! First transformer encoder layer from scratch!!! \n\nNote: Caveat with the way we set up encoders, they are invariant to the position of the tokens. Since the multi-headed attention is a fancy weighted sum, the information on token position is lost.\n\n**Positional Embeddings**\n\nIdea: augment token embeddings with position-dependent pattern of values arranged in a vector. Is a pattern is characteristic for each position, the attention heads and feed-forward layers can learn to incorporate positional information into their transformations.\n\nSeveral ways to achieve, one popular approach is to use a learnable pattern, especially when the pretraining dataset is sufficiently large. This works the same way as the token embeddings, but using the position index instead of token ID as input. So with that, an efficient way of encoding the token positions is learned during pretraining.\n\nCreate custom `Embeddings` module that combines a token embedding layer that projects the `input_ids` to a dense hidden state together with the positional embeddings that does the same for `position_ids`. Resulting embedding is simply the sum of both embeddings.","metadata":{}},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.token_embeddings = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=1e-12)\n        self.dropout = nn.Dropout()\n        \n    def forward(self, input_ids):\n        # create position IDs for input sequence\n        seq_length = input_ids.size(1)\n        position_ids = torch.arange(seq_length, dtype=torch.long).unsqueeze(0)\n        # create token and position embeddings\n        token_embeddings = self.token_embeddings(input_ids)\n        position_embeddings = self.position_embeddings(position_ids)\n        # combine token and position embeddings\n        embeddings = token_embeddings + position_embeddings\n        embeddings = self.layer_norm(embeddings)\n        embeddings = self.dropout(embeddings)\n        return embeddings","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.208173Z","iopub.execute_input":"2022-08-19T02:07:41.208728Z","iopub.status.idle":"2022-08-19T02:07:41.219346Z","shell.execute_reply.started":"2022-08-19T02:07:41.208664Z","shell.execute_reply":"2022-08-19T02:07:41.217421Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"embedding_layer = Embeddings(config)\nembedding_layer(inputs.input_ids).size()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.221137Z","iopub.execute_input":"2022-08-19T02:07:41.222438Z","iopub.status.idle":"2022-08-19T02:07:41.417463Z","shell.execute_reply.started":"2022-08-19T02:07:41.222387Z","shell.execute_reply":"2022-08-19T02:07:41.415790Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"So embedding layer now creates a single, dense embedding for each token! \n\nWhile learnable position embeddings are easy to implement and widely used, there are some alternatives:\n\n-  *Absolute positional representations*:\n    Transformers can use static patterns of modulated sine and cosine signals to encode the positions of the token. This works especially well when there are not large volumes of data available\n-  *Relative positional representations*: \n    Encode relative positions between tokens; can be set up by introducing a new relative embedding layer at the beginning, since the relative embedding changes for each token depending on where from the sequence we areattending to it. Instead, the attention mechanism is modified with additional terms that take the relative position between tokens into account. Models like DeBERTa use such represnetations.\n    \nBy combining the idea of absolute and relative positional represnetations, rotary positional embeddings achieve excellent results on many tasks; GPT-Neo is one such model example with rotary position embeddings.","metadata":{}},{"cell_type":"code","source":"# putting it all together; full transformer encoder combining embeddings with encoder layers\nclass TransformerEncoder(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.embeddings = Embeddings(config)\n        self.layers = nn.ModuleList([TransformerEncodingLayer(config) for _ in range(config.num_hidden_layers)])\n        \n    def forward(self, x):\n        x = self.embeddings(x)\n        for layer in self.layers:\n            x = layer(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.419074Z","iopub.execute_input":"2022-08-19T02:07:41.419491Z","iopub.status.idle":"2022-08-19T02:07:41.429082Z","shell.execute_reply.started":"2022-08-19T02:07:41.419457Z","shell.execute_reply":"2022-08-19T02:07:41.427373Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"encoder = TransformerEncoder(config)\nencoder(inputs.input_ids).size()","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:41.430918Z","iopub.execute_input":"2022-08-19T02:07:41.431518Z","iopub.status.idle":"2022-08-19T02:07:42.498449Z","shell.execute_reply.started":"2022-08-19T02:07:41.431481Z","shell.execute_reply":"2022-08-19T02:07:42.496988Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"We get a hidden state for each token in the batch! This output format makes the architecture very flexible, and can be adapted for various applications such as predicting missing tokens in masked language modeling or predicting start and end position of an answer in question answering.\n\n**Adding a Classification Head**\n\nHow to build a classification head like the one we had in Chapter 2.\n\nTransformer models are typically dvided to task-independent body and a task-specific head; this pattern emerges again in Chapter 4 when reviewing Transformer design patterns. So far we have the body, which provides a hidden state for each token, but we only need to make one prediction. Traditionally, the first token in such models is used for the prediction and we can attach a dropout and linear layer to make classification prediction.","metadata":{}},{"cell_type":"code","source":"class TransformerForSequenceClassification(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.encoder = TransformerEncoder(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        \n    def forward(self, x):\n        x = self.encoder(x)[:, 0, :] # select hidden state of [CLS] token\n        x = self.dropout(x)\n        x = self.classifier(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:42.500216Z","iopub.execute_input":"2022-08-19T02:07:42.501183Z","iopub.status.idle":"2022-08-19T02:07:42.509012Z","shell.execute_reply.started":"2022-08-19T02:07:42.501140Z","shell.execute_reply":"2022-08-19T02:07:42.507721Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"# we need to define how many classes we wish to predict before initialising our model\nconfig.num_labels = 3\nencoder_classifier = TransformerForSequenceClassification(config)\nencoder_classifier(inputs.input_ids).size() ","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:42.510953Z","iopub.execute_input":"2022-08-19T02:07:42.511755Z","iopub.status.idle":"2022-08-19T02:07:43.649983Z","shell.execute_reply.started":"2022-08-19T02:07:42.511709Z","shell.execute_reply":"2022-08-19T02:07:43.648310Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"So that's how we can combine the encoder with a task-specific head. Now, we can turn our attention to the decoder.\n\n**The Decoder**\n\nDifference between decoder and encoder is that the decoder has two attention sublayers:\n\n- *Masked multi-head self-attention layer*: Ensures tokens we generate at each timestep are only based on the past outputs and current tokens being predicted. Without this, the decoder could cheat during training by copying the target translations; masking the inputs ensures the task is not trivial\n- *Encoder-decoder attention layer*: Performs multi-head attention over output key and value vectors of the encoder stack, with intermediate representations of decoder acting as queries. So the encoder-decoder attention layer learns how to relate tokens from two different sequences, such as two different languages. The decoder has to access the encoder keys and values in each block.","metadata":{}},{"cell_type":"code","source":"# mask self-attention is to introduce a mask matrix with ones on lower diagonal and zeros above\nseq_len = inputs.input_ids.size(-1)\nmask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0) # lower triangular matrix\nprint(mask.shape)\nmask[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:43.652263Z","iopub.execute_input":"2022-08-19T02:07:43.652837Z","iopub.status.idle":"2022-08-19T02:07:43.666563Z","shell.execute_reply.started":"2022-08-19T02:07:43.652774Z","shell.execute_reply":"2022-08-19T02:07:43.664941Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# tensor.masked_fill() to prevent each attention head from peeking at future tokens\n# replace all zeros with negative infinity; guarantee that all attn weights are all zero once we take softmax over score\n# as e^-inf=0\nscores.masked_fill(mask==0, -float(\"inf\")) # so only focus on bottom left","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:43.668812Z","iopub.execute_input":"2022-08-19T02:07:43.669384Z","iopub.status.idle":"2022-08-19T02:07:43.679859Z","shell.execute_reply.started":"2022-08-19T02:07:43.669328Z","shell.execute_reply":"2022-08-19T02:07:43.678275Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# include masked behaviour in scaled dot-product attention earlier\ndef scaled_dot_product_attention(query, key, value, mask=None):\n    dim_k = query.size(-1)\n    scores = torch.bmm(query, key.transpose(1,2)) / sqrt(dim_k)\n    if mask is not None:\n        scores = scores.masked_fill(mask==0, float(\"-inf\"))\n    weights = F.softmax(scores, dim=-1)\n    return weights.bmm(value)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T02:07:17.963827Z","iopub.execute_input":"2022-08-19T02:07:17.965261Z","iopub.status.idle":"2022-08-19T02:07:17.980938Z","shell.execute_reply.started":"2022-08-19T02:07:17.965202Z","shell.execute_reply":"2022-08-19T02:07:17.979333Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Architectures\n\nThree main tree architectures (not cover all the different subtypes):\n- BERT\n- GPT\n- T5/BigBird\n\n### Encoder Branch\n\nFirst encoder-only transformer model was BERT, at the time it outperformed all SOTA on GLUE benchmark, which measures natural language understanding (NLU). BERT model and variants:\n- *BERT*: Trained with two objectives of predicting masked tokens in texts and if one text passage is likely to follow another - Masked language modeling (MLM) and next sentence prediction\n- *DistilBERT*: Knowledge distilation technique, achieves 97% of BERT performance with 40% less memory and 60% faster\n- *RoBERTa*: Further improve performance by modifying pretraining scheme. RoBERTa is trained longer and drops NSP taask\n- *XLM*: Different pretraining objectives, including autogregressive modellin; and pretraining including translation language modeling, achieving SOTA on several NLU benchmarks\n- *ALBERT*: Decouples token embedding dimension from hidden-dim; allowing embedding dimension to be small and saves parameters. Second, all layers share the same parameters which decreases number of effective parameters. Finally, NSP objective is replaced with sentence ordering; whether or not two consecutive sentences are swapped or not. So uses even fewer parameters\n- *DeBERTa*: Each token is represented as two vectors; one for content andthe other for relative position. Self-attn can better model dependency of nearby token pairs. Absolute position is important and is added just before softmax layer of token decoding head. First model to beat human baseline on SuperGLUE benchmark.\n\n### Decoder Branch\n\nExceptionally good at predicting next word in a sequence, this are used for text generation tasks. Progress fueled by using larger datasets and scaling language models to larger and larger sizes.\n- *GPT*: Novel and efficient transformer decoder architecture, and transfer learning. Pretrained by predicting next word based on previous, trained on BookCorpus and achieved great results on down-stream tasks such as classification\n- *GPT-2*: Upscaled GPT to produce longer sequences of text\n- *CTRL*: Adds \"control tokens\" to beginning of sequence, allowing the style of generated text to be controlled, allowing for diverse generation.\n- *GPT-3*: Upscale GPT-2 by factor of 100 to 175 billion parameters! After analysis of language models at different scales realise there is a power law of compute, dataset size and performance. Can generate realistic text passages, and exhibits few-shot learning capabilities e.g. translating text to code.\n- *GPT-Neo/GPT-J-6B*: Trained by EleutherAI, a researcher collective who aim to re-create and release GPT-3 scale models, which are on similar sizes to GPT series model and are competitive with OpenAI models.\n\n### Encoder-Decoder Branch\n\nSeveral encoder-decoder variants of transformer architecture that have novel applications across NLU and NLG (Natural language generation). \n- *T5*: Text-to-text tasks. All tasks are framed as sequence-to-sequence tasks, where adopting encoder-decoder architecture is natural. Decoder must generate label as a normal text instead of a class, uses the original Transformer architecture and trained using large cralwed C4 and masked language modelling. As well as SuperGLUE and translating them to text-to-text tasks. Largest 11 billion parameters yielded SOTA results\n- *BART*: Combines BERT and GPT within encoder-decoder architecture. Input sequences undergo several possible transformations, from simple masking to sentence permutation, token deletion and document rotation. Modified inputs are passed through encoder, and decoder must reconstruct original text. So is more flexible and can be used for NLU and NLG, achieving SOTA on both.\n- *M2M-100*: Translation model that can translate between any of 100 languages; allowing high-quality translation of rare and underrepresented languages. Model uses prefix tokens (similar to special [CLS]) to indicate source and target language\n- *BigBird*: Overcomes maximum content size which has quadratic memory scaling of attention. BigBird uses sparse form of attention that scales linearly, overcoming limit of 512 tokens to 4,096. Usefulin cases with long dependencies such as text summarization.\n\nAll the above models have pretrained checkpoints and can be fine-tuned to a use case with HuggingFace Transformers.\n\n## Conclusion\nIn this chapter we dived into transformers and self-attention, and added the necessary parts to build a transformer encoder layer with:\n- Embedding layers for tokens and positional information\n- Feed-forward layer to complement the attention heads\n- Classification head to model body to make predictions\n\nAlso we look at decoder of transformer architecture and review the most important model architectures. Next step we go beyond simple classification and build multilingual named entity recognition (NER) model!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}