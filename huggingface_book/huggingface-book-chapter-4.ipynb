{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Building a MultiLingual NER Tagger","metadata":{}},{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\nxtreme_subsets = get_dataset_config_names(\"xtreme\")\nprint(f\"XTREME has {len(xtreme_subsets)} configurations\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# many config names; 183. Narrow to those that start with \"PAN\"\npanx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\npanx_subsets[:3]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# code passed as suffix\nfrom datasets import load_dataset\n\nload_dataset(\"xtreme\", name=\"PAN-X.de\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realistic Swiss corpus: Sample German (de), French (fr), Italian (it) and English (en) corpora from PAN-X according to their spoken proportions. This creates a language imbalance very common in real-world datasets and so will simulate an imbalanced dataset, so we can seehow we can build a model that works across all languages.\n\nCreate a Python *defaultdict* that stores the language code as the key and PAN-X corpus of type DatasetDict as the value:","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\nfrom datasets import DatasetDict\n\nlangs = [\"de\", \"fr\", \"it\", \"en\"]\nfracs = [0.629, 0.229, 0.084, 0.059]\n# return a DatasetDict if a key doesn't exist\npanx_ch = defaultdict(DatasetDict)\n\nfor lang, frac in zip(langs, fracs):\n    # Load monolingual corpus\n    ds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n    # Shuffle and downsample each split according to spoken proportion\n    for split in ds:\n        panx_ch[lang][split] = (\n            # shuffle to not accidently bias dataset splits\n            # select allows to downsample each corpus according to values in fracs\n            ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows)))\n        )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\npd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"More German examples than all other languages combined, so can use it as a starting point for zero-shot cross-lingual transfer to French, Italian and English.","metadata":{}},{"cell_type":"code","source":"# inspect one of the examples in the German corpus\nelement = panx_ch[\"de\"][\"train\"][0]\nfor key, value in element.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Keys of example correspond to column names of an Arrow table, while the values denote the entries in each column. We see that *ner_tags* column corresponds to mapping of each entity to a class ID. Slightly cryptic to human eye, so create a new column with LOC, PER and ORG tags. \n\nFirst we use `features` attribute of Dataset object that specifies the underlying data types associated with each column.","metadata":{}},{"cell_type":"code","source":"for key, value in panx_ch[\"de\"][\"train\"].features.items():\n    print(f\"{key}: {value}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\nprint(tags)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def create_tag_names(batch):\n    # use int2str method encountered in chapt2 to create a new column in training set\n    # with class names for each tag\n    return {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n\npanx_de = panx_ch[\"de\"].map(create_tag_names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# look at how tokens and tags align for first example in training set\nde_example = panx_de[\"train\"][0]\npd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], [\"Tokens\", \"Tags\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\n# calculate frequencies of each entity across each split\nsplit2freqs = defaultdict(Counter)\nfor split, dataset in panx_de.items():\n    for row in dataset[\"ner_tags_str\"]:\n        for tag in row:\n            if tag.startswith(\"B\"):\n                tag_type = tag.split(\"-\")[1]\n                split2freqs[split][tag_type] += 1\npd.DataFrame.from_dict(split2freqs, orient=\"index\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Distributions of PER, LOC and ORG are roughly the same for each split; so validation and test sets should prove a good measure of NER tagger's ability to generalise.\n\n## MultiLingual Transformers\n\nMany architectures and training procedures as their monolingual counterparts, except that the corupus for pretraining consists of documents in many languages. Amazingly, our models are able to differentiate across languages for a number of downstream tasks, even being competitive in translation compared to monolingual models.\n\nTo measure the progress of cross-lingual transfer for NER, the CoNLL-2002 and CoNLL-2003 datasets are often used as a benchmark for English, Dutch, Spanish and German. Multilingual transformer models are usually evaluated in three different ways:\n\n- **en**: Fine-tune on English training data then evaluate on each language's test set\n- **each**: Fine-tune and evaluate on monolingual test data to measure per-language performance\n- **all**: Fine-tune on all the training data to evaluate on all on each language's test set\n\nWe will adopt a similar evaluation strategy for our NER task, but first need to select a model to evaluate. One of the first models for multilingual transformers was mBERT which uses the same architecture and pretraining objective as BERT but adds Wikipedia articles from many languages to pretraining corpus. Since then, mBERT has been superseded by XLM-RoBERTa, so that's the model we'll consider in this chapter. \n\nXLM-R is distinguised by the huge size of its pretraining corpus of Wikipedia dumps of each language and 2.5TB of Common Crawl data from the web. Compared to its predecessors, XLM-R provides a significant boost for low-resource languages like Burmese and Swahili.\n\nRoBERTA refers to the fact that pretraining approach is the samea s for monolingual RoBERTa models. Improving on BERT by removing next sentence prediction, also dropping language embeddings in XLM; using SentencePiece to tokenise raw texts directly. Also 250,000 tokens vs 55,000!\n\nXLM-R is a great coice for multilingual NLU tasks; next we'll explore how it can efficiently tokenise across many languages.\n\n## A Closer Look at Tokenisation","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nbert_model_name = \"bert-base-cased\"\nxlmr_model_name = \"xlm-roberta-base\" # uses SentencePiece trained on raw text of all 100 languages\nbert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\nxlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text = \"Jack Sparrow loves New York!\"\nbert_tokens = bert_tokenizer(text).tokens()\nprint(bert_tokens)\nxlmr_tokens = xlmr_tokenizer(text).tokens()\nprint(xlmr_tokens)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Difference in [CLS] and [SEP] tokens; as XLM-R uses <s> and </s> for start and end of a sequence. These tokens are added in the final stage of tokenization.\n\n### Tokenizer Pipeline\n\nHigh level view is one that transforms strings to integers, though if we take a closer look we usually have four steps:\n1. **Normalisation**: Make a raw string \"cleaner\". Common operations: Strip whitespace, remove accented characters; unicode normalisation also, lowercasing; or reducing the vocab size\n2. **Pretokenisation**: Splits text to smaller objects and gives upper bound to what tokens will be at the end of training. Pretokenizer will split text into \"words\" and final tokens will be parts of those words. Not always a good choice as splitting words can make sentences incoherent.\n3. **Tokeniser Model**: Applies subword splitting model on words; this part needs to be trained on corpus. Splits words into subwords to reduce size of vocabulary and reduce number of out-of-vocabulary tokens. Several algorithms exist, including BPE, Unigram and WordPiece. No longer have a list of strings but a list of integers\n4. **Postprocessing**: Final transformations, e.g. adding special tokens at beginning or end of input sequence of token indices, like [CLS], [SEP] etc.. To then feed into the model.\n\nSo comparing XLM-R and BERT, SentencePiece adds <s> and <\\s> instead of [CLS] and [SEP] in postprocessing. Look into SentencePiece tokenizer to understand more of what makes it special.\n    \n### SentencePiece Tokeniser\n    \nBased on Unigram subword segmentation, encodes each input text as sequence of Unicode characters. Useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation and that many languages do not have whitespace characters. Whitespace itself is assigned a unicode symbol so can detokenise without any ambiguities.","metadata":{}},{"cell_type":"code","source":"\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Lets see how we can encode our simple example in a form suitable for NER; first we load the pretrained model with token classification head. Although, we can build the transformer model ourselves!\n\n## Transformers for Named Entity Recognition\n\nBERT uses special CLS token to represent an entire sequence of text; this representation is fed through a fully connected or dense layer to output the distribution of all discrete label values. BERT and other encoder-only transformers take a similar approach forNER, except the representation of each individual input token is fed into the same fully connected layer to output the entity of the token. Thus, NER is often framed as *token classification* task. \n\nCan indicate ignored subwords with IGN. Can later propagate the predicted label of the first subword to subsequent subwords in postprocessing step. Can also have chosen to include the representation of \"##ista\" subword by assigning it a copy of the B-LOC label, but this violates IOB2 format. \n\nFortunately, all the architecture aspects we've seen in BERT carry over to XLM-R since its architecture is based on RoBERTa, which is identical to BERT! We can see how Transformers supports many other tasks with minor modifications\n\n## Anatomy of Transformers Model Class\n\nName convention <ModelName>For<Task>; or AutoModelFor<Task>.\n    \n### Bodies and Heads\n    \nLast layer is the model head; it is the part that is task-specific. The rest is the body, including the token embeddings and transformer layers which are task-agnostic. There are pure body models, like `BertModel` or `GPT2Model`. This separation of bodies and heads allows us to build a custom head for any task and mount it on top of a pretrained model.\n    \n### Creating a Custom Model for Token Classification\n    \nCustom token classification head for XLM-R; uses same architecture as RoBERTa, so will use RoBERTa as base model, but augmented with settings specific to XLM-R. ","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaConfig\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom transformers.models.roberta.modeling_roberta import RobertaModel\nfrom transformers.models.roberta.modeling_roberta import RobertaPreTrainedModel\n\n# require data structure to represent XLM-R NER tagger\n# need configuration object to initialise model and a forward() function to generate outputs\nclass XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n    config_class = XLMRobertaConfig\n    \n    def __init__(self, config):\n        \n        # initialise RobertaPreTrainedModel class\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        # load model body; add_pooling=False to ensure we return all the hidden states and note one associated with [CLS] token\n        self.roberta = RobertaModel(config, add_pooling_layer=False)\n        # set up classification head\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n        # load and initialise weights\n        self.init_weights\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n        # use model body to get encoder representations\n        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n        # apply classifier to encoder representation\n        sequence_output = self.dropout(outputs[0])\n        logits = self.classifier(sequence_output)\n        # calculate losses directly if we have labels\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n        # return model output object. Wrap in TokenClassifierOutput\n        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Loading a Custom Model","metadata":{}},{"cell_type":"code","source":"# provide label of each entity and mapping of each tag to an ID and vice-versa\nindex2tag = {idx: tag for idx, tag in enumerate(tags.names)}\ntag2index = {tag:idx for idx, tag in enumerate(tags.names)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# store these and tags.num_classes in AutoConfig\nfrom transformers import AutoConfig\n\n# holds blueprint of model architecture; usually pretrained model has one already, though if we want to modify\n# then we can load the configuration with parameters we would like to customise\nxlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# load model weights as usual with from_pretrained() with additional config argument\n# we get these weights for free by inheriting from RobertaPreTrainedModel\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nxlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# quick check on initialising the tokeniser and model correctly\n# test the predictions on small sequence of known entities\ninput_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\npd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass inputs to model and extract probabilities by taking argmax to get most likely class per token\noutputs = xlmr_model(input_ids.to(device)).logits\npredictions = torch.argmax(outputs, dim=-1)\nprint(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\nprint(f\"Shape of outputs: {outputs.shape}\") # shape [batch_size, num_tokens, num_tags]; each token has a logit among seven possible NER tags","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see what pretrained model predicts\npreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\npd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"]) # unsurprisingly, random weights layer leaves a lot to be desired!","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# wrap preceding steps into a helper function for later use\ndef tag_text(text, tags, model, tokenizer):\n    # get tokens with special characters\n    tokens = tokenizer(text).tokens()\n    # encode sequence into IDs\n    input_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n    # get predictions as distribution over 7 possible classes\n    outputs = model(input_ids)[0]\n    # take argmax to get most likely class per token\n    predictions = torch.argmax(outputs, dim=2)\n    # convert to DataFrame\n    preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n    return pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Before we can train model, we also need to tokenize inputs and prepare labels. We'll do that next.\n\n### Tokenising Texts for NER\n\nTokenise the whole dataset so we can pass to XLM-R model for fune-tuning. We can use the map() function to achieve this.","metadata":{}},{"cell_type":"code","source":"# collect words and tags as ordinary lists\nwords, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tokenise each word and use the is_split_into_words argument to tell tokeniser that our input sequence has already been split into words\ntokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\ntokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\npd.DataFrame([tokens], index=[\"Tokens\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We want to mask the subword representations after the first subword, e.g. Einwohnern is broken into \"Einwohner\" and \"n\". Luckily, tokenized_input has a class that contains a words_ids() function to help us achieve this","metadata":{}},{"cell_type":"code","source":"word_ids = tokenized_input.word_ids()\npd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"word_ids has mapped each subword to corresponding index in words sequence; with the same word being mapped to the same index even if broken into multiple subwords. Also we see special tokens like <s//> mapped to `None`. Let's set -100 as the label for the special tokens and subwords we wish to mask during training:","metadata":{}},{"cell_type":"code","source":"previous_word_idx = None\nlabel_ids = []\n\nfor word_idx in word_ids:\n    if word_idx is None or word_idx == previous_word_idx:\n        label_ids.append(-100)\n    elif word_idx != previous_word_idx:\n        label_ids.append(labels[word_idx])\n    previous_word_idx = word_idx\n    \nlabels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\nindex = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n\npd.DataFrame([tokens, word_ids, label_ids, labels], index=index)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We select -100 as id, because PyTorch has cross-enropy loss that has an attribute called `ignore_index` whose value is -100; so this is ignored during training, and we use it here to ignore tokens associated with consecutive subwords.","metadata":{}},{"cell_type":"code","source":"# scale to whole dataset by defining a single function that wraps all the logic\ndef tokenize_and_align_labels(examples):\n    tokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    \n    labels = []\n    for idx, label in enumerate(examples[\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=idx)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            if word_idx is None or word_idx == previous_word_idx:\n                label_ids.append(-100)\n            else:\n                label_ids.append(label[word_idx])\n            previous_word_idx = word_idx\n        labels.append(label_ids)\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# write a function we can iterate over\n\ndef encode_panx_dataset(corpus):\n    return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode our German corpus\npanx_de_encoded = encode_panx_dataset(panx_ch[\"de\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"panx_de_encoded[\"train\"][\"labels\"][0][:15]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Performance Measures\n\nAll words of an entity need to be predicted correctly in order for a prediction to be counted as correct. We have a library called seqeval designed for these kinds of tasks; it can compute metrics via classification_report() function","metadata":{}},{"cell_type":"code","source":"!pip install seqeval","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import classification_report\n\ny_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\ny_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\nprint(classification_report(y_true, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`seqeval` expects the predictions and labels as lists of lists, each list corresponing to a single example in our validation or test sets. So we need to write a function that converts the output of our model into lists that `seqeval` expects.","metadata":{}},{"cell_type":"code","source":"import numpy as np\n\ndef align_predictions(predictions, label_ids):\n    preds = np.argmax(predictions, axis=2)\n    batch_size, seq_len = preds.shape\n    labels_list, preds_list = [], []\n    \n    for batch_idx in range(batch_size):\n        example_labels, example_preds = [], []\n        for seq_idx in range(seq_len):\n            # Ignore label IDs = -100\n            if label_ids[batch_idx, seq_idx] != -100:\n                example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])\n                example_preds.append(index2tag[preds[batch_idx][seq_idx]])\n                \n        labels_list.append(example_labels)\n        preds_list.append(example_preds)\n    return preds_list, labels_list","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now with a performance metric, we can move on to actually train the model.\n\n## Fine-Tuning XLM-RoBERTa\n\nFine-tune base model on German subset of PAN-X, then evaluate its zero-shot cross-lingual performance on French, Italian and English. We will use Transformers Trainer to handle training loop, so first need to define the training attributes using the TrainingArguments class.","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nnum_epochs = 3\nbatch_size = 24\nlogging_steps = len(panx_de_encoded[\"train\"]) // batch_size\nmodel_name = f\"{xlmr_model_name}-finetuned-panx-de\"\ntraining_args = TrainingArguments(\n    output_dir=model_name, log_level=\"error\", num_train_epochs=num_epochs,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    evaluation_strategy=\"epoch\",\n    save_steps=1e6, weight_decay=0.01, disable_tqdm=False,\n    logging_steps=logging_steps, push_to_hub=True\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from seqeval.metrics import f1_score\n\n# convert format to what is needed by seqeval to calculate f1 score\ndef compute_metrics(eval_pred):\n    y_pred, y_true = align_predictions(eval_pred.predictions, eval_pred.label_ids)\n    return {\"f1\": f1_score(y_true, y_pred)}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define data collector to pad each input sequence to largest sequence length in a batch\n# Huggingface Transformers provides a dedicated data collator for token classification that will pad labels along with inputs\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(xlmr_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Padding the labels is necessary, as unlike in text-classification, the labels are also sequences. Note here: The label sequences are padded with value `-100` which is ignored by PyTorch's loss functions.\n\nDefine a `model_init()` method to load an untrained model and is at the beginning of `train()` call:","metadata":{}},{"cell_type":"code","source":"def model_init():\n    return (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass information to Trainer\nfrom transformers import Trainer\n\ntrainer = Trainer(model_init=model_init, args=training_args, \n                  data_collator=data_collator, compute_metrics=compute_metrics,\n                 train_dataset=panx_de_encoded[\"train\"],\n                 eval_dataset=panx_de_encoded[\"validation\"],\n                 tokenizer=xlmr_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# F1 scores are quite good for NER model; to confirm it works as expected, test on German Translation of a simple example\ntext_de = \"Jeff Dean ist ein Informatiker bei Google in Kalifornien\"\ntag_text(text_de, tags, trainer.model, xlmr_tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Great! Though we shouldn't get too excited over a single example; time for a more proper and thorough analysis of the model's errors.\n\n## Error Analysis\n\nThere are several failure modes to bear in mind where it looks like the model is performing well, but in practice has some serious flaws. Some examples:\n\n- Accidentally mask toomany tokens and also masking some of our labels to get a really promising loss drop\n- `compute_metrics()` function might have a bug that overestimates the true performance\n- Might include zero class or 0 entity in NER as a normal class; which will heavily skew accuracy and F1-score since it is the majority class by a large margin\n\nWhen the model performs much worse than expected, looking at errors can yield useful insights and reveal bugs that would be hard to spot by just reviewing code. Even if the model performs well and there are no bugs, error analysis is a useful tool to understand the model's strengths and weaknesses and something we want to bear in mind when we deploy a model to a production environment.\n\nOne of the most powerful tools at our disposal is to look at validation examples with the highest loss, so now we can look at the loss per token in the same sequence.","metadata":{}},{"cell_type":"code","source":"# a method to apply to the validation set\n\nfrom torch.nn.functional import cross_entropy\n\ndef forward_pass_with_label(batch):\n    \n    # convert dict of lists to list of dicts suitable for data collator\n    features = [dict(zip(batch, t)) for t in zip(*batch.values())]\n    \n    # pad inputs and labels and put all tensors on device\n    batch = data_collator(features)\n    input_ids = batch[\"input_ids\"].to(device)\n    attention_mask = batch[\"attention_mask\"].to(device)\n    labels = batch[\"labels\"].to(device)\n    \n    with torch.no_grad():\n        # pass data through model\n        output = trainer.model(input_ids, attention_mask) # logit size: [batch_size, sequence_length, classes]\n        # predict class with largest logit value on classes axis\n        predicted_label = torch.argmax(output.logits, axis=-1).cpu().numpy()\n        \n    # calculate loss per token after flattening batch dimension with view\n    loss = cross_entropy(output.logits.view(-1, 7), labels.view(-1), reduction=\"none\")\n    # unflatten batch dimension and convert to numpy array\n    loss = loss.view(len(input_ids), -1).cpu().numpy()\n    \n    return {\"loss\": loss, \"predicted_label\": predicted_label}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply to whole validation set using map() and load into dataframe for further analysis\nvalid_set = panx_de_encoded[\"validation\"]\nvalid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)\ndf = valid_set.to_pandas()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# map back to strings from IDs to better read results; assign IGN to -100 labels. Also get rid of padding by truncating to length of inputs\nindex2tag[-100] = \"IGN\"\ndf[\"input_tokens\"] = df[\"input_ids\"].apply(lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))\ndf[\"predicted_label\"] = df[\"predicted_label\"].apply(lambda x: [index2tag[i] for i in x])\ndf[\"labels\"] = df[\"labels\"].apply(lambda x: [index2tag[i] for i in x])\ndf[\"loss\"] = df.apply(lambda x: x[\"loss\"][:len(x[\"input_ids\"])], axis=1)\ndf[\"predicted_label\"] = df.apply(lambda x: x[\"predicted_label\"][:len(x[\"input_ids\"])], axis=1)\n\ndf.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# we can unpack by using pandas.Series.explode() fn; creating a row for each element in original rows list\n# can do in parallel for all columns as all lists in one row have the same length\n# also drop padding tokens as their loss is 0 and cast losses to standard floats\ndf_tokens = df.apply(pd.Series.explode)\ndf_tokens = df_tokens.query(\"labels != 'IGN'\")\ndf_tokens[\"loss\"] = df_tokens[\"loss\"].astype(float).round(2)\ndf_tokens.head(7)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can groupby input tokens and aggregate the losses for each token with count, mean and sum\n# then sort by sum of losses and see which tokens have the most loss in val set\n(df_tokens\n    .groupby(\"input_tokens\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1) # rid of multi-level columns\n    .sort_values(by=\"sum\", ascending=False)\n    .reset_index()\n    .round(2)\n    .head(10)\n.T)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n- Whitespace token has highest total loss; unsurprising as it is the most common token in the list. However it's mean loss is much lower than other tokens in the list, so the model doesn't struggle to classify it\n- Words like \"in\", \"von\", \"und\" appear relatively frequently and appear together with named entities and sometimes part of them, which explains why model might get mixed up\n- Parentheses, slashes and capitals are rarer but havea relatively high average loss. Investigate these further","metadata":{}},{"cell_type":"code","source":"# can also group label ids and look at losses for each class\n(\n    df_tokens.groupby(\"labels\")[[\"loss\"]]\n    .agg([\"count\", \"mean\", \"sum\"])\n    .droplevel(level=0, axis=1)\n    .sort_values(by=\"mean\", ascending=False)\n    .reset_index()\n    .round(2)\n    .T\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"I-LOC has highest average loss; determining the location subwords is a challenge to our model, as well as B-ORD, the beginning of an organisation is a challenge to our model.","metadata":{}},{"cell_type":"code","source":"# go further by plotting confusion matrix where we see beginning of an organisation is often confused with subsequent I-ORG\n\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6,6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n    \nplot_confusion_matrix(df_tokens[\"labels\"], df_tokens[\"predicted_label\"], tags.names)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See confusiion between B-ORD and I-ORG; otherwise is quite good at classifying remaining entities which is clear by the near diagonal of confusion matrix.\n\nMove on from token level. Now look at sequences with high losses. Revisit \"unexploded\" DataFrame and calculate losses by summing over loss per token","metadata":{}},{"cell_type":"code","source":"# first write a function to help us display the token sequences with labels and losses\ndef get_samples(df):\n    for _, row in df.iterrows():\n        labels, preds, tokens, losses = [], [], [], []\n        for i, mask in enumerate(row[\"attention_mask\"]):\n            if i not in {0, len(row[\"attention_mask\"])}:\n                labels.append(row[\"labels\"][i])\n                preds.append(row[\"predicted_label\"][i])\n                tokens.append(row[\"input_tokens\"][i])\n                losses.append(f\"{row['loss'][i]:2f}\")\n        df_tmp = pd.DataFrame({\"tokens\":tokens, \"labels\":labels, \"preds\":preds, \"losses\":losses}).T\n        \n        yield df_tmp\n    \ndf[\"total_loss\"] = df[\"loss\"].apply(sum)\ndf_tmp = df.sort_values(by=\"total_loss\", ascending=False).head(3)\n\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Some of the labels are incorrect! Such as United Nations ... is labelled as a person! The annotations for PAN-X were generated through an automated processes, and are referred to as \"silver standard\" (vs \"gold standard\" of human-generated annotations) thus it is no surprise we see such fail cases of non-sensible labels. Even when humans annotate, mistakes can occur when the annotator misunderstands or loses concentration.\n\nLets also look at parentheses and slashes which had a relatively high loss.","metadata":{}},{"cell_type":"code","source":"df_tmp = df.loc[df[\"input_tokens\"].apply(lambda x: u\"\\u2581(\" in x)].head(2)\nfor sample in get_samples(df_tmp):\n    display(sample)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems to be parentheses in the way automatic extraction annotated the documents.. Also these contain geographic specifications. In Wikipedia articles, the titles often contain some explanation in parentheses which are important details to know when we roll out the model and have implications on downstream performance of whole pipeline the model is part of.\n\nSo we have identified some weaknesses in both model and dataset. In real use-case we would iterate on each, clean the dataset and retrain the model; then re-analyze new errors until we are satisfied with the performance.\n\nThough here we will move on, and look at performances across languages.\n\n## Cross-Lingual Transfer","metadata":{}},{"cell_type":"code","source":"# evaluate ability to transfer to other languages via predict() method of trainer\ndef get_f1_score(trainer, dataset):\n    return trainer.predict(dataset).metrics[\"test_f1\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# evaluate performance on test set and keep track of scores in a dict\nf1_scores = defaultdict(dict)\nf1_scores[\"de\"][\"de\"] = get_f1_score(trainer, panx_de_encoded[\"test\"])\nprint(f\"F1-score of [de] model on [de] dataset: {f1_scores['de']['de']:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see how German performes on French\ntext_fr = \"Jeff Dean est informaticien chez Google en Californie\"\ntag_text(text_fr, tags, trainer.model, xlmr_tokenizer)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate_lang_performance(lang, trainer):\n    panx_ds = encode_panx_dataset(panx_ch[lang])\n    return get_f1_score(trainer, panx_ds[\"test\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_scores[\"de\"][\"fr\"] = evaluate_lang_performance(\"fr\", trainer)\nprint(f\"F1-score of [de] model on [fr] dataset: {f1_scores['de']['fr']:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Drop of about 15 points. Remember, our model has not seen a single labeled French example. Generally, the size of performance drop is related to how \"far away\" the languages are from each other; Germanic and Romance languages are different families after all..","metadata":{}},{"cell_type":"code","source":"# look at Italian; also Romance language so similar to French result\nf1_scores[\"de\"][\"it\"] = evaluate_lang_performance(\"it\", trainer)\nprint(f\"F1-score of [de] model on [it] dataset: {f1_scores['de']['it']:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally english, which belongs to Germanic language family\nf1_scores[\"de\"][\"en\"] = evaluate_lang_performance(\"en\", trainer)\nprint(f\"F1-score of [de] model on [en] dataset: {f1_scores['de']['en']:.3f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Model fares *worst* on English though intuitively we expect German to be more similar to English than French. So let's next examine wen it makes sense to fine-tune directly on target language..\n\n### When Does Zero-Shot Transfer Make Sense?\n\nFine-tune on training sets of increasing size; track performance to determine at which point zero-shot cross-lingual transfer is superior; which in practice can be useful to guide decisions on whether to collect more labeled data. \n\nKeep hyperparameters from fine-tuning on German corpus, and tweak logging_steps argument of TrainingArguments to account for changing training set sizes. Wrap all in a simple function that takes a DatasetDict object corresponding to a monolingual corpus, downsample it by num_samples and fine-tunes XLM-R on that to return metrics from the best epoch:","metadata":{}},{"cell_type":"code","source":"def train_on_subset(dataset, num_samples):\n    train_ds = dataset[\"train\"].shuffle(seed=42).select(range(num_samples))\n    valid_ds = dataset[\"validation\"]\n    test_ds = dataset[\"test\"]\n    \n    training_args.logging_steps = len(train_ds) // batch_size\n    \n    trainer = Trainer(model_init=model_init, args=training_args,\n                     data_collator=data_collator, compute_metrics=compute_metrics,\n                     train_dataset=train_ds, eval_dataset=valid_ds, tokenizer=xlmr_tokenizer)\n    trainer.train()\n    \n    if training_args.push_to_hub:\n        trainer.push_to_hub(commit_message=\"Training completed!\")\n        \n    f1_score = get_f1_score(trainer, test_ds)\n    return pd.DataFrame.from_dict(\n        {\"num_samples\": [len(train_ds)], \"f1_score\": [f1_score]}\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# encode French corpus to input ids, attention masks and label ids\npanx_fr_encoded = encode_panx_dataset(panx_ch[\"fr\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test with small training set of 250 examples\ntraining_args.push_to_hub = False\nmetrics_df = train_on_subset(panx_fr_encoded, 250)\nmetrics_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Small dataset underperforms zero-shot from German by a large margin; see how results vary with increasing training set sizes:","metadata":{}},{"cell_type":"code","source":"for num_samples in [500, 1000, 2000, 4000]:\n    metrics_df = metrics_df.append(\n        train_on_subset(panx_fr_encoded, num_samples), ignore_index=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot f1 score on test set as function of increasing training set size:\n\nfig, ax = plt.subplots()\nax.axhline(f1_scores[\"de\"][\"fr\"], ls=\"--\", color=\"r\")\nmetrics_df.set_index(\"num_samples\").plot(ax=ax)\nplt.legend([\"Zero-shot from de\", \"Fine-tuned on fr\"], loc=\"lower right\")\nplt.ylim((0, 1))\nplt.xlabel(\"Number of Training Samples\")\nplt.ylabel(\"F1 Score\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Zero-shot transfer remains competitive until about 750 training examples, after which fine-tuning on French reaches a siilar level of performance as we had when fine-tuning on German. But this result is not to be laughed at! Getting labels can be pricey and zero-shot transfer learning can have a large business impact.\n\nOne final technique: fine-tuning on multiple languages at once!\n\n## Fine-Tuning on Multiple Languages at Once\n\nFine-tune on multiple languages at the same time to prevent drop in performance. First concatenate_datasets() from HuggingFace Datasets to concat the German and French corpora together.","metadata":{}},{"cell_type":"code","source":"from datasets import concatenate_datasets\n\ndef concatenate_splits(corpora):\n    multi_corpus = DatasetDict()\n    for split in corpora[0].keys():\n        multi_corpus[split] = concatenate_datasets(\n            [corpus[split] for corpus in corpora]).shuffle(seed=42)\n    return multi_corpus\n\npanx_de_fr_encoded = concatenate_splits([panx_de_encoded, panx_fr_encoded])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use same hyperparameters frm previous sections, so can just update logging steps, model and datasets in trainer\ntraining_args.logging_steps = len(panx_de_fr_encoded[\"train\"]) // batch_size\ntraining_args.push_to_hub = True\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-de-fr\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n                 data_collator=data_collator, compute_metrics=compute_metrics, \n                 tokenizer=xlmr_tokenizer, train_dataset=panx_de_fr_encoded[\"train\"],\n                 eval_dataset=panx_de_fr_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(commit_message=\"Training completed!\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# see how model performs on each test set of language\nfor lang in langs:\n    f1 = evaluate_lang_performance(lang, trainer)\n    print(f\"F1-score of [de-fr] model on [{lang}] dataset: {f1:.3f})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Much better performance on French, matching German. Also increases performance on Italian and English by roughly 10 points. So adding data in another language improves the model performance on unseen languages!\n\nFinalise by comparing performance of fine-tuning on each language separately against multilingual learning on all corpora. Can fine-tune on remaining languages with `train_on_subset()` with `num_samples` equal to number of examples in training set.","metadata":{}},{"cell_type":"code","source":"corpora = [panx_de_encoded]\n\n# exclude German from iteration\nfor lang in langs[1:]:\n    training_args.output_dir = f\"xlm-roberta-base-finetuned-panx-{lang}\"\n    # finetune on monolingual corpus\n    ds_encoded = encode_panx_dataset(panx_ch[lang])\n    metrics = train_on_subset(ds_encoded, ds_encoded[\"train\"].num_rows)\n    # collect F1 scores in common dict\n    f1_scores[lang][lang] = metrics[\"f1_score\"][0]\n    # add monolingual corpus to corpora to concatenate\n    corpora.append(ds_encoded)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Now concatenate all the splits to create a multilingual corpus of all four languages; use concatenate_splits() as previous.\ncorpora_encoded = concatenate_splits(corpora)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run familiar steps with the trainer\ntraining_args.logging_steps = len(corpora_encoded[\"train\"]) // batch_size\ntraining_args.output_dir = \"xlm-roberta-base-finetuned-panx-all\"\n\ntrainer = Trainer(model_init=model_init, args=training_args,\n                 data_collator=data_collator, compute_metrics=compute_metrics,\n                 tokenizer=xlmr_tokenizer, train_dataset=corpora_encoded[\"train\"],\n                 eval_dataset=corpora_encoded[\"validation\"])\n\ntrainer.train()\ntrainer.push_to_hub(\"Training completed!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally generate predictions on each language's test set\nfor idx, lang in enumerate(langs):\n    f1_scores[\"all\"][lang] = get_f1_score(trainer, corpora[idx][\"test\"])\n    \nscores_data = {\"de\": f1_scores[\"de\"],\n              \"each\": {lang: f1_scores[lang][lang] for lang in langs},\n              \"all\": f1_scores[\"all\"]}\nf1_scores_df = pd.DataFrame(scores_data).T.round(4)\nf1_scores_df.rename_axis(index=\"Fine-tune on\", columns=\"Evaluated on\", inplace=True)\n\nf1_scores_df","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A few conclusions:\n- Multilingual learning can provide significant gains on performance, especially if the low-resource language for cross-lingual transfer belong to similar language families - German, French and Italian achieve similar performance in all; suggesting that these languages are more similar to each other than English\n- As a general strategy, it is a good idea to focus attention on cross-lingual transfer within language families; especially when dealing with different scripts like Japanese\n\n## Conclusion\n\nWe saw NLP task on multilingual corpus using a single transforer pretrained on 100 languages: XLM-R. We were able to show cross-lingual transfer from German to French is competitive when a small nmber of labeled examples is available for fine-tuning, this good performance does not occur if target language is significantly different from one the base model was fine-tuned on or not one of the 100 languages used during pretraining. Recent proposals like MAD-X are designed for such low-resource scenarios, and since MAD-X is built on top of HuggingFace Transformers you can easily adapt the code to work with it.\n\nSo far we have looked at sequence classification and token classification which fall into the domain of natural language understanding, where text is synthesized into predictions; next we can look at text generation where the input and output is text.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}