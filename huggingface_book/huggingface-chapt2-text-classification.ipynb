{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import list_datasets\n\n# display all datasets\nall_datasets = list_datasets()\nprint(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\nprint(f\"The first 10 are: {all_datasets[:10]}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-12T14:30:37.119249Z","iopub.execute_input":"2022-08-12T14:30:37.120635Z","iopub.status.idle":"2022-08-12T14:30:39.807848Z","shell.execute_reply.started":"2022-08-12T14:30:37.120491Z","shell.execute_reply":"2022-08-12T14:30:39.806118Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\nemotions = load_dataset(\"emotion\")\nemotions","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:39.812104Z","iopub.execute_input":"2022-08-12T14:30:39.813668Z","iopub.status.idle":"2022-08-12T14:30:55.369222Z","shell.execute_reply.started":"2022-08-12T14:30:39.813609Z","shell.execute_reply":"2022-08-12T14:30:55.367763Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"train_ds = emotions[\"train\"]\ntrain_ds # contains a Dataset class","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.371017Z","iopub.execute_input":"2022-08-12T14:30:55.373182Z","iopub.status.idle":"2022-08-12T14:30:55.383194Z","shell.execute_reply.started":"2022-08-12T14:30:55.373136Z","shell.execute_reply":"2022-08-12T14:30:55.381403Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"len(train_ds)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.388042Z","iopub.execute_input":"2022-08-12T14:30:55.389315Z","iopub.status.idle":"2022-08-12T14:30:55.398936Z","shell.execute_reply.started":"2022-08-12T14:30:55.389272Z","shell.execute_reply":"2022-08-12T14:30:55.397332Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"train_ds[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.401363Z","iopub.execute_input":"2022-08-12T14:30:55.402514Z","iopub.status.idle":"2022-08-12T14:30:55.413183Z","shell.execute_reply.started":"2022-08-12T14:30:55.402473Z","shell.execute_reply":"2022-08-12T14:30:55.411455Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_ds.column_names","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.415432Z","iopub.execute_input":"2022-08-12T14:30:55.416368Z","iopub.status.idle":"2022-08-12T14:30:55.425412Z","shell.execute_reply.started":"2022-08-12T14:30:55.416324Z","shell.execute_reply":"2022-08-12T14:30:55.423844Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# can see the data types under the \n# value is a string\n# label is  ClassLabel that contains inforamtion about class names and mapping to integers\nprint(train_ds.features)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.427797Z","iopub.execute_input":"2022-08-12T14:30:55.428735Z","iopub.status.idle":"2022-08-12T14:30:55.437529Z","shell.execute_reply.started":"2022-08-12T14:30:55.428695Z","shell.execute_reply":"2022-08-12T14:30:55.435966Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# can access several rows via a slice\ntrain_ds[:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.440115Z","iopub.execute_input":"2022-08-12T14:30:55.441144Z","iopub.status.idle":"2022-08-12T14:30:55.453048Z","shell.execute_reply.started":"2022-08-12T14:30:55.441102Z","shell.execute_reply":"2022-08-12T14:30:55.451104Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# or just full columns by name\ntrain_ds[\"text\"][:5]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.455464Z","iopub.execute_input":"2022-08-12T14:30:55.456647Z","iopub.status.idle":"2022-08-12T14:30:55.484116Z","shell.execute_reply.started":"2022-08-12T14:30:55.456608Z","shell.execute_reply":"2022-08-12T14:30:55.482486Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Sidebar: Can also load in datasets from csv,txt and json files with `load_dataset` function, and pass `data_files` argument specifying the path/URL to one/more files. Can also specify the delimiter with `sep` and column names. Datasets documentation gives a more complete overview.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# convenient to convert to PD dataframe so we can access APIs for data viz\n# Datasets provides a set_format() that allows us to change the output format of the dataset\nemotions.set_format(type=\"pandas\")\ndf = emotions[\"train\"][:]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.490514Z","iopub.execute_input":"2022-08-12T14:30:55.490898Z","iopub.status.idle":"2022-08-12T14:30:55.531544Z","shell.execute_reply.started":"2022-08-12T14:30:55.490865Z","shell.execute_reply":"2022-08-12T14:30:55.530197Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# convert int labels to string labels using int2str() method\ndef label_int2str(row):\n    return emotions[\"train\"].features[\"label\"].int2str(row)\n\ndf[\"label_name\"] = df[\"label\"].apply(label_int2str)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.533811Z","iopub.execute_input":"2022-08-12T14:30:55.534696Z","iopub.status.idle":"2022-08-12T14:30:55.590523Z","shell.execute_reply.started":"2022-08-12T14:30:55.534636Z","shell.execute_reply":"2022-08-12T14:30:55.589112Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# piece of advice, \"becoming one with the data\" is an essential step for training great models\n# so we want to look at the distribution of the dataset generally\n# a skewed distribution may require different treatment in terms of metric & loss than an evenly distributed one\nimport matplotlib.pyplot as plt\n\ndf[\"label_name\"].value_counts(ascending=True).plot.barh()\nplt.title(\"Frequency of Classes\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.593017Z","iopub.execute_input":"2022-08-12T14:30:55.593862Z","iopub.status.idle":"2022-08-12T14:30:55.854127Z","shell.execute_reply.started":"2022-08-12T14:30:55.593803Z","shell.execute_reply":"2022-08-12T14:30:55.852880Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Ways to deal with imbalance:\n- Randomly oversample minority\n- Randomly undersample majority\n- Gather more labeled data from underrepresented class\n\nCan use imbalanced-learn library to aid in sampling techniques. Make sure to apply sampling methods *after* creating train/test splits to avoid leakage.","metadata":{}},{"cell_type":"code","source":"# transformer models have a maximum input sequence, referred to as maximum context size\n# for distilBert is 512 tokens, approx a few paragraphs of text\ndf[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\ndf.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\nplt.suptitle(\"\")\nplt.xlabel(\"\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:55.855993Z","iopub.execute_input":"2022-08-12T14:30:55.856779Z","iopub.status.idle":"2022-08-12T14:30:56.284272Z","shell.execute_reply.started":"2022-08-12T14:30:55.856730Z","shell.execute_reply":"2022-08-12T14:30:56.282894Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# tweets approx 15 words long. Longest tweets are well below DistilBERT's max context size\n# Longer texts need to be truncated, which may result in loss of performance as we may lose\n# crucial information\n\n# reset format as we don't need dataframe anymore; will begin working towards Transformers\nemotions.reset_format()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.286231Z","iopub.execute_input":"2022-08-12T14:30:56.287783Z","iopub.status.idle":"2022-08-12T14:30:56.295287Z","shell.execute_reply.started":"2022-08-12T14:30:56.287738Z","shell.execute_reply":"2022-08-12T14:30:56.293894Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Tokenisation: Breaking down a string into atomic units used in a model.","metadata":{}},{"cell_type":"code","source":"# Character Tokenisation; feed each character individually to the model\n# str objects are really arrays under the hood; so we can have character level\n# tokens in one line\ntext = \"Tokenizing text is a core task of NLP.\"\ntokenized_text = list(text)\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.297232Z","iopub.execute_input":"2022-08-12T14:30:56.298153Z","iopub.status.idle":"2022-08-12T14:30:56.314424Z","shell.execute_reply.started":"2022-08-12T14:30:56.298111Z","shell.execute_reply":"2022-08-12T14:30:56.312860Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# not done yet; model expects integers, a process called numericalisation\n# so encode each unique token with a unique integer\ntoken2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\nprint(token2idx) # get mapping of each character in vocab to an integer","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.316531Z","iopub.execute_input":"2022-08-12T14:30:56.317293Z","iopub.status.idle":"2022-08-12T14:30:56.329101Z","shell.execute_reply.started":"2022-08-12T14:30:56.317181Z","shell.execute_reply":"2022-08-12T14:30:56.327501Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"input_ids = [token2idx[token] for token in tokenized_text]\nprint(input_ids) # each token mapped to a unique numerical identifier","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.330669Z","iopub.execute_input":"2022-08-12T14:30:56.332231Z","iopub.status.idle":"2022-08-12T14:30:56.341890Z","shell.execute_reply.started":"2022-08-12T14:30:56.332192Z","shell.execute_reply":"2022-08-12T14:30:56.340305Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# last step to convert input_ids to 2d tensor of one-hot vectors\n# e.g. can map each name to a unique ID\ncategorical_df = pd.DataFrame(\n    {\"Name\":[\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]}\n)\ncategorical_df","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.343777Z","iopub.execute_input":"2022-08-12T14:30:56.345498Z","iopub.status.idle":"2022-08-12T14:30:56.361084Z","shell.execute_reply.started":"2022-08-12T14:30:56.345456Z","shell.execute_reply":"2022-08-12T14:30:56.359546Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# creates fictitious ordering between names; and NNs are really good at learning these kinds of relationships.\n# so instead create new col for each category and 1 where true, 0 where false (one-hot encode)\npd.get_dummies(categorical_df[\"Name\"]) # rows are one-hot vectors","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.363695Z","iopub.execute_input":"2022-08-12T14:30:56.364214Z","iopub.status.idle":"2022-08-12T14:30:56.379770Z","shell.execute_reply.started":"2022-08-12T14:30:56.364158Z","shell.execute_reply":"2022-08-12T14:30:56.378111Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Here we have the case where if we have two 1's, then can be interpreted as both tokens (e.g. both Bumblebee and Megatron) co-occur. We can create one-hot encodings in PyTorchby converting input_ids to tensor with `one_hot()` function","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ninput_ids = torch.tensor(input_ids)\n# important to set num_classes, otherwise may have vectors shorter than vocab length\n# and padded with zeros manually.\none_hot_encodings = F.one_hot(input_ids, num_classes = len(token2idx))\n\n# for each of 38 input tokens, we have a one-hot vector with 20 dimensions\n# since our vocabulary consists of 20 unique characters\none_hot_encodings.shape ","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:56.381444Z","iopub.execute_input":"2022-08-12T14:30:56.381805Z","iopub.status.idle":"2022-08-12T14:30:58.187279Z","shell.execute_reply.started":"2022-08-12T14:30:56.381776Z","shell.execute_reply":"2022-08-12T14:30:58.185835Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"print(f\"Token: {tokenized_text[0]}\")\nprint(f\"Tensor index: {input_ids[0]}\")\nprint(f\"One-hot: {one_hot_encodings[0]}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:58.189902Z","iopub.execute_input":"2022-08-12T14:30:58.190338Z","iopub.status.idle":"2022-08-12T14:30:58.198728Z","shell.execute_reply.started":"2022-08-12T14:30:58.190296Z","shell.execute_reply":"2022-08-12T14:30:58.197285Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"So we see character-level tokenisation ignores structure in text and treats string as a stream of characters. This helps deal with misspellings and rare words, but the draw back is we lose information on linguistic structures such as words needs to be learned from data. Requiring significant compute, memory and data. So character tokenisation is rare in practice. \n\nInstead, some structure of text is preserved during tokenisation, word tokenisation is straightofrward to achieve this.\n\n\n**Word Tokenization**\n\nNow model doesn't need to learn words from characters so reduces complexity of training process.","metadata":{}},{"cell_type":"code","source":"tokenized_text = text.split()\nprint(tokenized_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:58.200531Z","iopub.execute_input":"2022-08-12T14:30:58.201405Z","iopub.status.idle":"2022-08-12T14:30:58.220382Z","shell.execute_reply.started":"2022-08-12T14:30:58.201350Z","shell.execute_reply":"2022-08-12T14:30:58.218564Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"There are simply too many words to consider, so we often discard rare words and limit the covabulary. Unknown words are mapped to \"unknown\" and mapped to a shared UNK token. We may potentially lose some important information however. \n\nA compromise between word and character tokenisation is *subword tokenisation*\n\n**Subword Tokenisation**\n\nSubword tokenisation is learned from pre-training corpus using a mix of statistical rules and algorithms. There are several subword tokenisation algs used in NLP, like WordPiece, used by BERT and DistilBERT tokenisers. Transformers provides `AutoTokenizer` class to quickly load the tokenizer associated with a pretrained model, just call its `from_pretrained()` method and provide the ID of the model to load from its checkpoint.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_ckpt = \"distilbert-base-uncased\"\n# Autotokenizer belongs to \"auto\" classes which automatically retrieve model's config, pre-trained weights or vocab from ckpt\n# so we can quickly switch between models\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:30:58.221961Z","iopub.execute_input":"2022-08-12T14:30:58.222590Z","iopub.status.idle":"2022-08-12T14:31:05.416856Z","shell.execute_reply.started":"2022-08-12T14:30:58.222514Z","shell.execute_reply":"2022-08-12T14:31:05.415382Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizer\n\ndistilbert_tokenizer = DistilBertTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:05.419100Z","iopub.execute_input":"2022-08-12T14:31:05.419514Z","iopub.status.idle":"2022-08-12T14:31:07.660177Z","shell.execute_reply.started":"2022-08-12T14:31:05.419469Z","shell.execute_reply":"2022-08-12T14:31:07.658842Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"encoded_text = tokenizer(text)\nprint(encoded_text)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.662131Z","iopub.execute_input":"2022-08-12T14:31:07.662529Z","iopub.status.idle":"2022-08-12T14:31:07.683958Z","shell.execute_reply.started":"2022-08-12T14:31:07.662488Z","shell.execute_reply":"2022-08-12T14:31:07.682649Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\nprint(tokens)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.685680Z","iopub.execute_input":"2022-08-12T14:31:07.686652Z","iopub.status.idle":"2022-08-12T14:31:07.695472Z","shell.execute_reply.started":"2022-08-12T14:31:07.686611Z","shell.execute_reply":"2022-08-12T14:31:07.693902Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"-  We have `[CLS]` and `[SEP]` tokens at the start and end of the sentence, which indicate the start and end of a sequence\n-  Tokens have been lowercased\n-  We split tokenizing and NLP to two tokens (as they are not common words). The ## prefix means the preceding string is not whitespace, and that any token in this prefix should be merged with the previous token when converted back into a string. We can use `convert_tokens_to_string()` method for this:","metadata":{}},{"cell_type":"code","source":"print(tokenizer.convert_tokens_to_string(tokens))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.698058Z","iopub.execute_input":"2022-08-12T14:31:07.699245Z","iopub.status.idle":"2022-08-12T14:31:07.707016Z","shell.execute_reply.started":"2022-08-12T14:31:07.699035Z","shell.execute_reply":"2022-08-12T14:31:07.705289Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# other attributes\ntokenizer.vocab_size","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.717513Z","iopub.execute_input":"2022-08-12T14:31:07.718727Z","iopub.status.idle":"2022-08-12T14:31:07.728445Z","shell.execute_reply.started":"2022-08-12T14:31:07.718686Z","shell.execute_reply":"2022-08-12T14:31:07.726870Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"tokenizer.model_max_length","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.730923Z","iopub.execute_input":"2022-08-12T14:31:07.731877Z","iopub.status.idle":"2022-08-12T14:31:07.740887Z","shell.execute_reply.started":"2022-08-12T14:31:07.731812Z","shell.execute_reply":"2022-08-12T14:31:07.739533Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"tokenizer.model_input_names","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.743993Z","iopub.execute_input":"2022-08-12T14:31:07.744795Z","iopub.status.idle":"2022-08-12T14:31:07.754085Z","shell.execute_reply.started":"2022-08-12T14:31:07.744740Z","shell.execute_reply":"2022-08-12T14:31:07.752459Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Tip: When using pretrained model, it is *really* important to ensure the tokenizer is the same as the one the model was trained with. For a model, switching the tokenizer is like shuffling the vocabulary, so the model would have a hard time understanding what is going on.","metadata":{}},{"cell_type":"code","source":"# tokenise the whole dataset. Use `map()` method of DatasetDict\n# convenient way to apply processing fn to each element in dataset\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n\nprint(tokenize(emotions[\"train\"][:2]))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.755681Z","iopub.execute_input":"2022-08-12T14:31:07.757026Z","iopub.status.idle":"2022-08-12T14:31:07.768541Z","shell.execute_reply.started":"2022-08-12T14:31:07.756973Z","shell.execute_reply":"2022-08-12T14:31:07.767146Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"We see that zeros have been added to element to make them the same length. Zeros have a corresponding `[PAD]` token (0), and special tokens include `[CLS]` and `[SEP]` tokens encountered earlier.\n\nWe also get `attention_mask` arrays, to not confuse the model with additional padding tokens, telling the model to ignore the padded parts of the input.","metadata":{}},{"cell_type":"code","source":"# apply across all the splits in the corpus with a single line of code\nemotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:07.771706Z","iopub.execute_input":"2022-08-12T14:31:07.772374Z","iopub.status.idle":"2022-08-12T14:31:11.050099Z","shell.execute_reply.started":"2022-08-12T14:31:07.772329Z","shell.execute_reply":"2022-08-12T14:31:11.048829Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"`batch_size=None` means that tokenize applies to full dataset as a single batch, this ensures the input tensors and attention masks have the same shape globally. This operation has added new `input_ids` and `attention_mask` columns to the dataset.","metadata":{}},{"cell_type":"code","source":"print(emotions_encoded[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:11.052001Z","iopub.execute_input":"2022-08-12T14:31:11.052905Z","iopub.status.idle":"2022-08-12T14:31:11.063372Z","shell.execute_reply.started":"2022-08-12T14:31:11.052812Z","shell.execute_reply":"2022-08-12T14:31:11.061914Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### Training a Text Classifier\nModels like DistlBERT are pretrained to predict masked words in a sequence of text. We would thus need to modify these slightly for use in text classification. Looking at architecture:\n-  First, text is tokenised and made to one-hot vectors called *token encodings*. The size of the tokenizer vocabulary determines the dimension, usualy 20k-200k unique tokens\n-  Then these token encodings are converted to token embeddings, which are vectors in low-dimensional space\n-  Token embedings pass through encoder blocks to yield hidden state for each input token\n-  Hidden state is fed to a layer that predicts the masked input tokens\n-  For classification, we replace language modeling with classification layer\n\nSidebar: In practice PyTorch skips the step of creating one-hot vectors for token encodings as multiplying a matrix with one-hot vector is the same as selecting a column from the matrix\n\nSo Two options to train such a model on our Twitter dataset:\n\n-  *Feature Extraction*: Use hidden states as features and just train a classifier on them, without modifying the pretrained model\n-  *Fine-Tuning*: Train whole model end-to-end, also updating the parameters of the pretrained model\n\nWe explore both options for DistilBERT and examine the trade-offs.\n\n**Transformers as Feature Extractors**\n\nFreeze the body's weights during training and use hidden states as features for classifier. Pros: Can quickly train a small/shallow model, even a model that does not rely on gradients, such as random forest. And is more convenient e.g. if a GPU is unavailable, as hidden states only need to be precomputed once.","metadata":{}},{"cell_type":"code","source":"# use AutoModel, this has a from_pretrained() method to load weights of pretrained model\nfrom transformers import AutoModel\n\nmodel_ckpt = \"distilbert-base-uncased\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AutoModel.from_pretrained(model_ckpt).to(device) # chain model to GPU if we have one","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:11.065575Z","iopub.execute_input":"2022-08-12T14:31:11.066635Z","iopub.status.idle":"2022-08-12T14:31:43.545594Z","shell.execute_reply.started":"2022-08-12T14:31:11.066578Z","shell.execute_reply":"2022-08-12T14:31:43.544051Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Automodel converts token encodings to embeddings, then feeds through encoder stack to return hidden states\n# Retrieving the last hidden states\ntext = \"this is a test\"\ninputs = tokenizer(text, return_tensors=\"pt\")\n\n# has [batch_size, n_tokens]\nprint(f\"Input tensor shape: {inputs['input_ids'].size()}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:43.547973Z","iopub.execute_input":"2022-08-12T14:31:43.548866Z","iopub.status.idle":"2022-08-12T14:31:43.560902Z","shell.execute_reply.started":"2022-08-12T14:31:43.548800Z","shell.execute_reply":"2022-08-12T14:31:43.559439Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# place tensors on same device and pass the inputs to the model\ninputs = {k:v.to(device) for k, v in inputs.items()}\n\n# use no_grad to not calculate automatic gradient, reduces memory footprint\nwith torch.no_grad():\n    outputs = model(**inputs)\n\n# model config can contain several objects, such as hidden states, losses, attns..\n# here we just have instance of BaseModelOutput and can access its attributes by name\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:43.562606Z","iopub.execute_input":"2022-08-12T14:31:43.563801Z","iopub.status.idle":"2022-08-12T14:31:44.687449Z","shell.execute_reply.started":"2022-08-12T14:31:43.563769Z","shell.execute_reply":"2022-08-12T14:31:44.685873Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"outputs.last_hidden_state.size() # [batch_size, n_tokens, hidden_dim]","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.690031Z","iopub.execute_input":"2022-08-12T14:31:44.690915Z","iopub.status.idle":"2022-08-12T14:31:44.700304Z","shell.execute_reply.started":"2022-08-12T14:31:44.690843Z","shell.execute_reply":"2022-08-12T14:31:44.698915Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# common practice in text classification to just use hidden state associated with [CLS] as input feature\n# as this token appears at the start of each sequence\n# we can extract it at the start of each sequence by indexing into last hidden state\noutputs.last_hidden_state[:,0].size() # last hidden state for single string","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.702096Z","iopub.execute_input":"2022-08-12T14:31:44.705083Z","iopub.status.idle":"2022-08-12T14:31:44.715097Z","shell.execute_reply.started":"2022-08-12T14:31:44.705040Z","shell.execute_reply":"2022-08-12T14:31:44.713560Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"tokenizer.model_input_names","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.716992Z","iopub.execute_input":"2022-08-12T14:31:44.718682Z","iopub.status.idle":"2022-08-12T14:31:44.731134Z","shell.execute_reply.started":"2022-08-12T14:31:44.718642Z","shell.execute_reply":"2022-08-12T14:31:44.729704Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# build a mapping fn that extracts all hidden states in one go for entire dataset\n\ndef extract_hidden_states(batch):\n    # place model inputs on GPU\n    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n    \n    # extract lst hidden states\n    with torch.no_grad():\n        last_hidden_state = model(**inputs).last_hidden_state\n        \n    # return vector for [CLS] token\n    return {\"hidden_state\": last_hidden_state[:, 0].cpu().numpy()}\n        ","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.733090Z","iopub.execute_input":"2022-08-12T14:31:44.734084Z","iopub.status.idle":"2022-08-12T14:31:44.742486Z","shell.execute_reply.started":"2022-08-12T14:31:44.734043Z","shell.execute_reply":"2022-08-12T14:31:44.741105Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# model expects tensors as inputs, so convert input_ids and attn masks to \"torch\" format\nemotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.744523Z","iopub.execute_input":"2022-08-12T14:31:44.745374Z","iopub.status.idle":"2022-08-12T14:31:44.758161Z","shell.execute_reply.started":"2022-08-12T14:31:44.745333Z","shell.execute_reply":"2022-08-12T14:31:44.756645Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# extract hidden states across all splits in one go\nemotions_hidden = emotions_encoded.map(extract_hidden_states, batched=True) # default batchsize=1000 is used","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:31:44.759720Z","iopub.execute_input":"2022-08-12T14:31:44.760605Z","iopub.status.idle":"2022-08-12T14:32:09.811516Z","shell.execute_reply.started":"2022-08-12T14:31:44.760555Z","shell.execute_reply":"2022-08-12T14:32:09.810129Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"emotions_hidden[\"train\"].column_names # a new exract_hidden_states() adds a new \"hidden_state\" column to our dataset","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:32:09.813406Z","iopub.execute_input":"2022-08-12T14:32:09.813856Z","iopub.status.idle":"2022-08-12T14:32:09.827802Z","shell.execute_reply.started":"2022-08-12T14:32:09.813793Z","shell.execute_reply":"2022-08-12T14:32:09.826270Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# create a feature matrix to train a classifier\nimport numpy as np\nX_train = np.array(emotions_hidden[\"train\"][\"hidden_state\"])\nX_valid = np.array(emotions_hidden[\"validation\"][\"hidden_state\"])\ny_train = np.array(emotions_hidden[\"train\"][\"label\"])\ny_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\nX_train.shape, X_valid.shape","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:32:09.832132Z","iopub.execute_input":"2022-08-12T14:32:09.832614Z","iopub.status.idle":"2022-08-12T14:32:10.115507Z","shell.execute_reply.started":"2022-08-12T14:32:09.832559Z","shell.execute_reply":"2022-08-12T14:32:10.114239Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# briefly visualise the training set, we can use UMAP to project to 2D\n# works best if we scale to [0,1] first souse MinMaxScaler\nfrom umap import UMAP\nfrom sklearn.preprocessing import MinMaxScaler\n\n# scale features to [0,1] range\nX_scaled = MinMaxScaler().fit_transform(X_train)\n# initialise and fit UMAP\nmapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n# create dataframe of 2D embedings\ndf_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\ndf_emb[\"label\"] = y_train\ndf_emb.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:32:10.117532Z","iopub.execute_input":"2022-08-12T14:32:10.117972Z","iopub.status.idle":"2022-08-12T14:33:14.478654Z","shell.execute_reply.started":"2022-08-12T14:32:10.117944Z","shell.execute_reply":"2022-08-12T14:33:14.477062Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# so we reduce 768 features to 2. Now we can plot the density of the points for each category\nfig, axes = plt.subplots(2,3, figsize=(7,5))\naxes = axes.flatten()\ncmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\nlabels = emotions[\"train\"].features[\"label\"].names\n\nfor i, (label, cmap) in enumerate(zip(labels, cmaps)):\n    df_emb_sub = df_emb.query(f\"label == {i}\")\n    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap, gridsize=20, linewidths=(0,))\n    axes[i].set_title(label)\n    axes[i].set_xticks([]), axes[i].set_yticks([])\n    \nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:33:14.480804Z","iopub.execute_input":"2022-08-12T14:33:14.481369Z","iopub.status.idle":"2022-08-12T14:33:14.997905Z","shell.execute_reply.started":"2022-08-12T14:33:14.481323Z","shell.execute_reply":"2022-08-12T14:33:14.996030Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Note: These are only projections in 2D space. Overlap does not mean they are not separable in original space. Conversely, if they are separable in the projected space then they will be separable in the original space.\n\nWe see that negative emotions: Sadness, anger, fear occupy similar regions with similar distributions. On the other hand, joy and love are well separated from the negative emotions and share a similar space. Surprise is scattered all over the place. Separation is not guaranteed as model was not trained to distinguish, only learned by implicitly guessing masked words in texts.\n\n**Training a simple classifier**","metadata":{}},{"cell_type":"code","source":"# use logistic regression, simple and does not require GPU\nfrom sklearn.linear_model import LogisticRegression\n\n# increase max_iter to guarantee convergence\nlr_clf = LogisticRegression(max_iter=3000)\nlr_clf.fit(X_train, y_train)\nlr_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:33:14.999761Z","iopub.execute_input":"2022-08-12T14:33:15.000300Z","iopub.status.idle":"2022-08-12T14:39:04.825144Z","shell.execute_reply.started":"2022-08-12T14:33:15.000259Z","shell.execute_reply":"2022-08-12T14:39:04.823209Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"As we are dealing with an unbalanced multiclass dataset, it is actually significantly better. We can have a dummyclassifier as a baseline which is basedon simple heuristics such as choosing the majority class, or always drawing on a random class.","metadata":{}},{"cell_type":"code","source":"from sklearn.dummy import DummyClassifier\n\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\ndummy_clf.fit(X_train, y_train)\ndummy_clf.score(X_valid, y_valid)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:39:04.833046Z","iopub.execute_input":"2022-08-12T14:39:04.837864Z","iopub.status.idle":"2022-08-12T14:39:04.863742Z","shell.execute_reply.started":"2022-08-12T14:39:04.837755Z","shell.execute_reply":"2022-08-12T14:39:04.861575Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"We can investigate performance by looking at confusion matrix of the classifier, which tells us the relationship between true and predicted labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n\ndef plot_confusion_matrix(y_preds, y_true, labels):\n    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n    fig, ax = plt.subplots(figsize=(6, 6))\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n    plt.title(\"Normalized confusion matrix\")\n    plt.show()\n    \ny_preds = lr_clf.predict(X_valid)\nplot_confusion_matrix(y_preds, y_valid, labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:39:04.865697Z","iopub.execute_input":"2022-08-12T14:39:04.866649Z","iopub.status.idle":"2022-08-12T14:39:05.705123Z","shell.execute_reply.started":"2022-08-12T14:39:04.866609Z","shell.execute_reply":"2022-08-12T14:39:05.703843Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"We see anger and fear are often confused with sadness, which aligns with the previous 2D observations of the embeddings. Also love and surprise are frequently mistaken for joy.\n\nWe will fine-tune now which leads to superior classification perofrmance, but may require more computational resources, ie. GPUs. If not available then such traditional and feature-based ML is a good compromise.\n\n### Fine-Tuning Transformers\n\nThis requires the classification head to be differentiable, therefore a NN is preferable for classification. In fine-tuning the entire model, the hidden states adapt during training to decrease total model loss and increase performance.","metadata":{}},{"cell_type":"code","source":"# Sequence classification automodel has a classification head on top of pretrained model outputs so can easily be trained with base\n# just need to specify number of labels the classification head has\nfrom transformers import AutoModelForSequenceClassification\n\nnum_labels = 6\nmodel = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device))","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:39:07.883940Z","iopub.execute_input":"2022-08-12T14:39:07.884449Z","iopub.status.idle":"2022-08-12T14:39:12.323638Z","shell.execute_reply.started":"2022-08-12T14:39:07.884402Z","shell.execute_reply":"2022-08-12T14:39:12.322215Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# define performance metrics\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndef compute_metrics(pred):\n    \"\"\"Returrns EvalPrediction object (named tuple with predictions and label_ids attributes)\n    and needs toreturn a dictionary that maps each metric's name to its value. Here we have\n    F1-Score and model accuracy.\"\"\"\n    labels = pred.label_ids\n    pred = pred.predictions.argmax(-1)\n    f1 = f1_score(labels, pred, average=\"weighted\")\n    acc = accuracy_score(labels, pred)\n    return {\"accuracy\": acc, \"f1\": f1}","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:53:17.538960Z","iopub.execute_input":"2022-08-12T14:53:17.539379Z","iopub.status.idle":"2022-08-12T14:53:17.553835Z","shell.execute_reply.started":"2022-08-12T14:53:17.539347Z","shell.execute_reply":"2022-08-12T14:53:17.552054Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"# log into huggingface hub so we can push our fine-tuned model to hub account and share with community\nfrom huggingface_hub import notebook_login\nnotebook_login() \n# can run `huggingface-cli login` via terminal","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:39:35.272143Z","iopub.execute_input":"2022-08-12T14:39:35.272561Z","iopub.status.idle":"2022-08-12T14:39:35.333328Z","shell.execute_reply.started":"2022-08-12T14:39:35.272499Z","shell.execute_reply":"2022-08-12T14:39:35.331794Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# TrainingArguments class stores a lot of information and allows fine-grained control over training and evaluation\nfrom transformers import Trainer, TrainingArguments\n\nbatch_size = 64\nlogging_steps = len(emotions_encoded[\"train\"]) // batch_size\nmodel_name = f\"{model_ckpt}-finetuned-emotion\"\ntraining_args = TrainingArguments(output_dir=model_name,\n                                 num_train_epochs=2,\n                                 learning_rate=2e-5,\n                                 per_device_train_batch_size=batch_size,\n                                 per_device_eval_batch_size=batch_size,\n                                 weight_decay=0.01,\n                                 evaluation_strategy=\"epoch\",\n                                 disable_tqdm=False,\n                                 logging_steps=logging_steps,\n                                 report_to=None,\n                                 push_to_hub=True,\n                                 log_level=\"error\")","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:47:36.463086Z","iopub.execute_input":"2022-08-12T14:47:36.463583Z","iopub.status.idle":"2022-08-12T14:47:36.478004Z","shell.execute_reply.started":"2022-08-12T14:47:36.463549Z","shell.execute_reply":"2022-08-12T14:47:36.476168Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"!sudo apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:44:22.005019Z","iopub.execute_input":"2022-08-12T14:44:22.005486Z","iopub.status.idle":"2022-08-12T14:44:29.365486Z","shell.execute_reply.started":"2022-08-12T14:44:22.005450Z","shell.execute_reply":"2022-08-12T14:44:29.363736Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"# finally, instantiate trainer and train model\ntrainer = Trainer(model=model, args=training_args, \n                  compute_metrics=compute_metrics, \n                  train_dataset=emotions_encoded[\"train\"],\n                  eval_dataset=emotions_encoded[\"validation\"],\n                  tokenizer=tokenizer\n                 )\ntrainer.train();","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:53:19.794899Z","iopub.execute_input":"2022-08-12T14:53:19.795298Z","iopub.status.idle":"2022-08-12T14:56:25.244020Z","shell.execute_reply.started":"2022-08-12T14:53:19.795267Z","shell.execute_reply":"2022-08-12T14:56:25.242303Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# get prediction outputs\npreds_output = trainer.predict(emotions_encoded[\"validation\"])\n\n# we get a PredictionOutput object that has arrays of predictions and label_ids along with metrics\npreds_output.metrics","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:56:25.253650Z","iopub.execute_input":"2022-08-12T14:56:25.260271Z","iopub.status.idle":"2022-08-12T14:56:28.826150Z","shell.execute_reply.started":"2022-08-12T14:56:25.260085Z","shell.execute_reply":"2022-08-12T14:56:28.824376Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"# can greedily decode with np.argmax(), yields predicted labels\ny_preds = np.argmax(preds_output.predictions, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:56:28.830050Z","iopub.execute_input":"2022-08-12T14:56:28.831580Z","iopub.status.idle":"2022-08-12T14:56:28.842709Z","shell.execute_reply.started":"2022-08-12T14:56:28.831533Z","shell.execute_reply":"2022-08-12T14:56:28.840886Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"plot_confusion_matrix(y_preds, y_valid, labels)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:56:28.847619Z","iopub.execute_input":"2022-08-12T14:56:28.848593Z","iopub.status.idle":"2022-08-12T14:56:29.674881Z","shell.execute_reply.started":"2022-08-12T14:56:28.848547Z","shell.execute_reply":"2022-08-12T14:56:29.672342Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"Results look much better, still some confusion between love and joy, also surprise and joy. Before we conclude, lets dive deeper into the mistakes our model is likely to make.\n\n**Error Analysis**\n\nA simple and powerful technique is to sort the validation samples by model loss. ","metadata":{}},{"cell_type":"code","source":"from torch.nn.functional import cross_entropy\n\n# a function to return the loss along with predicted label\ndef forward_pass_with_label(batch):\n    \n    # place all input tensors on same device as model\n    inputs = {k:v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n    \n    # get output loss\n    with torch.no_grad():\n        output = model(**inputs)\n        pred_label = torch.argmax(output.logits, axis=-1)\n        loss = cross_entropy(output.logits, batch[\"label\"].to(device), reduction=\"none\")\n    \n    # place outputs on CPU for compatibility with other dataset columns\n    return {\"loss\":loss.cpu().numpy(), \"predicted_label\": pred_label.cpu().numpy()}\n","metadata":{"execution":{"iopub.status.busy":"2022-08-12T14:58:52.791922Z","iopub.execute_input":"2022-08-12T14:58:52.792371Z","iopub.status.idle":"2022-08-12T14:58:52.808748Z","shell.execute_reply.started":"2022-08-12T14:58:52.792338Z","shell.execute_reply":"2022-08-12T14:58:52.801592Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"# convert dataset back to PyTorch tensors\nemotions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\n# compute loss values\nemotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n    forward_pass_with_label, batched=True, batch_size=16\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:00:25.046115Z","iopub.execute_input":"2022-08-12T15:00:25.046602Z","iopub.status.idle":"2022-08-12T15:00:27.884799Z","shell.execute_reply.started":"2022-08-12T15:00:25.046571Z","shell.execute_reply":"2022-08-12T15:00:27.883407Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# finally, create dataframe with texts, losses and pred/true labels\nemotions_encoded.set_format(\"pandas\")\ncols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\ndf_test = emotions_encoded[\"validation\"][:][cols]\ndf_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\ndf_test[\"predicted_label\"] = df_test[\"predicted_label\"].apply(label_int2str)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:01:42.198084Z","iopub.execute_input":"2022-08-12T15:01:42.199005Z","iopub.status.idle":"2022-08-12T15:01:42.239603Z","shell.execute_reply.started":"2022-08-12T15:01:42.198964Z","shell.execute_reply":"2022-08-12T15:01:42.238350Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":"Can now easily sort by losses. This is to detect:\n-  *Wrong labels*: mistake in labelling process to quickly correct\n-  *Quirks of dataset*; may always be messy, so inspecting weaknesses can help identify features ofsuch dataset","metadata":{}},{"cell_type":"code","source":"pd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_colwidth', 0)\n\n\n# samples with highest losses\ndf_test.sort_values(\"loss\", ascending=False).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:04:07.806946Z","iopub.execute_input":"2022-08-12T15:04:07.807669Z","iopub.status.idle":"2022-08-12T15:04:07.843094Z","shell.execute_reply.started":"2022-08-12T15:04:07.807610Z","shell.execute_reply":"2022-08-12T15:04:07.841888Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"Some of the texts appear dubious, so the labels are questionable in some places. So with this information we can refine the dataset which can lead to a big performance gain as having more data or larger models.\n\n**Data Quality Matters!**\n\nIt is also worth looking at samples which model is most confident about, as there is a chance our model is exploiting shortcuts to get a prediction. Look at smallest loss:","metadata":{}},{"cell_type":"code","source":"df_test.sort_values(\"loss\", ascending=True).head(10)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:08:11.854644Z","iopub.execute_input":"2022-08-12T15:08:11.855313Z","iopub.status.idle":"2022-08-12T15:08:11.884509Z","shell.execute_reply.started":"2022-08-12T15:08:11.855282Z","shell.execute_reply":"2022-08-12T15:08:11.882460Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"markdown","source":"Our model is most confident about joy it seems. So we can keep an eye on this class and make targeted improvements to our training dataset.","metadata":{}},{"cell_type":"code","source":"# share a model to the community\ntrainer.push_to_hub(commit_message = \"Training completed!\")","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:09:24.650319Z","iopub.execute_input":"2022-08-12T15:09:24.652022Z","iopub.status.idle":"2022-08-12T15:09:58.937629Z","shell.execute_reply.started":"2022-08-12T15:09:24.651976Z","shell.execute_reply":"2022-08-12T15:09:58.935956Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline\n\n# can use the model to make predictions on new tweets\nmodel_id = \"stevevee0101/distilbert-base-uncased-finetuned-emotion\"\nclassifier = pipeline(\"text-classification\", model=model_id)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:15:18.878067Z","iopub.execute_input":"2022-08-12T15:15:18.879086Z","iopub.status.idle":"2022-08-12T15:15:54.010678Z","shell.execute_reply.started":"2022-08-12T15:15:18.879050Z","shell.execute_reply":"2022-08-12T15:15:54.009262Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"custom_tweet = \"I walked up the stairs and saw in the corner of my eye a giant tomato!\"\npreds = classifier(custom_tweet, return_all_scores=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:16:03.512935Z","iopub.execute_input":"2022-08-12T15:16:03.513337Z","iopub.status.idle":"2022-08-12T15:16:03.624082Z","shell.execute_reply.started":"2022-08-12T15:16:03.513306Z","shell.execute_reply":"2022-08-12T15:16:03.622484Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"# plot probabilities\npreds_df = pd.DataFrame(preds)\nplt.bar(labels, 100 * preds_df[\"score\"], color='C0')\nplt.title(f'\"{custom_tweet}\"')\nplt.ylabel(\"Class probability (%)\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-12T15:17:29.304345Z","iopub.execute_input":"2022-08-12T15:17:29.304784Z","iopub.status.idle":"2022-08-12T15:17:29.550134Z","shell.execute_reply.started":"2022-08-12T15:17:29.304739Z","shell.execute_reply":"2022-08-12T15:17:29.548696Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"markdown","source":"Trained a model to classify emotions in tweets! Have seen two complementary approaches in features and fine-tuning, assessing their strengths and weaknesses.\n\nPotential issues:\n-  Model in prodction: Want to serve predictions, like on huggingface hub\n-  Faster predictions, knowledge distillation and tricks to speed up models in Chap8\n-  Other things your model can do?\n-  Non-english; multi-lingual variety!\n-  No labels; then fine-tuning may not be an option, but there are potentially other techniques!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}