{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Directions\n",
    "\n",
    "## Scaling Transformers\n",
    "- Leverage computation in long run; better than encoding domain knowledge\n",
    "- Power-law scaling for transformer models\n",
    "\n",
    "Challenges:\n",
    "- Infrastructure\n",
    "- Cost\n",
    "- Dataset curation; high quality, and control biases..\n",
    "- Model evaluation\n",
    "- Deployment\n",
    "\n",
    "## Attention!\n",
    "Can make more efficient models by focusing on different areas, namely: \n",
    "- Sparse Matrix\n",
    "- Linearized self-attention (linear vs the $n^2$ order of computation)\n",
    "\n",
    "It may be difficult to instill and moderate:\n",
    "- Human reporting bias: See through the internet's view of the world..\n",
    "- Common sense: Common sense is assumed and rarely written down; so LMs may have facts but lack common sense\n",
    "- Facts: Cannot store facts in a reliable way and can produce text that is factually wrong\n",
    "- Modality: LM's have no way to connect to other modalities; such as audio/visual signals, or tabular data..\n",
    "\n",
    "### Vision\n",
    "- Applying transformers to images; viewing pixels as a sequence of text\n",
    "- Approaches scale better CNN's\n",
    "\n",
    "### Tables\n",
    "- Table parser to help answer queries (QA); or analyse columns of numbers\n",
    "\n",
    "### Speech to Text\n",
    "- wav2vec 2.0 model family; combine CNN with transformers: competitive results with only a few minutes of data\n",
    "- There is a model in HF transformers!\n",
    "- Using model outputs to train new models; wav2Vec-U using GANs\n",
    "\n",
    "### Vision and Text\n",
    "- Models to analyse images (ViT)\n",
    "- Models to generate images from text (DALL-E)\n",
    "    - Words and pixels as one sequence of tokens; so can generate images from text prompt\n",
    "- CLIP: Trains using contrastive learning; encodes both text and images to create embeddings of captions and images, then the contrastive objective in a batch is to maximise the similarity of embeddings of the corresponding pair (dot product) while minimising the similarity of the rest. Requires a labeled dataset; and provides captions for images given a set of options. Can define classes through text!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
