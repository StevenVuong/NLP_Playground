{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapter 9:Training with Little/No Data","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"import pandas as pd\n\ndataset_url = \"https://git.io/nlp-with-transformers\"\ndf_issues = pd.read_json(dataset_url, lines=True)\nprint(f\"DataFrame shape: {df_issues.shape}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cols = [\"url\", \"id\", \"title\", \"user\", \"labels\", \"state\", \"created_at\", \"body\"]\ndf_issues.loc[2, cols].to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# issues is a list of json objects with metadata on each label\ndf_issues.loc[1, 'user']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# overwrite labels with just the names as thats all we're interested in\ndf_issues[\"labels\"] = (df_issues[\"labels\"].apply(lambda x: [meta[\"name\"] for meta in x]))\ndf_issues[[\"labels\"]].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute length of each row to find num of labels per issue\ndf_issues[\"labels\"].apply(lambda x: len(x)).value_counts().to_frame().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Majority of issues 0 or one label.","metadata":{}},{"cell_type":"code","source":"# loop at top 10 most frequent labels in dataset\ndf_counts = df_issues[\"labels\"].explode().value_counts()\nprint(f\"Number of labels: {len(df_counts)}\")\n# top-8 label categories\ndf_counts.to_frame().head(8).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Large class imbalance here.. We'll turn our classification problem to focusing on a tagger for a subset of the labels; some labels such as \"good first issue\" or \"help wanted\" are very difficult to predict from description..","metadata":{}},{"cell_type":"code","source":"label_map = {\"Core: Tokenization\": \"tokenization\",\n \"New model\": \"new model\",\n \"Core: Modeling\": \"model training\",\n \"Usage\": \"usage\",\n \"Core: Pipeline\": \"pipeline\",\n \"TensorFlow\": \"tensorflow or tf\",\n \"PyTorch\": \"pytorch\",\n \"Examples\": \"examples\",\n \"Documentation\": \"documentation\"}\n\ndef filter_labels(x):\n    return [label_map[label] for label in x if label in label_map]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_issues[\"labels\"] = df_issues[\"labels\"].apply(filter_labels)\nall_labels = list(label_map.values())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_counts = df_issues[\"labels\"].explode().value_counts()\ndf_counts.to_frame().T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create new col for labeled or not\ndf_issues[\"split\"] = \"unlabeled\"\nmask = df_issues[\"labels\"].apply(lambda x: len(x)) > 0\ndf_issues.loc[mask, \"split\"] = \"labeled\"\ndf_issues[\"split\"].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in [\"title\", \"body\", \"labels\"]:\n    print(f\"{col}: {df_issues[col].iloc[26][:500]}\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concat title with body\ndf_issues[\"text\"] = (df_issues.apply(lambda x: x[\"title\"] + \"\\n\\n\" + x[\"body\"], axis=1))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len_before = len(df_issues)\n# remove duplicates\ndf_issues = df_issues.drop_duplicates(subset=\"text\")\nprint(f\"Removed {(len_before - len(df_issues)) / len_before:.2%} duplicates.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n(df_issues[\"text\"].str.split().apply(len).hist(bins=np.linspace(0,500,50),grid=False,edgecolor=\"C0\"))\nplt.title(\"Words per issue\")\nplt.xlabel(\"Number of words\")\nplt.ylabel(\"Number of issues\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Hopefully truncating 512 or longer won't have too much of an impact.","metadata":{}},{"cell_type":"markdown","source":"## Creating Training Sets","metadata":{}},{"cell_type":"markdown","source":"Multilabel problems; slightly trickier because there is no guaranteed balance for all labels. We can approximate, however with scikit-multilearn library.\n\nFirst need to tsfm our set of labels into a form that the model can process. Use MultiLabelBinarizer which takes a list of label names and creates a vector with 0's for absent labels and 1's for present labels.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MultiLabelBinarizer\n\nmlb = MultiLabelBinarizer()\nmlb.fit([all_labels]) # 1's corresponds to present labels\nmlb.transform([[\"tokenization\", \"new model\"], [\"pytorch\"]])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use `iterative_train_test_split()` function to create splits which creates train/test splits iteratively to achieve balanced labels. Wrap in a function that we can apply to DataFramess. As it expects a 2D feature matrix, we add a dimension to the possible indices before splitting.","metadata":{}},{"cell_type":"code","source":"from skmultilearn.model_selection import iterative_train_test_split\n\ndef balanced_split(df, test_size=0.5):\n    # add dimension\n    ind = np.expand_dims(np.arange(len(df)), axis=1)\n    # get labels\n    labels = mlb.transform(df[\"labels\"])\n    # iteratively split\n    ind_train, _, ind_test, _ = iterative_train_test_split(ind, labels, test_size)\n    \n    return df.iloc[ind_train[:, 0]], df.iloc[ind_test[:, 0]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# split to supervised and unsupervised datasets\ndf_clean = df_issues[[\"text\", \"labels\", \"split\"]].reset_index(drop=True).copy()\ndf_unsup = df_clean.loc[df_clean[\"split\"] == \"unlabeled\", [\"text\", \"labels\"]]\ndf_sup = df_clean.loc[df_clean[\"split\"] == \"labeled\", [\"text\", \"labels\"]]\n\n# then create balanced training, val and test for supervised part\nnp.random.seed(0)\ndf_train, df_tmp = balanced_split(df_sup, test_size=0.5)\ndf_valid, df_test = balanced_split(df_tmp, test_size=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\n# create DatasetDict with splits to easily tokenize dataset\n# and integrate with Trainer\nds = DatasetDict({\n    \"train\": Dataset.from_pandas(df_train.reset_index(drop=True)),\n    \"valid\": Dataset.from_pandas(df_valid.reset_index(drop=True)),\n    \"test\": Dataset.from_pandas(df_test.reset_index(drop=True)),\n    \"unsup\": Dataset.from_pandas(df_unsup.reset_index(drop=True))\n})","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create Training Slices","metadata":{}},{"cell_type":"markdown","source":"We'd like to investigate sparse labeled data and multilabel classification. Only 220 examples to train with in training set. Start with eight samples per label and build up until the slide covers the full training set using `iterative_train_test_split()`","metadata":{}},{"cell_type":"code","source":"np.random.seed(0)\nall_indices = np.expand_dims(list(range(len(ds[\"train\"]))), axis=1)\nindices_pool = all_indices\nlabels = mlb.transform(ds[\"train\"][\"labels\"])\ntrain_samples = [8, 16, 32, 64, 128]\ntrain_slices, last_k = [], 0\n\nfor i, k in enumerate(train_samples):\n    # split samples necessary to fill the gap to the next split size\n    indices_pool, labels, new_slice, _ = iterative_train_test_split(\n        indices_pool, labels, (k-last_k)/len(labels)\n    )\n    last_k = k\n    if i==0: train_slices.append(new_slice)\n    else: train_slices.append(np.concatenate((train_slices[-1], new_slice)))\n        \n# add full dataset as last slice\ntrain_slices.append(all_indices), train_samples.append(len(ds[\"train\"]))\ntrain_slices = [np.squeeze(train_slice) for train_slice in train_slices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"See that we approximately split samples to desired sizes.","metadata":{}},{"cell_type":"code","source":"print(\"Target split sizes:\")\nprint(train_samples)\nprint(\"Actual split sizes:\")\nprint([len(x) for x in train_slices])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementing a Naive Baseline\n","metadata":{}},{"cell_type":"markdown","source":"Why?\n1. Simple baseline based on regex or rules may solve the problem really well, so no need to introduce big models like transformers etc..\n2. Baselines provide quick checks as we explore more complex models. Gives us an understanding of the results to expect\n\nNaive Bayes is a good baseline for text classification as it is simple, quick to train and fairlyrobust.\n\nUse scikit-learn multilearn to cast problem as one vs rest classification.","metadata":{}},{"cell_type":"code","source":"def prepare_labels(batch):\n    batch[\"labels_ids\"] = mlb.transform(batch[\"labels\"])\n    return batch\n\nds = ds.map(prepare_labels, batched=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create defaultdict with list to store scores per split\n\nfrom collections import defaultdict\n\nmacro_scores, micro_scores = defaultdict(list), defaultdict(list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfor train_slice in train_slices:\n    \n    # get training slice and test data\n    ds_train_sample = ds[\"train\"].select(train_slice)\n    y_train = np.array(ds_train_sample[\"labels_ids\"])\n    y_test = np.array(ds[\"test\"][\"labels_ids\"])\n    \n    # use simple count vectoriser to encode texts as token counts (BoW approach)\n    count_vect = CountVectorizer()\n    X_train_counts = count_vect.fit_transform(ds_train_sample[\"text\"])\n    X_test_counts = count_vect.transform(ds[\"test\"][\"text\"])\n    \n    # create and trian our model\n    classifier = BinaryRelevance(classifier=MultinomialNB())\n    classifier.fit(X_train_counts, y_train)\n    \n    # generate predictions and evaluate\n    y_pred_test = classifier.predict(X_test_counts)\n    clf_report = classification_report (\n        y_test, y_pred_test, target_names=mlb.classes_, zero_division=0,\n        output_dict=True\n    )\n    \n    # Store metrics (micro and macro F1 scores)\n    macro_scores[\"Naive Bayes\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Naive Bayes\"].append(clf_report[\"micro avg\"][\"f1-score\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot results from experiment\nimport matplotlib.pyplot as plt\n\ndef plot_metrics(micro_scores, macro_scores, sample_sizes, current_model):\n    fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10,4), sharey=True)\n    \n    for run in micro_scores.keys():\n        if run == current_model:\n            ax0.plot(sample_sizes, micro_scores[run], label=run, linewidth=2)\n            ax1.plot(sample_sizes, macro_scores[run], label=run, linewidth=2)\n        else:\n            ax0.plot(sample_sizes, micro_scores[run], label=run, linestyle=\"dashed\")\n            ax1.plot(sample_sizes, macro_scores[run], label=run, linestyle=\"dashed\")\n    \n    ax0.set_title(\"Micro F1 scores\")\n    ax1.set_title(\"Macro F1 scores\")\n    ax0.set_ylabel(\"Test set F1 score\")\n    ax0.legend(loc=\"lower right\")\n    \n    for ax in [ax0, ax1]:\n        ax.set_xlabel(\"Number of training samples\")\n        ax.set_xscale(\"log\")\n        ax.set_xticks(sample_sizes)\n        ax.set_xticklabels(sample_sizes)\n        ax.minorticks_off()\n    plt.tight_layout()\n    plt.show()\n    \nplot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note plot number of samples on log scale. Can see micro and macro F1 scores improve as we increase numbere of training samples.\n\nWith so few samples to train on, the results are also slightly noisy since each slice can have a different class distribution. What's important here though is the trend!\n\nNow have a look with transformer-based approaches.\n\n### Working with No Labeled Data\n\nFirst technique: zero-shot classification, suitable for settings with no labeled data at all.\n\nFirst load BERT-base in fill-mask pipeline which uses masked language model to predict the content of masked tokens..","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"fill-mask\", model=\"bert-base-uncased\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movie_desc = \"The main characters of the movie madagascar are a lion, a zebra, a giraffe, and a hippo.\"\nprompt = \"The movie is about [MASK].\"\n\noutput = pipe(movie_desc + prompt)\nfor element in output:\n    print(f\"Token {element['token_str']}: \\t{element['score']:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"can also query pipeline for probability of a few given tokens","metadata":{}},{"cell_type":"code","source":"output = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}: \\t {element['score']:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can also try on a description closer to cars.","metadata":{}},{"cell_type":"code","source":"movie_desc = \"In the movie transformers aliens can morph into a wide range of vehicles.\"\n\noutput = pipe(movie_desc + prompt, targets=[\"animals\", \"cars\"])\nfor element in output:\n    print(f\"Token {element['token_str']}: \\t {element['score']:.3f}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It works! Let's see if we can do better by adapting a model that has been fine-tuned on a task closer to text classification: *natural language inference (NLI)*.\n\nThere is a proxy task called *text entailment*, where the model needs to determine whether two passages are likely to follow or contradict each other. \n\nEach asmple has three parts: a premise, a hypothesis, and a label; which can be entailment, neutral or contradiction. \n\nWe can hijack a model trained on MNLI dataset to build a classifier without needing any labels at all! The trick is to treat the text we wish to classify as the premise then formulate the hypothesis as:\n\n`\"This example is about {label}.\"`\n\nWhere we insert class name for label; entailment score tells us how likely is the premise to be about that topic, and we can run for any number of classes sequentially. Downside: We need to execute a forward pass for each class, making it less efficient than a standard classifier.\n\nTricky also, choice of label names can have a large impact on accuracy, and choosing labels with a semantic meaning is generally the best approach.","metadata":{}},{"cell_type":"code","source":"# transformers have a MNLI model for zero-shot classification built in\n# we can initialise it via pipeline as follows\nfrom transformers import pipeline\n\n# device=0 to run model on GPU instead of CPU\npipe = pipeline(\"zero-shot-classification\", device=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = ds[\"train\"][0]\nprint(f\"Labels: {sample['labels']}\")\n\n# mult label = True to return all scores and not max for single-label classification\noutput = pipe(sample[\"text\"], all_labels, multi_label=True)\nprint(output[\"sequence\"][:400])\nprint(f\"\\nPredictions:\")\nfor label, score in zip(outputs[\"labels\"], output[\"scores\"]):\n    print(f\"{label}, {score:.2f}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"> Since we are using a subword tokenizer, we can even pass code to the model! Tokenization may not be efficient as only a small fraction of pretraining dataset has code snippets, but since code has lots of natural language this is note an issue!\n\n> Also code block may contain important information, such as the framework (PyTorch or TensorFlow)..\n\nModel is confident this text is about a new model, also relatively high scores for other labels. This is quite a challenging task for model as it is quite technical and about coding.","metadata":{}},{"cell_type":"code","source":"# function to feed single example through zero-shot pipeline\n# scale out to whole validation set by running map()\ndef zero_shot_pipeline(example):\n    output = pipe(example[\"text\"], all_labels, multi_label=True)\n    example[\"predicted_labels\"] = output[\"labels\"]\n    example[\"scores\"] = output[\"scores\"]\n    return example\n\nds_zero_shot = ds[\"valid\"].map(zero_shot_pipeline)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the scores, next step is to determine which sets of labels to be assigned to each example. A few options we can experiment with:\n- Define threshold and select all labels above the threshold\n- Pick top-k labels with the k highest scores","metadata":{}},{"cell_type":"code","source":"# applies one of approaches to retrieve predictions\ndef get_preds(example, threshold=None, topk=None):\n    preds = []\n    if threshold:\n        for label, score in zip(example[\"predicted_labels\"], example[\"scores\"]):\n            if score >= threshold:\n                preds.append(label)\n    elif topk:\n        for i in range(topk):\n            preds.append(example[\"predicted_labels\"][i])\n    else:\n        raise ValueError(\"Set either `threshold` or `topk`.\")\n    \n    return {\"pred_label_ids\": list(np.squeeze(mlb.transform([preds])))}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# returns sklearn clf report from dataset with predicted labels\ndef get_clf_report(ds):\n    y_true = np.array(ds[\"label_ids\"])\n    y_pred = np.array(ds[\"pred_label_ids\"])\n    return classification_report(\n        y_true, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True\n    )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Macro weighs (divides by) each class equally\n- Micro weighs each sample equally\n\nIf we ha+e equal number of samples per class, the micro and macro will be the same.","metadata":{}},{"cell_type":"code","source":"macros, micros = [], []\ntopks = [1, 2, 3, 4]\nfor topk in topks:\n    ds_zero_shot = ds_zero_shot.map(get_preds, batched=False, fn_kwargs={'topk':topk})\n    clf_report = get_clf_report(ds_zero_shot)\n    micros.append(clf_report['micro avg']['f1-score'])\n    macros.append(clf_report['macro_avg']['f1-score'])\n    \nplt.plot(topks, micros, label='Micro F1')\nplt.plot(topks, macros, label='Macro F1')\nplt.xlabel('Top-K')\nplt.ylabel('F1-score')\nplt.legend(loc='best')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_t, best_micro = thresholds[np.argmax(micros)], np.max(micros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_micro:.2f}.')\nbest_t, best_macro = thresholds[np.argmax(macros)], np.max(macros)\nprint(f'Best threshold (micro): {best_t} with F1-score {best_macro:.2f}.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see the trade-off: Threshold too low has too many predictions, leading to low precision. If threshold too high, then hardly any predictions so low recall. So threshold around 0.8 is about the sweet-spot between the two.\n\nCompare top-1 of zero-shot classification (since its best performing) to compare with Naive Bayes on test set:","metadata":{}},{"cell_type":"code","source":"ds_zero_shot = ds['test'].map(zero_shot_pipeline)\nds_zero_shot = ds_zero_shot.map(get_preds, fn_kwargs={'topk': 1})\nclf_report = get_clf_report(ds_zero_shot)\n\nfor train_slice in train_slices:\n    macro_scores['Zero Shot'].append(clf_report['macro avg']['f1-score'])\n    micro_scores['Zero Shot'].append(clf_report['micro avg']['f1-score'])\n    \nplot_metrics(micro_scores, macro_scores, train_samples, \"Zero Shot\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Observations:\n- < 50 samples, zero-shot barely outperforms baseline\n- \\> 50 samples, the zero-shot performance is superior when considering both micro and macro F1-scores. Results for micro F1-score tells us baseline performs well on frequent classes, while the zero-shot pipeline excels at those since it doesn't require any examples to learn from.\n\n\n> In a real use-case, it makes sense to gather a handful of labeled examples to do some quick evaluations. Important point is that we did not adapt the model parameters (fine-tune) with data, instead, just adapt some hyperparameters.\n\n\nIf struggle to get good resultst on own dataset, some things to improve the zero-shot pipeline:\n- Pipeline is very sensitive to name of labels. If the names don't make much sense or are not easily connected to the texts, the pipeline will probably perform poorly. Either try using different names or try using several names in parallel and aggregate them in an extra step.\n- Can improve the form of the hypothesis. This may improve performance depending on tthe use-case.","metadata":{}},{"cell_type":"markdown","source":"## Working with a Few Labels","metadata":{}},{"cell_type":"markdown","source":"### Data Augmentation\nGenerate new training examples from existing ones by slightly perturbing them. Can be tricky as perturbations can change their meaning. In text, generally two types:\n- *Back Translation*: Take a text in source language, translate it to a language using machine translation and translate it back. Tends to work best for high-resource languages or corpora that do not have too many domain specific language\n- *Token Perturbations*: Randomly choose and perform simple transformations like random synonym replacement, word insertion, swap or deletion to a text\n\nHere we will focus on synonym replacement, as it is simple to implement and gets across the main idea behind data augmentation.","metadata":{}},{"cell_type":"code","source":"from transformers import set_seed\nfrom nlpaug.augmenter.word as naw\n\nset_seed(3)\n# leverage contextual word embeddings of DistilBERT for synonym replacements\naug = naw.ContextualWordEmbsAug(\n    model_path=\"distilbert-base-uncased\", device=\"cpu\", action=\"substitute\"\n)\ntext = \"Transformers are the most popular toys\"\nprint(f\"Original text: {text}\")\nprint(f\"Augmented text: {aug.augment(text)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So applies a replacement to generate a new synthetic training example.","metadata":{}},{"cell_type":"code","source":"# wrap augmentation in a simple function\ndef augment_text(batch, transformations_per_example=1):\n    text_aug, label_ids = [], []\n    for text, labels in zip(batch[\"text\"], batch[\"label_ids\"]):\n        text_aug += [text]\n        label_ids += [labels]\n        for _ in range(transformations_per_example):\n            text_aug += [aug.augment(text)]\n            label_ids += [labels]\n    return {\"text\": text_aug, \"label_ids\": label_ids}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# can generate any num of new examples \n# train NB clf with one line after we select slice\nds_train_sample = ds_train_sample.map(\n    augment_text, batched=True, remove_columns=ds_train_sample.column_names\n).shuffle(seed=42)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(micro_scores, macro_scores, train_samples, \"Naive Bayes + Aug\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Small amount of data augmentation improves $F_1 - score$ of NB by ~5 pts, and overtakes zero-shot for macro scores after ~170 training samples!\n\nNow look at method using embeddings of large language models.\n\n### Using Embeddings as a Lookup Table\n\nBecause large language models can encode information, embeddings can be used as a semantic search engine, find similar documents/comments, or even classify text. It is usually a three step process:\n1. Use Language model to embed all labeled texts\n2. NN search over stored embeddings\n3. Aggregate labels of NN's to get a prediction\n\nImportant to calibrate num neighbours to search, as too few may be noisy and too many might mix in neighbouring groups.\n\nPros: No model fine-tuning to leverage the few available data points. Just need to select a appropriate model pretrained on a similar domain to our dataset.\n\nUse GPT2 for this technique, and pool layers together; make sure we avoid the padded layers so use attention mask to help handle that.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModel\n\nmodel_ckpt = \"miguelvictor/python-gpt2-large\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModel.from_pretrained(model_ckpt)\n\ndef mean_pooling(model_output, attention_mask):\n    # Extract token embeddings\n    token_embeddings = model_output[0]\n    \n    # compute attention mask\n    input_mask_expanded = (\n        attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n    )\n    # sum embeddings, ignore masked tokens\n    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n    # return avg as a single vector\n    return sum_embeddings / sum_mask\n\ndef embed_text(examples):\n    # can get embeddings for each split with this fn\n    inputs = tokenizer(examples[\"text\"], padding=True, truncation=True,\n                      max_length=128, return_tensors=\"pt\")\n    with torch.no_grad():\n        model_output = model(**inputs)\n    pooled_embeds = mean_pooling(model_output, inputs[\"attention_mask\"])\n    return {\"embedding\": pooled_embeds.cpu().numpy()}","metadata":{"execution":{"iopub.status.busy":"2022-10-04T10:10:20.170206Z","iopub.execute_input":"2022-10-04T10:10:20.170640Z","iopub.status.idle":"2022-10-04T10:16:43.073200Z","shell.execute_reply.started":"2022-10-04T10:10:20.170604Z","shell.execute_reply":"2022-10-04T10:16:43.071703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"GPT models don't have padding token, so need to add one before we can get embeddings in a batched fashion. Just recycle end-of-string token for this..","metadata":{}},{"cell_type":"code","source":"tokenizer.pad_token = tokenizer.eos_token\nembs_train = ds[\"train\"].map(embed_text, batched=True, batch_size=16)\nembs_valid = ds[\"valid\"].map(embed_text, batched=True, batch_size=16)\nembs_test = ds[\"test\"].map(embed_text, batched=True, batch_size=16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use *FAISS* index to query embeddings. Create index.","metadata":{}},{"cell_type":"code","source":"embds_train.add_faiss_indedx(\"embedding\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Query nearest neighbours.","metadata":{}},{"cell_type":"code","source":"i, k = 0, 3 # select first query and 3 NN\nrn, nl = \"\\r\\n\\r\\n\", \"\\n\" # used to remove newlines in text for compact display\n\nquery = np.array(embs_valid[i][\"embedding\"], dtype=np.float32)\nscores, samples = embs_train.get_nearest_examples(\"embedding\", query, k=k)\n\nprint(f\"QUERY LABELS: {embs_valid[i]['labels']}\")\nprint(f\"QUERY TEXT: \\n{embs_valid[i]['text'][:200].replace(rn, nl) [...]\\n}\")\nprint(\"=\" * 50)\nprint(f\"Retrieved documents:\")\n\nfor score, label, text in zip(scores, samples[\"labels\"], samples[\"text\"]):\n    print(\"=\"*50)\n    print(f\"TEXT:\\n{text[:200].replace(rn, nl)} [...]\")\n    print(f\"SCORE: {score:.2f}\")\n    print(f\"LABELS: {label}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Retrieved documents around adding new and efficient transformer models. Next question: What is the best value for k? Also, how to aggregate labels of retrieved documents?\n\nTry several values of *k*, and vary threshold m < k for label assignment. Record macro and micro performance for each so can decide later which run performed best. Instead of looping over each sample in validation set, we can make use of function `get_nearest_examples_batch()` which accepts a batch of queries:","metadata":{}},{"cell_type":"code","source":"def get_sample_preds(sample, m):\n    return (np.sum(sample[\"label_ids\"], axis=0) >= m).astype(int)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def find_best_k_m(ds_train, valid_queries, valid_labels, max_k=17):\n    max_k = min(len(ds_train), max_k)\n    perf_micro = np.zeros((max_k, max_k))\n    perf_macro = np.zeros((max_k, max_k))\n    for k in range(1, max_k):\n        for m in range(1, k+1):\n            _, samples = ds_train.get_nearest_examples_batch(\n                \"embdding\", valid_queries, k=k)\n            y_pred = np.array([get_sample_preds(s, m) for s in samples])\n            clf_report = classification_report(\n                valid_labels, y_pred, target_names=mlb.classes_, \n                zero_division=0, output_dict=True)\n            perf_micro[k, m] = clf_report[\"micro avg\"][\"f1-score\"]\n            perf_macro[k, m] = clf_report[\"macro avg\"][\"f1-score\"]\n    return perf_micro, perf_macro","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Find best values with all training samples and visualise scores for all *k* and *m* configurations.","metadata":{}},{"cell_type":"code","source":"valid_labels = np.array(embs_valid[\"label_ids\"])\nvalid_queries = np.array(embs_valid[\"embedding\"], dtype=np.float32)\nperf_micro, perf_macro = find_best_k_m(embs_train, valid_queries, valid_labels)\n\nfig, (ax0, ax1) = plt.subplots(1, 2, figsize=(10, 3.5), sharey=True)\nax0.imshow(perf_micro)\nax1.imshow(perf_macro)\n\nax0.set_title(\"micro scores\")\nax0.set_ylabel(\"k\")\nax1.set_title(\"macro scores\")\nfor ax in [ax0, ax1]:\n    ax.set_xlim([0.5, 17 - 0.5])\n    ax.set_ylim([17 - 0.5, 0.5])\n    ax.set_xlabel(\"m\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optimal ratio of $m/k = 1/3$","metadata":{}},{"cell_type":"code","source":"k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\nprint(f\"Best k: {k}, bestt m: {m}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Best with k=15 and m=5; so when we retrieve 15 nearest neighbors and assign labels that occured at least 5 times.\n\nBefore we can slice the dataset, we must remove the FAISS index as we cannot slice that. We need to slice for Naive Bayes where we go through slices of the training set and evaluate the performance.","metadata":{}},{"cell_type":"code","source":"embs_train.drop_index(\"embedding\")\ntest_labels = np.array(embs_test[\"label_ids\"])\ntest_queries = np.array(embs_test[\"embedding\"], dtype=np.float32)\n\nfor train_slice in train_slices:\n    # create FAISS index from training slice\n    embs_train_tmp = embs_train.select(train_slice)\n    embs_train_tmp.add_faiss_index(\"embedding\")\n    \n    # get best k, m values with validation set\n    perf_micro, _ = find_best_k_m(embs_train_tmp, valid_queries, valid_labels)\n    k, m = np.unravel_index(perf_micro.argmax(), perf_micro.shape)\n    \n    # get predictions on test set\n    _, samples = embs_train_tmp.get_nearest_examples_batch(\n        \"embedding\", test_queries, k=int(k))\n    y_pred = np.array([get_sample_preds(s, m) for s in samples])\n    \n    # evaluate predictions\n    clf_report = classification_report(\n        test_labels, y_pred, target_names=mlb.classes_, zero_division=0, output_dict=True\n    )\n    \n    macro_scores[\"Embedding\"].append(clf_report[\"macro avg\"][\"f1-score\"])\n    micro_scores[\"Embedding\"].append(clf_report[\"micro avg\"][\"f1-score\"])\n    \nplot_metrics(micro_scores, macro_scores, train_samples, \"Embedding\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Embedding lookup is competitive on micro scores while only having two \"learnable\" parameters, k and m, but performs slightly worse on the macro scores.\n\nMethod working depends also on the domain; consider model domain also.\n\n> FAISS partitions initially with k-means clustering; and we get a centroid vector. So we go from searching `n` to `k + n/k`. If *k* is too small, there are many samples we need to compare against in seecond step, if too large then there are many centroids we need to search through. So we look for the minimum of $f(k) = k + n/k$ WRT *k*. We find $k = \\sqrt{n}$. We can also use GPUs for spedeup and can compress vectors with advanced quantisation schemes. Credit goes to Facebook for developing this.","metadata":{}},{"cell_type":"markdown","source":"We will compare the embedding lookup to fine-tuning a model.\n\n### Fine-Tuning a Vanilla Transformer\n\nIf we have labeled data, the first thing we can do is fine-tune a pretrained transformer. Start with standard BERT, then later see what effect fine-tuning a language model has on performance.\n\n> For many applications, there are models already pretrained on that given domain. So it's worth having a look!","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification)\n\nmodel_ckpt = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\ndef tokenize(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=128)\n\nds_enc = ds.map(tokenize, batched=True)\nds_enc = ds_enc.remove_columns(['labels', 'text'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds_enc.set_format(\"torch\")\n# must be float as multi-label loss also allows discrete labels for class probabilities\nds_enc =  ds_enc.map(\n    lambda x: {\"label_ids_f\": x[\"label_ids\"].to(torch.float)}, remove_columns=[\"label_ids\"])\n# workaround: Create new col with labels; format of col is inferred from first one\n# then delete the original column and rename the new one to replace the old one\n# since changing format of column element wise doesn't work well with Arrow's typed format\nds_enc = ds_enc.rename_column(\"label_ids_f\", \"label_ids\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args_fine_tune = TrainingArguments(\n    output_dir=\"./results\", num_train_epochs=20, learning_rate=3e-5,\n    lr_scheduler_type='constant', per_device_train_batch_size=4,\n    per_device_eval_batch_size=32, weight_decay=0.0,\n    evaluation_strategy=\"epoch\", save_strategy=\"epoch\", logging_strategy=\"epoch\",\n    load_best_model_at_end=True, metric_for_best_model='micro f1',\n    save_total_limit=1, log_level='error'\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Need $F_1$ score to choose best model. However, model returns logits so need to normalise predictions with signmoid before binarising with simple threshold. Then return scores interested in from the classification report.","metadata":{}},{"cell_type":"code","source":"from scipy.special import expit as sigmoid\n\ndef compute_metrics(pred):\n    y_true = pred.label_ids\n    y_pred = sigmoid(pred.predictions)\n    y_pred = (y_pred > 0.5).astype(float)\n    \n    clf_dict = classification_report(y_true, y_pred, target_names=all_labels,\n                                    zero_division=0, output_dict=True)\n    \n    return {\"micro f1\": clf_dict[\"micro avg\"][\"f1-score\"],\n           \"macro f1\": clf_dict[\"macro avg\"][\"f1-score\"]}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For each training set slice, we'll train a classifier from scratch, load the best model at the end of the training loop and store the results on the test set.","metadata":{}},{"cell_type":"code","source":"config = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\n\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_ckpt, config=config)\n    \n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, \n        args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"]\n    )\n    \n    trainer.train()\n    pred = trainer.predict(ds_enc[\"test\"])\n    metrics = compute_metrics(pred)\n    \n    macro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"macro f1\"])\n    micro_scores[\"Fine-tune (vanilla)\"].append(metrics[\"micro f1\"])\n    \nplot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (vanilla)\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Can see that fine-tuning vanilla BERT on dataset leads to competitive results after ~64 examples; and before 64 examples the behaviour is erroatic, in that a model trained on a small sample can be unbalanced. \n\nTake a look at another promising approach for language models in few-shot domain.\n\n### In-Context and Few-Shot Learning with Prompts","metadata":{}},{"cell_type":"markdown","source":"Large language model presents the ability of models to learn effectively from examples presented in the prompt; and the larger a language model is scaled, the better it is at using in-context examples leading to significant performance boosts. This means we can get reasonable results without having to further train.\n\nAn alternative is to use labeled data to create examples and fine-tune the language model head on the examples. \n\nNext we will look at how to make good use of few labeled examples we have, and also the large volume of unlabeled data we have.\n\n## Leveraging Unlabeled Data\n\nBetter if downstream task is closer to domain we previously trained the model on.\n\nWe use domain adaptation instead, continue training a model on data from our domain; predicting masked words, so no need for labeled data; then can load the adapted model as a classifier and fine-tune it, leveraging the unlabeled data.\n\nGreat thing is, unlabeled data is abundantly available, and the adapted model can be reused for many use-cases. After domain adaptation, can apply entity recognition, or another classification task like sentiment analysis since the approach is agnostic to downstream tasks.\n\n### Fine-Tuning a Language Model\n\nWe want to make sure we don't train the model to predict special tokens [CLS] and [SEP]; so apply a mask when tokenising by setting `return_speecical_tokens_mask=True`. ","metadata":{}},{"cell_type":"code","source":"def tokenize(batch):\n    return tokenizer(\n        batch[\"text\"], truncation=True, max_length=128, return_special_tokens_mask=True)\n\nds_mlm = ds.map(tokenize, batched=True)\nds_mlm = ds_mlm.remove_columns([\"labels\", \"text\", \"label_ids\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We're missing the mechanism to mask tokens in the input sequence and have the target tokens in the outputs. One way to do is to create a function that masks random tokens and creates labels for these sequences, but this would double the size of the dataset as we also store the target sequence in the dataset and we would use the same masking of a sequence every epoch.\n\nMore elegant solution is to use a data collator; acts as the bridge between the dataset and the model calls. A batch is sampled from the dataset, and the data collator prepares elements in the batch to feed to the model. It concatenates the tensors of each element into a single tensor. We can use it to do masking and label generation on the fly; so don't need to store labels and get new masks every time we sample.\n\nUse `DataCollatorForLanguageModeling`; initialise with model's tokeniser and fraction of tokens we want to mask via `mlm_probability` argument. We'll mask 15% of the tokens, following the procedure in the BERT paper.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForLanguageModeling, set_seed\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, mlm_probability=0.15)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"set_seed(3)\ndata_collator.return_tensors = \"np\" # switch return format to numpy\ninputs = tokenizer(\"Transformers are awesome!\", return_tensors=\"np\")\noutputs = data_collator([{\"input_ids\": inputs[\"input_ids\"][0]}])\n\npd.DataFrame({\n    \"Original tokens\": tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]),\n    \"Masked tokens\": tokenizer.convert_ids_to_tokens(outputs[\"input_ids\"][0]),\n    \"Original input_ids\": original_input_ids,\n    \"Masked input_ids\": masked_input_ids,\n    \"Labels\": outputs[\"labels\"][0]\n}).T","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Replaced ! with mask token; also data collator returns a label array, -100 for original tokens and the token ID for masked tokens. Entries containing -100 are ignored when calculating loss..","metadata":{}},{"cell_type":"code","source":"# switch data collator format back to PyTorch\ndata_collator.return_tensors = \"pt\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForMaskedLM\n\n# fine-tune masked language model\ntraining_args = TrainingArguments(\n    output_dir = f\"{model_ckpt}-issues-128\", per_device_train_batch_size=32,\n    logging_strategy=\"epoch\", evaluation_strategy=\"epoch\", save_strategy=\"no\",\n    num_train_epochs=16, push_to_hub=True, log_level=\"error\", report_to=\"none\"\n)\n\ntrainer = Trainer(\n    model=AutoModelForMaskedLM.from_pretrained('bert-base-uncased'),\n    tokenizer=tokenizer, args=training_args, data_collator=data_collator,\n    train_dataset=ds_mlm[\"unsup\"], eval_dataset=ds_mlm[\"train\"]\n)\n\ntrainer.train()\ntrainer.push_to_hub(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2022-10-05T13:09:25.308087Z","iopub.execute_input":"2022-10-05T13:09:25.308474Z","iopub.status.idle":"2022-10-05T13:09:26.049803Z","shell.execute_reply.started":"2022-10-05T13:09:25.308401Z","shell.execute_reply":"2022-10-05T13:09:26.048604Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/2341188706.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# fine-tune masked language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{model_ckpt}-issues-128\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlogging_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"],"ename":"NameError","evalue":"name 'TrainingArguments' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# use trainers history to look at training and val losses of model\n# all stored in trainer.state.log_history as list of dicts; load as dataframe\n# training and val loss are recorded at different steps so we have gaps in df\ndf_log = pd.DataFrame(trainer.state.log_history)\n\n(df_log.dropna(subset=[\"eval_loss\"]).reset_index()[\"eval_loss\"]\n.plot(label=\"Validation\"))\ndf_log.dropna(subset=[\"loss\"]).reset_index()[\"loss\"].plot(label=\"Train\")\n\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend(loc=\"upper right\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Both train and val loss went down considerably! See if we can also see an improvement when we fine-tune a classifier based on this model...\n\n### Fine-Tuning a Classifier","metadata":{}},{"cell_type":"code","source":"# repeat fine-tuning, but we will load the custom ckpt\n\nmodel_ckpt = f\"{model_ckpt}-issues-128\"\nconfig = AutoConfig.from_pretrained(model_ckpt)\nconfig.num_labels = len(all_labels)\nconfig.problem_type = \"multi_label_classification\"\n\nfor train_slice in train_slices:\n    model = AutoModelForSequenceClassification.from_pretrained(\n    model_ckpt, config=config)\n    \n    trainer = Trainer(\n        model=model, tokenizer=tokenizer, args=training_args_fine_tune,\n        compute_metrics=compute_metrics,\n        train_dataset=ds_enc[\"train\"].select(train_slice),\n        eval_dataset=ds_enc[\"valid\"]   \n    )\n    trainer.train()\n    pred = trainer.predict(ds_enc[\"test\"])\n    metrics = compute_metrics(pred)\n    \n    # DA refers to domain adaptation\n    macro_scores[\"Fine-tune (DA)\"].append(metrics['macro f1'])\n    micro_scores[\"Fine-tune (DA)\"].append(metrics['micro f1'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(micro_scores, macro_scores, train_samples, \"Fine-tune (DA)\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Advantage especially in low data domain; also gain percentage points in regime where there is more labeled data available.\n\nSo domain adaptation can provide a boost to model performance with unlabeled data and a little effort. Consider volumes of data, which would also impact what we get with this method.\n\nFinally.. Some final tricks to take advantage of unlabeled data.\n\n### Advanced Methods","metadata":{}},{"cell_type":"markdown","source":"#### Unsupervised Data Augmentation\n\nA model's predictions should be consistent for an unlabeled example and a slightly distorted one. Consistency is then enforced by minimising KL divergence between predictions of original and distorted examples. \n\nConsistency requirement is incorporated by augmenting the cross-entropy loss with an additional term from unlabeled examples. So one trains a model on labeled data with supervised approach, but constrains the model to make consistent predictions on the unlabeled data.\n\nImpressive results, a handful of labeled examples for BERT gets similar performance to a model trained on thousands of examples. Downside is that you would need a data augmentation pipeline and training takes much longer since you need multiple forward passes to generate the predicted distributions on the unlabeled and augmented examples.\n\n#### Uncertainty-aware self-training\n\nTrain teacher model on the labeled data then use that model to create pseudo-labels on unlabeled data; then a student is trained on pseudo-labeled data and after training becomes the teacher for the next iteration.\n\nTo get an uncertainty measure of a model's predictions, the same input is fed several times through the model with dropout turned on; the variancec in predictions gives a proxy for the certainty of the model on a specific sample.\n\nWith the uncertainty measure the pseudo-labels are sampled with a method called Bayesian Active Learning by Disagreement (BALD). And the teacher constantly gets better at creating pseudo-labels; this increasing the model's performance. And in the end gets a few % of models trained using the full training data with thousands of samples and even beats UDA on several datasets.","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n\nMethods\n- One shot pretrianed model\n- Domain adaptation\n\nDepends on:\n- Amount of labeled data\n- How noisy is data\n- How close is data to pretraining corpus etc..\n\nBest to set up a pipeline and iterate quickly!\n\nWorth to consider trade-off between more complex approaches like UDA and UST vs getting more data. Good to build a validation and test set early on; and can gather more labeled data at every step of the way.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}