{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Text Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","\n","# load 1.5bn parameter version of GPT-2 with a language modelling head\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model_name = \"gpt2-xl\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# iteratively feed inputs and run decoder for 8 timesteps\n","# then at each, pick out the model's logits for last token in prompt\n","# and wrap in softmax to get probability distribution\n","# then pick token with highest probability, add to input sequence and run again\n","\n","import pandas as pd\n","input_txt = \"Transformers are the\"\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","iterations = []\n","n_steps = 8\n","choices_per_step = 5\n","\n","with torch.no_grad():\n","    for _ in range(n_steps):\n","        iteration = dict()\n","        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n","        output = model(input_ids = input_ids)\n","        \n","        # select logics of first batch and the last token and apply softmax\n","        next_token_logits = output.logits[0, -1, :]\n","        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n","        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n","        \n","        # store token with highest probabilities\n","        for choice_idx in rance(choices_per_step):\n","            token_id = sorted_ids[choice_idx]\n","            token_prob = next_token_probs[token_id].cpu().numpy()\n","            token_choice = (f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f})%\")\n","            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n","            \n","        # append predicted next token to input\n","        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n","        iterations.append(iteration)\n","        \n","pd.DataFrame(iterations)"]},{"cell_type":"markdown","metadata":{},"source":["- Interestingly GPT-2 shows that it has internalised knowledge that Transformers is a media franchise\n","- Shows also the iterative nature of text generation; need to decode output tokens and feed back in, one at a time"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# use built-in generate() function to explore more sophisticated decoding methods\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n","print(tokenizer.decode(output[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# prompt to reproduce famous unicorn story\n","max_length = 128\n","input_txt = \"\"\"In a shocking finding, scientist discovered \\\n","a herd of unicorns living in a remote, previously unexplored \\\n","valley, in the Andes Mountains. Even more surprising to the \\\n","researchers was the fact that the unicorns spoke perfect English.\\n\\n\n","\"\"\"\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output_greedy = model.generate(input_ids, max_length=max_length,\n"," do_sample=False)\n","print(tokenizer.decode(output_greedy[0]))"]},{"cell_type":"markdown","metadata":{},"source":["Results are different from OpenAI famous example; revealing a major drawback of greedy search decoding, that it tends to produce repetitive output sequences, undesirable in longer texts. So can miss higher overall probability for short-term probability gains at each word.\n","\n","Fortunately, can do better with a popular method called *beam search decoding*\n","\n","## Beam Search Decoding\n","\n","Keeps track of *top-b* most probable next tokens, where *b* is the number of beams or partial hypotheses. The next set of beams are chosen by considering all possible next-token extensions of existing set and selecting *b* most likely extensions. This is repeated until we reach a maximum length or EOS token; finally the most likely sequence is selected by ranking *b* beams according to their log probabilities.\n","\n","We take log probability as the calculation is the product of conditional probabilities each between [0,1] which can lead to underflow (cannot precisely represent the calculation); as the number becomes untenably small and numerical instability.\n","\n","So then we have the sum of log probabilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","\n","sum([np.log(0.5)] * 1024)"]},{"cell_type":"markdown","metadata":{},"source":["And we have a number we can easily deal with. As we only want to compare relative probabilities; we can do this directly with log probabilities.\n","\n","Next step is to calculate and compare log probabilities of greedy and beam search to see if beam search can increase overall probability. As Transformers return the unnormalized logits for next token given input tokens, we first need to normalize the logits to create a probability distribution over whole vocabulary for each token in the sequence; then select only the token probabilities present in the sequence."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","def log_probs_from_logits(logits, labels):\n","    # log probbility of a single token\n","    logp = F.log_softmax(logits, dim=-1)\n","    logp_label = torch.gather(logp, 2, labels.unsqueeze(2)).squeeze(-1)\n","    return logp_label \n","\n","def sequence_logprob(model, labels, input_len=0):\n","    # sum log probabilities for each token\n","    with torch.no_grad():\n","        output = model(labels)\n","        # as model predicts next token; no need to get logit for the first\n","        # label; and no need for last logit because we have no ground truth token\n","        # for it. Ignores log p of input sequence as these are not generated by \n","        # model\n","        log_probs = log_probs_from_logits(\n","            output.logits[:, :-1, :], labels[:, 1:]\n","        )\n","        seq_log_prob = torch.sum(log_probs[:, input_len:])\n","    return seq_log_prob.cpu().numpy()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# greedy decoder output (default option)\n","logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n","print(tokenizer.decode(output_greedy[0]))\n","print(f\"\\nlog-prob: {logp:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# compare with beam search; use generate() and specify num_beams param\n","# more beams means better potential results; but slower generation process\n","# due to more parallel computations\n","output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)\n","logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n","print(tokenizer.decode(output_beam[0]))\n","print(f\"\\nlog-prob: {logp:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["Much better log prob (higher score) with beam search than greedy decoding. Though we see that beam search also suffers from repetitive text. We can address this with n-gram penalty with `no_repeat_ngram_size` parameter that tracks which n-grams have been seen and sets the next token probability to zero if it would produce a previously seen n-gram."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output_beam = model.generate(\n","    input_ids, max_length=max_length, num_beams=5, do_sample=False, no_repeat_ngram_size=2)\n","logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n","print(tokenizer.decode(output_beam[0]))\n","print(f\"\\nlog-prob: {logp:.2f}\")"]},{"cell_type":"markdown","metadata":{},"source":["Lower score but less repetition; so we can balance num_beams and n_gram penalty to find trade-off between high-probability tokens while reducing repetitions. n_gram penalty is commonly used in applications such as summarisation or machine translation, where factual correctness is important. When factual correctness is less important than the diversity of generated output, eg. in story-telling or chatbot, another alternative to reducing repetitions while improving diversity is to use sampling. We can examine some of the most common sampling methods.\n","\n","## Sampling Methods\n","\n","Simplest: Randomly sample from probability distribution of model's outputs over full vocabulary at each timestep; and can easily control the diversityof output by adding a temperature parameter T that rescales logits before taking softmax. When T<<1; the distribution becomes peaked around origin and rare tokens are suppressed. On the flipside, when T >> 1, the distribution flattens and each token becomes equally likely. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# see how temperature influences text generation\n","output_temp = model.generate(\n","    input_ids, max_length=max_length, do_sample=True, temperature=2.0, top_k=0)\n","print(tokenizer.decode(output_temp[0]))"]},{"cell_type":"markdown","metadata":{},"source":["High temperature has produced mostly gibberish; by accentuating rare tokens. Even have strange grammar and made up words!"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# much cooler temperature\n","output_temp = model.generate(\n","    input_ids, max_length=max_length, do_sample=True, temperature=0.5, top_k=0\n",")\n","print(tokenizer.decode(output_temp[0]))"]},{"cell_type":"markdown","metadata":{},"source":["Temperature allows us to control the quality of samples; but there's a trade-off between coherence (low-temperature) and diversity (high temperature) that one can tune for the case at hand.\n","\n","Another way is to truncate the distribution of the vocabulary, allowing us to adjust the diversity freely with temperature, but in a more limited range that excludes words that would be too strange in the context (low-probability words); the two ways are *top-k* and nucleus (top-p) sampling.\n","\n","## Top-k and Nucleus Sampling\n","\n","*Top-k* and nucleus (top-p) sampling are two popular alternatives or extensions to using temperature. The basic idea is to restrict the number of possible tokens we can sample from at each timestep.\n","\n","From plotting the probability distribution, we get that there is a very high chance (96%) of picking the top 1,000 tokens; so the probability rises quickly to above 90% and saturates close to 100%. To illustrate further, 1% chance of not picking tokens not in top 2,000 (99% chance of picking in top 2,000).\n","\n","Though if we sample hundreds of times there is a significant chance of picking an unlikely token at some point. Picking such tokens when sampling can badly influence the quality of generated text; so usually we want to avoid these unlikely tokens.\n","\n","The idea behind top-k is to avoid low-probability choices by sampling only from *k* tokens with highest probability. So has a fixed cut on long-tail of distribution and ensures we only sample from likely choices."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# generate provides a very easy method to achieve this with top_k argument\n","output_topk = model.generate(input_ids, max_length=max_length, do_sample=True, top_k=50)\n","print(tokenizer.decode(output_topk[0]))"]},{"cell_type":"markdown","metadata":{},"source":["*k* is chosen manually and is independent of the actual distribution; we can find a good *k* value by looking at text quality metrics, though a fixed cutoff may not be very satisfactory.\n","\n","Alternative is a *dynamic cutoff*, where we set a condition to cut off. This is when a certain probability mass in the selection has been reached, eg. 95%. Arrange in descending order and add one token after another from top of list until sum of probabilities is 95%; so have a smaller range of tokens to select from."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# again; generate() function has an argument to activate top-p sampling\n","output_topp = model.generate(input_ids, max_length=max_length, do_sample=True, top_p=0.90)\n","print(tokenizer.decode(output_topp[0]))"]},{"cell_type":"markdown","metadata":{},"source":["Produced a more coherent story. We can combine this with previous approaches ie. top_k and top_p to choose tokens with probability mass 90% from a pool of at most 50 tokens. Can also apply beam search when sampling to build up the beam instead of greedy search.\n","\n","### Selecting the \"Best\" Decoding Method\n","\n","No universally best, it very much is a trade-off depending on the nature of the task you are generating for. \n","\n","For a precise arithmetic task, one should lower the temperature for more deterministic methods like greedy search in combination with beam to guarantee getting the most likely answer. \n","\n","Or if longer and more creative texts, then sampling methods and increase temperature or use mix of top-k and nucleus sampling.\n","\n","## Conclusion\n","\n","Looked at text-generation, very different from NLU previously. Generation of text requires at least one forward pass per generated token and more if we use beam search; therefore makes text generation computationally demanding, and one needs the right infrastructure to run at scale. \n","\n","Additionally, a good decoding method can make a big difference in transforming the model output probabilities to discrete tokens and can improve the text quality; this is something that requires some experimentation and a subjective evaluation of generated texts.\n","\n","Though this doesn't have to be subjective! We can select a performance metric that reflects the problem we want to solve, and luckily, there is a wide range of such choices."]},{"cell_type":"markdown","metadata":{},"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.13 ('venv')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"6138c66c79b31a4bca4eaa4f91251832e85484788f371bcf4523f4ab34ea2bc5"}}},"nbformat":4,"nbformat_minor":4}
