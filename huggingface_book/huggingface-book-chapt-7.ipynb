{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from datasets import get_dataset_config_names\n\ndomains = get_dataset_config_names(\"subjqa\")\ndomains","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-20T14:07:55.073293Z","iopub.execute_input":"2022-09-20T14:07:55.074379Z","iopub.status.idle":"2022-09-20T14:07:56.427617Z","shell.execute_reply.started":"2022-09-20T14:07:55.074257Z","shell.execute_reply":"2022-09-20T14:07:56.426538Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.65k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"762f0b4c1e1344519a0b777633b2e975"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acd50fa336d048368339e286d21cb96a"}},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"['books', 'electronics', 'grocery', 'movies', 'restaurants', 'tripadvisor']"},"metadata":{}}]},{"cell_type":"code","source":"# focus on QA for electronics domain\nfrom datasets import load_dataset\n\nsubjqa = load_dataset(\"subjqa\", name=\"electronics\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:07:56.430007Z","iopub.execute_input":"2022-09-20T14:07:56.430901Z","iopub.status.idle":"2022-09-20T14:08:12.441666Z","shell.execute_reply.started":"2022-09-20T14:07:56.430862Z","shell.execute_reply":"2022-09-20T14:08:12.440718Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset subjqa/electronics (download: 10.86 MiB, generated: 3.01 MiB, post-processed: Unknown size, total: 13.86 MiB) to /root/.cache/huggingface/datasets/subjqa/electronics/1.1.0/e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4213fb8cc60b49158e6973aad7ce599c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1295 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/255 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset subjqa downloaded and prepared to /root/.cache/huggingface/datasets/subjqa/electronics/1.1.0/e5588f9298ff2d70686a00cc377e4bdccf4e32287459e3c6baf2dc5ab57fe7fd. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fca93e6f72d14fc4bb3a7f4ef3ca43f8"}},"metadata":{}}]},{"cell_type":"code","source":"# stores answer as nested dict\nprint(subjqa[\"train\"][\"answers\"][1])","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.443298Z","iopub.execute_input":"2022-09-20T14:08:12.443965Z","iopub.status.idle":"2022-09-20T14:08:12.479212Z","shell.execute_reply.started":"2022-09-20T14:08:12.443928Z","shell.execute_reply":"2022-09-20T14:08:12.478343Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"{'text': ['Bass is weak as expected', 'Bass is weak as expected, even with EQ adjusted up'], 'answer_start': [1302, 1302], 'answer_subj_level': [1, 1], 'ans_subj_score': [0.5083333253860474, 0.5083333253860474], 'is_ans_subjective': [True, True]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# flatten with flatten() and convert splits to Pandas Dataframes\nimport pandas as pd\n\ndfs = {split: dset.to_pandas() for split, dset in subjqa.flatten().items()}\n\nfor split, df in dfs.items():\n    print(f\"Number of questions in {split}: {df['id'].nunique()}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.482103Z","iopub.execute_input":"2022-09-20T14:08:12.482481Z","iopub.status.idle":"2022-09-20T14:08:12.522462Z","shell.execute_reply.started":"2022-09-20T14:08:12.482427Z","shell.execute_reply":"2022-09-20T14:08:12.521578Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Number of questions in train: 1295\nNumber of questions in test: 358\nNumber of questions in validation: 255\n","output_type":"stream"}]},{"cell_type":"code","source":"dfs['train'].sample(1)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.525471Z","iopub.execute_input":"2022-09-20T14:08:12.525757Z","iopub.status.idle":"2022-09-20T14:08:12.551281Z","shell.execute_reply.started":"2022-09-20T14:08:12.525732Z","shell.execute_reply":"2022-09-20T14:08:12.550646Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"           domain nn_mod nn_asp        query_mod query_asp  \\\n1260  electronics  right   size  more reasonable     price   \n\n                          q_reviews_id  question_subj_level  ques_subj_score  \\\n1260  bd9a52715ed9ceabbc2115176d70e6d4                    5              0.2   \n\n      is_ques_subjective                         review_id  \\\n1260               False  d4f9c6f9af5e705eb1bf6fa1b019624a   \n\n                                    id       title  \\\n1260  62326ee218c2e24b73f9b23f55208b72  B00DVFLJDS   \n\n                                                context  \\\n1260  This smartphone meets my needs.. The size is j...   \n\n                           question answers.text answers.answer_start  \\\n1260  How much is the price to pay?           []                   []   \n\n     answers.answer_subj_level answers.ans_subj_score  \\\n1260                        []                     []   \n\n     answers.is_ans_subjective  \n1260                        []  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>domain</th>\n      <th>nn_mod</th>\n      <th>nn_asp</th>\n      <th>query_mod</th>\n      <th>query_asp</th>\n      <th>q_reviews_id</th>\n      <th>question_subj_level</th>\n      <th>ques_subj_score</th>\n      <th>is_ques_subjective</th>\n      <th>review_id</th>\n      <th>id</th>\n      <th>title</th>\n      <th>context</th>\n      <th>question</th>\n      <th>answers.text</th>\n      <th>answers.answer_start</th>\n      <th>answers.answer_subj_level</th>\n      <th>answers.ans_subj_score</th>\n      <th>answers.is_ans_subjective</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1260</th>\n      <td>electronics</td>\n      <td>right</td>\n      <td>size</td>\n      <td>more reasonable</td>\n      <td>price</td>\n      <td>bd9a52715ed9ceabbc2115176d70e6d4</td>\n      <td>5</td>\n      <td>0.2</td>\n      <td>False</td>\n      <td>d4f9c6f9af5e705eb1bf6fa1b019624a</td>\n      <td>62326ee218c2e24b73f9b23f55208b72</td>\n      <td>B00DVFLJDS</td>\n      <td>This smartphone meets my needs.. The size is j...</td>\n      <td>How much is the price to pay?</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"qa_cols = [\"title\", \"question\", \"answers.text\", \"answers.answer_start\", \"context\"]\nsample_df = dfs[\"train\"][qa_cols].sample(2, random_state=7)\nsample_df","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.552800Z","iopub.execute_input":"2022-09-20T14:08:12.553555Z","iopub.status.idle":"2022-09-20T14:08:12.569770Z","shell.execute_reply.started":"2022-09-20T14:08:12.553510Z","shell.execute_reply":"2022-09-20T14:08:12.568960Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"           title                        question                answers.text  \\\n791   B005DKZTMG  Does the keyboard lightweight?  [this keyboard is compact]   \n1159  B00AAIPT76             How is the battery?                          []   \n\n     answers.answer_start                                            context  \n791                 [215]  I really like this keyboard.  I give it 4 star...  \n1159                   []  I bought this after the first spare gopro batt...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n      <th>question</th>\n      <th>answers.text</th>\n      <th>answers.answer_start</th>\n      <th>context</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>791</th>\n      <td>B005DKZTMG</td>\n      <td>Does the keyboard lightweight?</td>\n      <td>[this keyboard is compact]</td>\n      <td>[215]</td>\n      <td>I really like this keyboard.  I give it 4 star...</td>\n    </tr>\n    <tr>\n      <th>1159</th>\n      <td>B00AAIPT76</td>\n      <td>How is the battery?</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>I bought this after the first spare gopro batt...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"1. Questions are not always grammatically correct\n2. Empty answers.text denotes \"unanswerable\" questions whose answer cannot be found in review","metadata":{}},{"cell_type":"code","source":"# take slice of span of text that corresponds to answer\nstart_idx = sample_df[\"answers.answer_start\"].iloc[0][0]\nend_idx = start_idx + len(sample_df[\"answers.text\"].iloc[0][0])\nsample_df[\"context\"].iloc[0][start_idx:end_idx]","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.571575Z","iopub.execute_input":"2022-09-20T14:08:12.572185Z","iopub.status.idle":"2022-09-20T14:08:12.581699Z","shell.execute_reply.started":"2022-09-20T14:08:12.572148Z","shell.execute_reply":"2022-09-20T14:08:12.580574Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'this keyboard is compact'"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.583912Z","iopub.execute_input":"2022-09-20T14:08:12.584174Z","iopub.status.idle":"2022-09-20T14:08:12.593935Z","shell.execute_reply.started":"2022-09-20T14:08:12.584151Z","shell.execute_reply":"2022-09-20T14:08:12.592654Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# get a feel of the questions in the training set; count questions that begin\n# with a few common starting words\ncounts = {}\nquestion_types = [\"What\", \"How\", \"Is\", \"Does\", \"Do\", \"Was\", \"Where\", \"Why\"]\n\nfor q in question_types:\n    counts[q] = dfs[\"train\"][\"question\"].str.startswith(q).value_counts()[True]\n    \npd.Series(counts).sort_values().plot.barh()\nplt.title(\"Frequency of Question Types\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.595708Z","iopub.execute_input":"2022-09-20T14:08:12.595971Z","iopub.status.idle":"2022-09-20T14:08:12.855862Z","shell.execute_reply.started":"2022-09-20T14:08:12.595947Z","shell.execute_reply":"2022-09-20T14:08:12.854987Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":"\"How\", \"What\" and \"Is\" are the most common. Let's look at some examples..","metadata":{}},{"cell_type":"code","source":"for question_type in [\"How\", \"What\", \"Is\"]:\n    for question in (\n    dfs[\"train\"][dfs[\"train\"].question.str.startswith(question_type)]\n    .sample(n=3, random_state=42)['question']):\n        print(question)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.859878Z","iopub.execute_input":"2022-09-20T14:08:12.860158Z","iopub.status.idle":"2022-09-20T14:08:12.874782Z","shell.execute_reply.started":"2022-09-20T14:08:12.860133Z","shell.execute_reply":"2022-09-20T14:08:12.873355Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"How is the camera?\nHow do you like the control?\nHow fast is the charger?\nWhat is direction?\nWhat is the quality of the construction of the bag?\nWhat is your impression of the product?\nIs this how zoom works?\nIs sound clear?\nIs it a wireless keyboard?\n","output_type":"stream"}]},{"cell_type":"markdown","source":"To extract answers from text, we must:\n- Frame the supervised learning problem\n- Tokenise and encode text for QA tasks\n- Deal with long passages that exceeded a model's maximum content size","metadata":{}},{"cell_type":"markdown","source":"**Framing the Problem**: Span classification, must predict start and end labels that a model needs to predict. Can generally start with a model that has been fine-tuned on a large-scale QA dataset like SQuAD. We can actually start with a fine-tuned model as the structure of the labels are the same across datasets.\n\n**Tokenising Text for QA**: ","metadata":{}},{"cell_type":"code","source":"# load MiniLM model checkpoint from HuggingFace Hub\nfrom transformers import AutoTokenizer\n\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:12.876419Z","iopub.execute_input":"2022-09-20T14:08:12.877085Z","iopub.status.idle":"2022-09-20T14:08:15.039103Z","shell.execute_reply.started":"2022-09-20T14:08:12.877050Z","shell.execute_reply":"2022-09-20T14:08:15.038087Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/107 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7965492405740b58b5bbe68d6dd7860"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/477 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"544dcddf351d49f7965251170ac4bd79"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79df38739e9244ff855f82b706243d5f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9cd9a244d2644355bf8262c54bec3264"}},"metadata":{}}]},{"cell_type":"code","source":"# try to extract an answer from short passage of text\n# for extractive, must provide (question, context) pairs\nquestion = \"How much music can this hold?\"\ncontext = \"\"\"An MP3 is about 1MB/minute, so about 6000 hours depending on file size.\"\"\"\n# use tensors as we'll need them to run forward pass through model\ninputs = tokenizer(question, context, return_tensors=\"pt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:15.040400Z","iopub.execute_input":"2022-09-20T14:08:15.040784Z","iopub.status.idle":"2022-09-20T14:08:16.649395Z","shell.execute_reply.started":"2022-09-20T14:08:15.040748Z","shell.execute_reply":"2022-09-20T14:08:16.648284Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"inputs","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:16.650837Z","iopub.execute_input":"2022-09-20T14:08:16.651211Z","iopub.status.idle":"2022-09-20T14:08:16.659471Z","shell.execute_reply.started":"2022-09-20T14:08:16.651175Z","shell.execute_reply":"2022-09-20T14:08:16.658498Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101,  2129,  2172,  2189,  2064,  2023,  2907,  1029,   102,  2019,\n         23378,  2003,  2055,  1015, 14905,  1013,  3371,  1010,  2061,  2055,\n         25961,  2847,  5834,  2006,  5371,  2946,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1]])}"},"metadata":{}}]},{"cell_type":"markdown","source":"0 indicates a question token, 1 indicates a context token.","metadata":{}},{"cell_type":"code","source":"# decode input_ids tensor - to understand how tokenizer formats inputs for QA tasks\nprint(tokenizer.decode(inputs[\"input_ids\"][0]))","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:16.660960Z","iopub.execute_input":"2022-09-20T14:08:16.661560Z","iopub.status.idle":"2022-09-20T14:08:21.028332Z","shell.execute_reply.started":"2022-09-20T14:08:16.661515Z","shell.execute_reply":"2022-09-20T14:08:21.027165Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[CLS] how much music can this hold? [SEP] an mp3 is about 1mb / minute, so about 6000 hours depending on file size. [SEP]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Has layout `[CLS] question tokens [SEP] context tokens [SEP]`; where first `[SEP]` token is determined by `token_type_ids`.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForQuestionAnswering\n\n# instantiate model with QA head\nmodel = AutoModelForQuestionAnswering.from_pretrained(model_ckpt)\n\n# run inputs with forward pass\nwith torch.no_grad():\n    outputs = model(**inputs)\nprint(outputs)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:21.030144Z","iopub.execute_input":"2022-09-20T14:08:21.030524Z","iopub.status.idle":"2022-09-20T14:08:25.232297Z","shell.execute_reply.started":"2022-09-20T14:08:21.030488Z","shell.execute_reply":"2022-09-20T14:08:25.231053Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/127M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"673185961c1848fb93d388ae6f1003f7"}},"metadata":{}},{"name":"stdout","text":"QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-1.1224, -4.7834, -5.3935, -5.2447, -5.2899, -5.4980, -4.9885, -6.1783,\n         -1.1224,  0.3651, -0.0807, -1.5551,  3.9556,  4.9206, -2.7833, -3.7461,\n         -1.7211, -4.6096, -1.3769,  4.0256,  4.9715, -0.2477, -3.0354, -4.8180,\n         -2.2386, -3.4867, -3.5462, -1.1224]]), end_logits=tensor([[-1.0707, -5.4840, -5.0304, -5.1770, -5.4496, -5.5120, -5.2013, -4.6178,\n         -1.0707, -3.7223, -0.7583, -3.6969, -2.9205, -1.7346,  0.3122, -2.8688,\n          4.8807,  0.2692, -3.1317, -3.2404,  0.8239,  5.6645, -0.2568, -4.8910,\n         -3.1677, -0.0130,  1.6739, -1.0707]]), hidden_states=None, attentions=None)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Take the start and end logits to get the answer span.","metadata":{}},{"cell_type":"code","source":"start_logits = outputs.start_logits\nend_logits = outputs.end_logits","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:25.234070Z","iopub.execute_input":"2022-09-20T14:08:25.235042Z","iopub.status.idle":"2022-09-20T14:08:25.242396Z","shell.execute_reply.started":"2022-09-20T14:08:25.234985Z","shell.execute_reply":"2022-09-20T14:08:25.240248Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"print(f\"Input IDs shape: {inputs.input_ids.size()}\")\nprint(f\"Start logits shape: {start_logits.size()}\")\nprint(f\"End logits shape: {end_logits.size()}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:25.244896Z","iopub.execute_input":"2022-09-20T14:08:25.245614Z","iopub.status.idle":"2022-09-20T14:08:25.263322Z","shell.execute_reply.started":"2022-09-20T14:08:25.245573Z","shell.execute_reply":"2022-09-20T14:08:25.262336Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Input IDs shape: torch.Size([1, 28])\nStart logits shape: torch.Size([1, 28])\nEnd logits shape: torch.Size([1, 28])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We have a start and end logit associated with each input token. Positive logits is a more likely candidate for start and end token, so we can compute the argmax and slice the span from the inputs. That would give us our answer!","metadata":{}},{"cell_type":"code","source":"import torch\n\nstart_idx = torch.argmax(start_logits)\nend_idx = torch.argmax(end_logits) + 1\nanswer_span = inputs[\"input_ids\"][0][start_idx:end_idx]\nanswer = tokenizer.decode(answer_span)\nprint(f\"Question: {question}\")\nprint(f\"Answer: {answer}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:25.265128Z","iopub.execute_input":"2022-09-20T14:08:25.265502Z","iopub.status.idle":"2022-09-20T14:08:25.276534Z","shell.execute_reply.started":"2022-09-20T14:08:25.265467Z","shell.execute_reply":"2022-09-20T14:08:25.275369Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Question: How much music can this hold?\nAnswer: 6000 hours\n","output_type":"stream"}]},{"cell_type":"markdown","source":"It works! Huggingface transformers conveniently wraps it all in a dedicated pipeline; we can instantiate and pass through our tokenizer and fine-tuned model as follows:","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\npipe = pipeline(\"question-answering\", model=model, tokenizer=tokenizer)\npipe(question=question, context=context, topk=3)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:25.278086Z","iopub.execute_input":"2022-09-20T14:08:25.278682Z","iopub.status.idle":"2022-09-20T14:08:28.052986Z","shell.execute_reply.started":"2022-09-20T14:08:25.278539Z","shell.execute_reply":"2022-09-20T14:08:28.052073Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/transformers/pipelines/question_answering.py:189: UserWarning: topk parameter is deprecated, use top_k instead\n  warnings.warn(\"topk parameter is deprecated, use top_k instead\", UserWarning)\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"[{'score': 0.24360179901123047,\n  'start': 37,\n  'end': 47,\n  'answer': '6000 hours'},\n {'score': 0.23150932788848877,\n  'start': 16,\n  'end': 47,\n  'answer': '1MB/minute, so about 6000 hours'},\n {'score': 0.10571775585412979,\n  'start': 16,\n  'end': 26,\n  'answer': '1MB/minute'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Also returns model's probability estimate (softmax over logits); and have multiple answers with `topk` parameter. Sometimes possible to have questions with no answer; in which case the model will assign a high start and end score to the `CLS` token, and pipeline maps this to an empty string.","metadata":{}},{"cell_type":"code","source":"pipe(question=\"Why is there no data?\", context=context, handle_imposible_answer=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:28.054350Z","iopub.execute_input":"2022-09-20T14:08:28.054981Z","iopub.status.idle":"2022-09-20T14:08:28.107437Z","shell.execute_reply.started":"2022-09-20T14:08:28.054931Z","shell.execute_reply":"2022-09-20T14:08:28.106383Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'score': 0.00033265346428379416,\n 'start': 10,\n 'end': 70,\n 'answer': 'about 1MB/minute, so about 6000 hours depending on file size'}"},"metadata":{}}]},{"cell_type":"markdown","source":"Taking start and end index can take out-of-scope answers by selecting tokens belonging to qestion instead of context. In practice, the pipeline computes best start-end indices combination subject to constraints, such as being in-scope; start indices precede end index and so on..","metadata":{}},{"cell_type":"markdown","source":"### Long Passages","metadata":{}},{"cell_type":"code","source":"# set retrn_overflowing=True in tokenizer to enable sliding window\n# size of which is controlled by max_seq_length argument\n# and size of stride is controlled by doc_stride\nexample = dfs[\"train\"].iloc[0][[\"question\", \"context\"]]\ntokenized_example = tokenizer(example[\"question\"], example[\"context\"],\n                             return_overflowing_tokens=True, max_length=100,\n                             stride=25)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:28.109120Z","iopub.execute_input":"2022-09-20T14:08:28.109586Z","iopub.status.idle":"2022-09-20T14:08:28.119845Z","shell.execute_reply.started":"2022-09-20T14:08:28.109548Z","shell.execute_reply":"2022-09-20T14:08:28.118695Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stderr","text":"Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"}]},{"cell_type":"code","source":"# now get list of input ids, one for each window; check tokens we have in each window\nfor idx, window in enumerate(tokenized_example[\"input_ids\"]):\n    print(f\"Window #{idx} has {len(window)} tokens\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:28.121057Z","iopub.execute_input":"2022-09-20T14:08:28.124106Z","iopub.status.idle":"2022-09-20T14:08:28.130604Z","shell.execute_reply.started":"2022-09-20T14:08:28.124074Z","shell.execute_reply":"2022-09-20T14:08:28.129342Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Window #0 has 100 tokens\nWindow #1 has 88 tokens\n","output_type":"stream"}]},{"cell_type":"code","source":"# can see where two windows overlap by decoding inputs\nfor window in tokenized_example[\"input_ids\"]:\n    print(f\"{tokenizer.decode(window)} \\n\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:28.132329Z","iopub.execute_input":"2022-09-20T14:08:28.132755Z","iopub.status.idle":"2022-09-20T14:08:29.383699Z","shell.execute_reply.started":"2022-09-20T14:08:28.132703Z","shell.execute_reply":"2022-09-20T14:08:29.382624Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[CLS] how is the bass? [SEP] i have had koss headphones in the past, pro 4aa and qz - 99. the koss portapro is portable and has great bass response. the work great with my android phone and can be \" rolled up \" to be carried in my motorcycle jacket or computer bag without getting crunched. they are very light and don't feel heavy or bear down on your ears even after listening to music with them on all day. the sound is [SEP] \n\n[CLS] how is the bass? [SEP] and don't feel heavy or bear down on your ears even after listening to music with them on all day. the sound is night and day better than any ear - bud could be and are almost as good as the pro 4aa. they are \" open air \" headphones so you cannot match the bass to the sealed types, but it comes close. for $ 32, you cannot go wrong. [SEP] \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So we have intuition on how QA model can extract answers from text; now we can look at other components needed to build E2E QA pipeline. \n\nTypically QA systems retrieve based on retriever-reader architecture:\n- *Retriever*: Retrieve relevant documents for a query; are categorised as either *sparse* or *dense*. Sparse retrievers use word frequencies to represent each document as a sparse vector. The relevancy of each query and a document is then determined by computing an inner product of the vectors. On the other hand, dense retrievers use encoders like transformers to represent the query and document as contextualised embeddings (which are dense vectors). Dense vectors encode semantic meaning, so improve search accuracy by understanding query context.\n- *Reader*: Is responsible for extracting an answer from the documents provided by the reader. Usually a reading comprehension model; though also exist are models that can generate free-form answers\n    - There may be other post-processing of retriever to eliminate noisy documents\n\nAlso two more components in QA pipeline with Haystack (based on retriever-reader architecture, abstracts complexity involved in building these systems, and integrates tightly with HFTransformers).:\n- *Document Store*: Document-oriented database that stores documents and metadata which are provided to the retriever at query time\n- *Pipeline*: Combines all components of QA system to enable custom query flows, merging documents from multiple retrievers, and more.\n\nWe will use Elasticsearch as the search engine/document store as it has full-text search features.","metadata":{}},{"cell_type":"code","source":"# install ElasticSearch\nurl = \"\"\"https://artifacts.elastic.co/downloads/elasticsearch/\\\nelasticsearch-7.9.2-linux-x86_64.tar.gz\"\"\"\n!wget -nc -q {url}\n!tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:29.385475Z","iopub.execute_input":"2022-09-20T14:08:29.386767Z","iopub.status.idle":"2022-09-20T14:08:47.706788Z","shell.execute_reply.started":"2022-09-20T14:08:29.386724Z","shell.execute_reply":"2022-09-20T14:08:47.705370Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Popen() to spawn a new process, as we are inside a Jupyter notebook\n# also run the subprocess in the background using chown shell\nimport os\nfrom subprocess import Popen, PIPE, STDOUT\n\n# Run Elastisearch as a background process\n!chown -R daemon:daemon elasticsearch-7.9.2\n# specify program we wish to execute; \n# PIPE creates a new pipe for standard output\n# STDOUT collects errors in the same pipe\n# preexec_fn specifies the subprocess id we wish to use\nes_server = Popen(args=['elasticsearch-7.9.2/bin/elasticsearch'],\n stdout=PIPE, stderr=STDOUT, preexec_fn=lambda: os.setuid(1))\n# Wait until\n!sleep 30","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:08:47.709078Z","iopub.execute_input":"2022-09-20T14:08:47.709476Z","iopub.status.idle":"2022-09-20T14:09:20.027036Z","shell.execute_reply.started":"2022-09-20T14:08:47.709436Z","shell.execute_reply":"2022-09-20T14:09:20.025812Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"# test connection to local port 9200\n!curl -X GET \"localhost:9200/?pretty\"","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:09:20.029009Z","iopub.execute_input":"2022-09-20T14:09:20.029475Z","iopub.status.idle":"2022-09-20T14:09:21.226319Z","shell.execute_reply.started":"2022-09-20T14:09:20.029431Z","shell.execute_reply":"2022-09-20T14:09:21.225147Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n{\n  \"name\" : \"86a4e7f7b413\",\n  \"cluster_name\" : \"elasticsearch\",\n  \"cluster_uuid\" : \"FvXHlBcdSwmp0CsIECDopg\",\n  \"version\" : {\n    \"number\" : \"7.9.2\",\n    \"build_flavor\" : \"default\",\n    \"build_type\" : \"tar\",\n    \"build_hash\" : \"d34da0ea4a966c4e49417f2da2f244e3e97b4e6e\",\n    \"build_date\" : \"2020-09-23T00:45:33.626720Z\",\n    \"build_snapshot\" : false,\n    \"lucene_version\" : \"8.6.2\",\n    \"minimum_wire_compatibility_version\" : \"6.8.0\",\n    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"\n  },\n  \"tagline\" : \"You Know, for Search\"\n}\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install farm-haystack","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:09:21.229788Z","iopub.execute_input":"2022-09-20T14:09:21.230117Z","iopub.status.idle":"2022-09-20T14:09:53.412113Z","shell.execute_reply.started":"2022-09-20T14:09:21.230084Z","shell.execute_reply":"2022-09-20T14:09:53.410642Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nCollecting farm-haystack\n  Downloading farm_haystack-1.8.0-py3-none-any.whl (666 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m666.4/666.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting elastic-apm\n  Downloading elastic_apm-6.12.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.4/381.4 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting posthog\n  Downloading posthog-2.1.2-py2.py3-none-any.whl (32 kB)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (2.5)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (4.6.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (2.28.1)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (1.7.3)\nCollecting rapidfuzz<3,>=2.0.15\n  Downloading rapidfuzz-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: more-itertools in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (8.14.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (0.3.5.1)\nCollecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (4.12.0)\nCollecting azure-ai-formrecognizer==3.2.0b2\n  Downloading azure_ai_formrecognizer-3.2.0b2-py2.py3-none-any.whl (219 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m219.7/219.7 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torch<1.13,>1.9 in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (1.11.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (1.3.5)\nRequirement already satisfied: mmh3 in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (3.0.0)\nCollecting python-docx\n  Downloading python-docx-0.8.11.tar.gz (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting huggingface-hub<0.8.0,>=0.5.0\n  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.2/86.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting sentence-transformers>=2.2.0\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers==4.20.1 in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (4.20.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (4.64.0)\nCollecting mlflow\n  Downloading mlflow-1.29.0-py3-none-any.whl (16.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting langdetect\n  Downloading langdetect-1.0.9.tar.gz (981 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m52.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting quantulum3\n  Downloading quantulum3-0.7.10-py3-none-any.whl (10.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.7/10.7 MB\u001b[0m \u001b[31m83.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scikit-learn>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (1.0.2)\nCollecting tika\n  Downloading tika-1.24.tar.gz (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting elasticsearch<7.11,>=7.7\n  Downloading elasticsearch-7.10.1-py2.py3-none-any.whl (322 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pydantic in /opt/conda/lib/python3.7/site-packages (from farm-haystack) (1.8.2)\nCollecting azure-core<2.0.0,>=1.13.0\n  Downloading azure_core-1.25.1-py3-none-any.whl (178 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.8/178.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting azure-common~=1.1\n  Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\nRequirement already satisfied: six>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from azure-ai-formrecognizer==3.2.0b2->farm-haystack) (1.15.0)\nCollecting msrest>=0.6.21\n  Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (2021.11.10)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (0.12.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==4.20.1->farm-haystack) (3.7.1)\nRequirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from elasticsearch<7.11,>=7.7->farm-haystack) (1.26.12)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from elasticsearch<7.11,>=7.7->farm-haystack) (2022.6.15)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<0.8.0,>=0.5.0->farm-haystack) (4.3.0)\nCollecting jarowinkler<2.0.0,>=1.2.0\n  Downloading jarowinkler-1.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (104 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->farm-haystack) (1.0.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=1.0.0->farm-haystack) (3.1.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=2.2.0->farm-haystack) (0.12.0)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from sentence-transformers>=2.2.0->farm-haystack) (0.1.97)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->farm-haystack) (3.8.0)\nRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->farm-haystack) (0.18.1)\nRequirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->farm-haystack) (21.4.0)\nRequirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->farm-haystack) (5.8.0)\nRequirement already satisfied: pytz<2023 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (2022.1)\nCollecting prometheus-flask-exporter<1\n  Downloading prometheus_flask_exporter-0.20.3-py3-none-any.whl (18 kB)\nRequirement already satisfied: cloudpickle<3 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (2.1.0)\nRequirement already satisfied: docker<7,>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (5.0.3)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (0.4)\nRequirement already satisfied: gitpython<4,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (3.1.27)\nCollecting gunicorn<21\n  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting databricks-cli<1,>=0.8.7\n  Downloading databricks-cli-0.17.3.tar.gz (77 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (0.4.2)\nRequirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (8.0.4)\nRequirement already satisfied: Flask<3 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (2.2.2)\nRequirement already satisfied: sqlalchemy<2,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (1.4.39)\nRequirement already satisfied: alembic<2 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (1.8.1)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from mlflow->farm-haystack) (3.19.4)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->farm-haystack) (2.8.2)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->farm-haystack) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->farm-haystack) (3.3)\nRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->farm-haystack) (5.1.1)\nCollecting monotonic>=1.5\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nCollecting backoff<2.0.0,>=1.10.0\n  Downloading backoff-1.11.1-py2.py3-none-any.whl (13 kB)\nRequirement already satisfied: lxml>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from python-docx->farm-haystack) (4.9.1)\nCollecting inflect\n  Downloading inflect-6.0.0-py3-none-any.whl (34 kB)\nCollecting num2words\n  Downloading num2words-0.5.12-py3-none-any.whl (125 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tika->farm-haystack) (59.8.0)\nRequirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic<2->mlflow->farm-haystack) (1.2.1)\nRequirement already satisfied: pyjwt>=1.7.0 in /opt/conda/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack) (2.4.0)\nRequirement already satisfied: oauthlib>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack) (3.2.0)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli<1,>=0.8.7->mlflow->farm-haystack) (0.8.10)\nRequirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<7,>=4.0.0->mlflow->farm-haystack) (1.3.3)\nRequirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.7/site-packages (from Flask<3->mlflow->farm-haystack) (3.1.2)\nRequirement already satisfied: Werkzeug>=2.2.2 in /opt/conda/lib/python3.7/site-packages (from Flask<3->mlflow->farm-haystack) (2.2.2)\nRequirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.7/site-packages (from Flask<3->mlflow->farm-haystack) (2.1.2)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython<4,>=2.1.0->mlflow->farm-haystack) (4.0.9)\nRequirement already satisfied: requests-oauthlib>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from msrest>=0.6.21->azure-ai-formrecognizer==3.2.0b2->farm-haystack) (1.3.1)\nCollecting isodate>=0.6.0\n  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers==4.20.1->farm-haystack) (3.0.9)\nRequirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter<1->mlflow->farm-haystack) (0.14.1)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy<2,>=1.4.0->mlflow->farm-haystack) (1.1.2)\nRequirement already satisfied: docopt>=0.6.2 in /opt/conda/lib/python3.7/site-packages (from num2words->quantulum3->farm-haystack) (0.6.2)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->sentence-transformers>=2.2.0->farm-haystack) (9.1.1)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=2.1.0->mlflow->farm-haystack) (3.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=3.0->Flask<3->mlflow->farm-haystack) (2.1.1)\nBuilding wheels for collected packages: sentence-transformers, langdetect, python-docx, seqeval, tika, databricks-cli\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=f088ad08f33988bbd0c27746766a9a882fd8196cab64fcf48b61f3b4576a06ed\n  Stored in directory: /root/.cache/pip/wheels/bf/06/fb/d59c1e5bd1dac7f6cf61ec0036cc3a10ab8fecaa6b2c3d3ee9\n  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=e913c803aed79b1850d5c03a342685ae857366556b06288e147aaa6299565c35\n  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n  Building wheel for python-docx (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for python-docx: filename=python_docx-0.8.11-py3-none-any.whl size=184507 sha256=caecedb12610f0c9ca270c52c909b2401eef23aeb4c376ad556682971495169a\n  Stored in directory: /root/.cache/pip/wheels/f6/6f/b9/d798122a8b55b74ad30b5f52b01482169b445fbb84a11797a6\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=bfd3cd2d5cbe5d246b6501a2ebbbc3c76946071af3718a159fa766014f5cdcd8\n  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n  Building wheel for tika (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32893 sha256=0d824453abe10aa0737b58692dbd0d2da27f411bf1b5f861d352a4dabc30c86b\n  Stored in directory: /root/.cache/pip/wheels/ec/2b/38/58ff05467a742e32f67f5d0de048fa046e764e2fbb25ac93f3\n  Building wheel for databricks-cli (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for databricks-cli: filename=databricks_cli-0.17.3-py3-none-any.whl size=139102 sha256=8b5eef9f1af40362e9f1b38649a85c681d942957ca96364bcbfa37893998f854\n  Stored in directory: /root/.cache/pip/wheels/3f/73/87/c1e4b2145eb6049bb6c9aaf7ea1e38302b77ca219b6fef5d5c\nSuccessfully built sentence-transformers langdetect python-docx seqeval tika databricks-cli\nInstalling collected packages: monotonic, azure-common, querystring-parser, python-docx, num2words, langdetect, jarowinkler, isodate, gunicorn, elasticsearch, elastic-apm, backoff, tika, rapidfuzz, posthog, inflect, huggingface-hub, azure-core, seqeval, quantulum3, msrest, databricks-cli, sentence-transformers, prometheus-flask-exporter, azure-ai-formrecognizer, mlflow, farm-haystack\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.8.1\n    Uninstalling huggingface-hub-0.8.1:\n      Successfully uninstalled huggingface-hub-0.8.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncached-path 1.1.5 requires huggingface-hub<0.9.0,>=0.8.1, but you have huggingface-hub 0.7.0 which is incompatible.\nallennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed azure-ai-formrecognizer-3.2.0b2 azure-common-1.1.28 azure-core-1.25.1 backoff-1.11.1 databricks-cli-0.17.3 elastic-apm-6.12.0 elasticsearch-7.10.1 farm-haystack-1.8.0 gunicorn-20.1.0 huggingface-hub-0.7.0 inflect-6.0.0 isodate-0.6.1 jarowinkler-1.2.1 langdetect-1.0.9 mlflow-1.29.0 monotonic-1.6 msrest-0.7.1 num2words-0.5.12 posthog-2.1.2 prometheus-flask-exporter-0.20.3 python-docx-0.8.11 quantulum3-0.7.10 querystring-parser-1.2.4 rapidfuzz-2.10.0 sentence-transformers-2.2.2 seqeval-1.2.2 tika-1.24\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# instantiate document store\nfrom haystack.document_stores.elasticsearch import ElasticsearchDocumentStore\n\n# return document embedding for later use with dense retriever\n# creates two indices by default; document for storing docs and\n# label; for storing annotated answer spans\ndocument_store = ElasticsearchDocumentStore(return_embedding=True)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:09:53.425344Z","iopub.execute_input":"2022-09-20T14:09:53.427817Z","iopub.status.idle":"2022-09-20T14:09:57.503167Z","shell.execute_reply.started":"2022-09-20T14:09:53.427771Z","shell.execute_reply":"2022-09-20T14:09:57.501916Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Haystack's document stores expect a list of dictionaries with text and meta keys as follows:\n\n```\n{\n \"text\": \"<the-context>\",\n \"meta\": {\n \"field_01\": \"<additional-metadata>\",\n \"field_02\": \"<additional-metadata>\",\n ...\n }\n}\n```\n\nThe fields in `meta` can be used to apply filters during retrieval. We'll include `item_id` and `q_review_id` columns of SubjQA so we can filter by product and question ID, also the training split.\n\nThen can loop through examples in each DataFrame and add them to the index with `write_documents()` method.","metadata":{"execution":{"iopub.status.busy":"2022-09-15T14:26:27.466459Z","iopub.status.idle":"2022-09-15T14:26:27.466941Z","shell.execute_reply.started":"2022-09-15T14:26:27.466724Z","shell.execute_reply":"2022-09-15T14:26:27.466748Z"}}},{"cell_type":"code","source":"for split, df in dfs.items():\n    # Exclude duplicate reviews\n    docs = [\n        {\"content\": row[\"context\"],\n        \"meta\":{\"item_id\": row[\"title\"], \"question_id\": row[\"id\"],\n        \"split\": split}} for _,row in df.drop_duplicates(subset=\"context\").iterrows()]\n    document_store.write_documents(docs, index=\"document\")\n\nprint(f\"Loaded {document_store.get_document_count()} documents\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:09:57.505083Z","iopub.execute_input":"2022-09-20T14:09:57.505442Z","iopub.status.idle":"2022-09-20T14:10:04.239822Z","shell.execute_reply.started":"2022-09-20T14:09:57.505406Z","shell.execute_reply":"2022-09-20T14:10:04.238813Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Loaded 1615 documents\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Have loaded all reviews into an index! Will need a retriever to search the index; so can look at initialising one for Elasticsearch.\n\nWill start with BM25: \"Best Match 25\"; an improved version of TF-IDF, and represents the question and context as sparse vectors that can be searched efficiently on Elasticsearch. \n\nBM25 measures how much matched text is about a search query and improves TF-IDF by saturating TF values quickly and normalising the document length so short documents are favoured over long ones.","metadata":{}},{"cell_type":"code","source":"# BM25 is used by default in ElasticsearchRetriever; initialise class with doc store\nfrom haystack.retriever.sparse import ElasticsearchRetriever\n\nes_retriever = ElasticsearchRetriever(document_store=document_store)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:04.243954Z","iopub.execute_input":"2022-09-20T14:10:04.244446Z","iopub.status.idle":"2022-09-20T14:10:04.258822Z","shell.execute_reply.started":"2022-09-20T14:10:04.244405Z","shell.execute_reply":"2022-09-20T14:10:04.257485Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"It's important to restrict the queries in our review-based QA to a single item, otherwise we would get item reviews that we don't care about.\n\nThings like ASIN help us identify items uniquely.","metadata":{}},{"cell_type":"code","source":"item_id = \"B0074BW614\"\nquery = \"Is it good for reading?\"\n# specified how many docs with top_k, applied a filter on item id and split keys\nretrieved_docs = es_retriever.retrieve(\n    query=query, top_k=3, filters={\"item_id\":[item_id], \"split\":[\"train\"]}\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:04.263515Z","iopub.execute_input":"2022-09-20T14:10:04.266080Z","iopub.status.idle":"2022-09-20T14:10:04.356098Z","shell.execute_reply.started":"2022-09-20T14:10:04.266036Z","shell.execute_reply":"2022-09-20T14:10:04.354961Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"from pprint import pprint\nprint(len(retrieved_docs))\npprint(retrieved_docs[0])","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:04.361480Z","iopub.execute_input":"2022-09-20T14:10:04.363110Z","iopub.status.idle":"2022-09-20T14:10:04.373647Z","shell.execute_reply.started":"2022-09-20T14:10:04.363066Z","shell.execute_reply":"2022-09-20T14:10:04.372423Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"3\n<Document: {'content': 'This is a gift to myself.  I have been a kindle user for 4 years and this is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my laptop, my phone and my iPod classic.  I love my iPod but watching movies on the plane with it can be challenging because it is so small. Laptops battery life is not as good as the Kindle.  So the Fire combines for me what I needed all three to do. So far so good.', 'content_type': 'text', 'score': 0.6857824513476455, 'meta': {'item_id': 'B0074BW614', 'question_id': '868e311275e26dbafe5af70774a300f3', 'split': 'train'}, 'embedding': None, 'id': '252e83e25d52df7311d597dc89eef9f6'}>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Can also see a `score` that Elasticsearch computed for its relevance to the query (high score = better match). Under the hood, Elasticsearch relies on Lucene for indexing and search, so uses Lucene's practical scoring function by default. \n\nFirst filters by boolean (does doc match the query?), then applies a similarity metric based on representing both the document and query as vectors.\n\nNow we can retrieve relevant documents, we need to figure out a way to extract answers from them. This is where the reader comes in, so look at how we can load our MiniLM model in Haystack.\n\nTwo types of readers one can use to extract answers from a context:\n- **FARMReader**: Based on deepset's FARM framework for fine-tuning and deploying transformers. Compatible with models trained using HF Transformers and can load models directly from HF Hub\n- **TransformersReader**: Based on QA pipeline from HF Transformers. Suitable for running inference only.\n\nThough both readers handle a model's weights in the same way, there are some differences in the way the predictions are converted to produce answers.\n\n- In HF Transformers, the QA pipeline normalises the start and end logits with a softmax in each passage. So it is only meaningful to compare answer scores between answers extracted from the same passage, where probabilities sum to 1. So 0.9 in one passage is not necessarily better than 0.8 in another passage. In FARM, the logits are normalised so inter-passage answers can be compared more easily.\n- TransormersReader sometimes predicts the same answers twice, but with different scores. This can happen in long contexts if the answer lies across two overlapping windows. In FARM, these duplicates are removed.","metadata":{}},{"cell_type":"code","source":"# use FARMReader as we'll be fine-tuning the reader later\nfrom haystack.reader.farm import FARMReader\n\n# specify MiniLM checkpoint; similar to HF Transformers\nmodel_ckpt = \"deepset/minilm-uncased-squad2\"\nmax_seq_length, doc_stride = 384, 128\nreader = FARMReader(\n    model_name_or_path=model_ckpt, progress_bar=False,\n    max_seq_len=max_seq_length, doc_stride=doc_stride,\n    return_no_answer=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:04.376329Z","iopub.execute_input":"2022-09-20T14:10:04.378546Z","iopub.status.idle":"2022-09-20T14:10:12.320100Z","shell.execute_reply.started":"2022-09-20T14:10:04.377862Z","shell.execute_reply":"2022-09-20T14:10:12.318624Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Note: It is also possible to fine-tune a model directly in HF Transformers then load in TransformersReader to run inference.","metadata":{}},{"cell_type":"code","source":"# behaviour of sliding window is controlled by max_seq_length and \n# doc_stride. Test reader on example from earlier\nprint(reader.predict_on_texts(question=question, texts=[context], top_k=1))","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:12.323088Z","iopub.execute_input":"2022-09-20T14:10:12.323633Z","iopub.status.idle":"2022-09-20T14:10:13.553590Z","shell.execute_reply.started":"2022-09-20T14:10:12.323582Z","shell.execute_reply":"2022-09-20T14:10:13.552374Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"{'query': 'How much music can this hold?', 'no_ans_gap': 12.829152584075928, 'answers': [<Answer {'answer': '6000 hours', 'type': 'extractive', 'score': 0.5138804167509079, 'context': 'An MP3 is about 1MB/minute, so about 6000 hours depending on file size.', 'offsets_in_document': [{'start': 37, 'end': 47}], 'offsets_in_context': [{'start': 37, 'end': 47}], 'document_id': '4dc55d90fb64f6d8b34472666e73a16d', 'meta': {}}>]}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Tie together with Haystack pipeline; allows us to combine retrievers, readers and more components as a graph that can be customised for each use-case.","metadata":{}},{"cell_type":"code","source":"from haystack.pipeline import ExtractiveQAPipeline\n\n# want to extract answers; so use ExtractiveQAPipeline, which takes a single\n# retriever-reader pair\npipe = ExtractiveQAPipeline(reader, es_retriever)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.555850Z","iopub.execute_input":"2022-09-20T14:10:13.556560Z","iopub.status.idle":"2022-09-20T14:10:13.566225Z","shell.execute_reply.started":"2022-09-20T14:10:13.556515Z","shell.execute_reply":"2022-09-20T14:10:13.565142Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"n_answers = 3\n# run() specifies how query flow should be executed\n# also specify filter over item. \npreds = pipe.run(query=query, \n                 params={\n                     \"Retriever\": {\"top_k\": 3}, \n                     \"Reader\": {\"top_k\": n_answers},\n                     \"filters\": {\"item_id\": [item_id], \"split\": [\"train\"]}\n                 })\n# Retrieve and return the extracted answers\nprint(f\"Question: {preds['query']} \\n\")\nfor idx in range(n_answers):\n    print(f\"Answer {idx+1}: {preds['answers'][idx].answer}\")\n    print(f\"Review snippet: ...{preds['answers'][idx].context}... \")\n    print(\"\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.568102Z","iopub.execute_input":"2022-09-20T14:10:13.568534Z","iopub.status.idle":"2022-09-20T14:10:13.743028Z","shell.execute_reply.started":"2022-09-20T14:10:13.568496Z","shell.execute_reply":"2022-09-20T14:10:13.741923Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Question: Is it good for reading? \n\nAnswer 1: it is great for reading books when no light is available\nReview snippet: ...ecoming addicted to hers! Our son LOVES it and it is great for reading books when no light is available. Amazing sound but I suggest good headphones t... \n\n\n\nAnswer 2: I mainly use it for book reading\nReview snippet: ... is my third one.  I never thought I would want a fire for I mainly use it for book reading.  I decided to try the fire for when I travel I take my la... \n\n\n\nAnswer 3: \nReview snippet: ...None... \n\n\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"So we have an E2E QA system for Amazon product reviews! Retriever, reader! Good start, but we could do better. Need to get some metrics to quantify the performance of the retriever and reader.\n\n### Improving Our QA Pipeline\n\nIn practice, it doesn't matter how good reader is if retriever can't find relevant documents! Retriever sets an upper bound on performance of whole QA system, so important to ensure it's doing a good job. Look at some common metrics to evaluate the retriever so can compare the performance of sparse and dense representations:\n\n- **Recall**: Fraction of all relevant documents retrieved. \"Relevant\" means whether answer is present in a text passage or not, so number of times an answer appears in top-k docs returned by retriever\n- **Mean Average Precision (mAP)**: Rewards retrievers that can place the correct answers higher up in document ranking\n\nIn Haystack, we can use two methods:\n- Built in `eval()` method. Can be used for both open and closed domain QA, but not for datasets like SubjQA where each document is paired with a single product and we need to filter by Product ID for every query\n- Build a custom pipeline that combines `EvalRetriever` class. This enables implementation of custom metrics and query flows","metadata":{}},{"cell_type":"code","source":"# opt for secnd approach; each node in Pipeline graph represents a class\n# that takes some inputs and produces some outputs via a run() method\nclass PipelineNode:\n    def __init__(self):\n        self.outgoing_edges = 1\n        \n    # kwargs is outputs from previous node in graph\n    # manipulated with run() to return tuple of outputs for next node\n    # along with name for outgoing edge attribute that indicates num of outputs\n    # from the node\n    def run(self, **kwargs):\n        ...\n        return (outputs, \"outgoing_edge_name\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.744539Z","iopub.execute_input":"2022-09-20T14:10:13.745162Z","iopub.status.idle":"2022-09-20T14:10:13.751797Z","shell.execute_reply.started":"2022-09-20T14:10:13.745123Z","shell.execute_reply":"2022-09-20T14:10:13.750126Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"Need to evaluate the retriever, so will use `EvalRetriever` class whose run() method keeps track of which documents have answers that match the ground truth.","metadata":{}},{"cell_type":"code","source":"from haystack.pipeline import Pipeline\nfrom haystack.nodes.evaluator import EvalDocuments\n\nclass EvalRetrieverPipeline:\n    def __init__(self, retriever):\n        self.retriever = retriever\n        self.eval_retriever = EvalDocuments()\n        pipe = Pipeline()\n        # each node has a name and a list of inputs\n        # in most cases each node has a single outgoing edge so just need to\n        # include name of previous node in input\n        pipe.add_node(component=self.retriever, name=\"ESRetriever\", inputs=[\"Query\"])\n        pipe.add_node(component=self.eval_retriever, name=\"EvalRetriever\", inputs=[\"ESRetriever\"])\n        self.pipeline = pipe\n        \npipe = EvalRetrieverPipeline(es_retriever)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.753980Z","iopub.execute_input":"2022-09-20T14:10:13.754765Z","iopub.status.idle":"2022-09-20T14:10:13.764933Z","shell.execute_reply.started":"2022-09-20T14:10:13.754728Z","shell.execute_reply":"2022-09-20T14:10:13.764016Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"Pass in some queries and their corresponding answers. We'll add the answers to a dedicated label index on our document store. Haystack provides a Label object that represents the answer spans and their metadata in a standardized fashion. We'll first create a list of Label objects by looping over each question in the test set and extract the matching answers and additional metadata.","metadata":{}},{"cell_type":"code","source":"from haystack import Label, Answer, Document\n\nlabels = []\nfor i, row in dfs[\"test\"].iterrows():\n    # Metadata used for filtering in the Retriever\n    meta = {\"item_id\": row[\"title\"], \"question_id\": row[\"id\"]}\n    # Populate labels for questions with answers\n    if len(row[\"answers.text\"]):\n        for answer in row[\"answers.text\"]:\n            label = Label(\n                query=row[\"question\"], document=Document(row['context']), \n                answer=Answer(answer), id=i, origin=row[\"id\"],\n                meta=meta, is_correct_answer=True, is_correct_document=True,\n                no_answer=False\n            )\n            labels.append(label)\n    # Pupulate labels for questions without answers\n    else:\n        label = Label(\n            query=row[\"question\"], document=Document(row['context']), \n            answer=Answer(\"\"), id=i, origin=row[\"id\"],\n            meta=meta, is_correct_answer=True, is_correct_document=True,\n            no_answer=True\n        )\n        labels.append(label)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.768205Z","iopub.execute_input":"2022-09-20T14:10:13.768515Z","iopub.status.idle":"2022-09-20T14:10:13.830554Z","shell.execute_reply.started":"2022-09-20T14:10:13.768489Z","shell.execute_reply":"2022-09-20T14:10:13.829605Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"labels[0]","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.832138Z","iopub.execute_input":"2022-09-20T14:10:13.832484Z","iopub.status.idle":"2022-09-20T14:10:13.841247Z","shell.execute_reply.started":"2022-09-20T14:10:13.832449Z","shell.execute_reply":"2022-09-20T14:10:13.840177Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<Label: {'id': '05eeb8b1-bfb9-429a-b7f1-8fb72d4d0cbd', 'query': 'What is the tonal balance of these headphones?', 'document': {'content': \"I have been a headphone fanatic for thirty years and have owned and used a variety of headphones over those years, to include Stax SR-5, Sennheiser HD-424 and HD-580.  The Sony MDRV6 excells as the best value of any headphone that I've ever owned.  They are especially good at producing natural-sounding deep bass, and the overall octave-to-octave balance is excellent.  The sound quality is all in all comparable to other headphones that cost considerably more.The MDRV6 is especially well-suited for travel due to the collapsible design, and for noisy environments or for quiet environments such as a library where the sound emitted by open-back headphones would distract others.The MDRV6 is not quite as comfortable as some other headphones, but the comfort can be improved enormously by replacing the pads with the velour pads from BeyerDynamic.  The pads that come on the MDRV6 have a non-breathable cover, and significant additional discomfort is caused by the way that the thin foam cover is glued to the pad around the inner circumference of the pad, which prevents the top and back of your ear from slipping into the space between the pad and the face of the headphone.  This forces the pad to rest on the back of the ear, which compresses the ear and eventually becomes uncomfortable.  I read on a web forum where several people had replaced the pads (which eventually come apart) with a velour pad made by BeyerDynamic, and after I did this replacement on my one pair that had a damaged pad, the increase in comfort was so great that I immediately did the other pair.  The thin cover can be glued down to the headphone face if needed to hold it in place, which then allows the top and rear of your ear to naturally  slide under the inner circumference of the pad, which is considerably more comfortable.  I can recommend this trivial modification without reservation to anyone who owns these headphones.  Even if you don't replace the pads with the BeyerDynamic velour pads, I suggest removing the pads anyway, then carefully separating the thin cover from the pad, and glueing the cover directly to the headphone surface.\", 'content_type': 'text', 'id': 'a73f54ddc5b1fcc7f4fbf04bd564a278', 'meta': {}, 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'd0781d13200014aa25860e44da9d5ea7', 'answer': {'answer': 'I have been a headphone fanatic for thirty years', 'type': 'extractive', 'score': None, 'context': None, 'offsets_in_document': None, 'offsets_in_context': None, 'document_id': None, 'meta': {}}, 'no_answer': False, 'pipeline_id': None, 'created_at': '2022-09-20 14:10:13', 'updated_at': None, 'meta': {'item_id': 'B00001WRSJ', 'question_id': 'd0781d13200014aa25860e44da9d5ea7'}, 'filters': None}>"},"metadata":{}}]},{"cell_type":"markdown","source":"Contains QA pair and unique ID to filter the document store per question. Also added the product_id to metadata so can filter labels by product.","metadata":{}},{"cell_type":"code","source":"# write to label index on Elasticsearch\ndocument_store.write_labels(labels, index=\"label\")\nprint(f\"\"\"Loaded {document_store.get_label_count(index=\"label\")} question-answer pairs\"\"\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:13.843214Z","iopub.execute_input":"2022-09-20T14:10:13.844083Z","iopub.status.idle":"2022-09-20T14:10:15.477582Z","shell.execute_reply.started":"2022-09-20T14:10:13.844046Z","shell.execute_reply":"2022-09-20T14:10:15.476691Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Loaded 358 question-answer pairs\n","output_type":"stream"}]},{"cell_type":"code","source":"# build mapping between question IDs and corresponding answers that we can pass to pipeline\n# aggregate all QA pairs associated with a unique ID. \n# Return list of MultiLabel objects; only get one element since we filter by question ID\n\nlabels_agg = document_store.get_all_labels_aggregated(\n index=\"label\",\n open_domain=True,\n aggregate_by_meta=[\"item_id\"]\n)\nprint(len(labels_agg))","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:34:53.527749Z","iopub.execute_input":"2022-09-20T14:34:53.528162Z","iopub.status.idle":"2022-09-20T14:34:53.638936Z","shell.execute_reply.started":"2022-09-20T14:34:53.528125Z","shell.execute_reply":"2022-09-20T14:34:53.637522Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":61,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/document_stores/elasticsearch.py\u001b[0m in \u001b[0;36mget_all_labels\u001b[0;34m(self, index, filters, headers, batch_size)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/document_stores/elasticsearch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mLabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_source\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/schema.py\u001b[0m in \u001b[0;36mfrom_dict\u001b[0;34m(cls, dict)\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pydantic_dataclass_from_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpydantic_dataclass_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/schema.py\u001b[0m in \u001b[0;36m_pydantic_dataclass_from_dict\u001b[0;34m(dict, pydantic_dataclass_type)\u001b[0m\n\u001b[1;32m    726\u001b[0m     \"\"\"\n\u001b[0;32m--> 727\u001b[0;31m     \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpydantic_dataclass_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_model__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m     \u001b[0mbase_mode_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__fields__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydantic/main.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.parse_obj\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pydantic/main.cpython-37m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mpydantic.main.BaseModel.__init__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mValidationError\u001b[0m: 1 validation error for Label\norigin\n  unexpected value; permitted: 'user-feedback', 'gold-label' (type=value_error.const; given=d0781d13200014aa25860e44da9d5ea7; permitted=('user-feedback', 'gold-label'))","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mDocumentStoreError\u001b[0m                        Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/1080115287.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m  \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m  \u001b[0mopen_domain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m  \u001b[0maggregate_by_meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_agg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/document_stores/base.py\u001b[0m in \u001b[0;36mget_all_labels_aggregated\u001b[0;34m(self, index, filters, open_domain, drop_negative_labels, drop_no_answers, aggregate_by_meta, headers)\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0maggregate_by_meta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_all_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;31m# drop no_answers in order to not create empty MultiLabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/haystack/document_stores/elasticsearch.py\u001b[0m in \u001b[0;36mget_all_labels\u001b[0;34m(self, index, filters, headers, batch_size)\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValidationError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             raise DocumentStoreError(\n\u001b[0;32m--> 694\u001b[0;31m                 \u001b[0;34mf\"Failed to create labels from the content of index '{index}'. Are you sure this index contains labels?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m             )\n\u001b[1;32m    696\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mDocumentStoreError\u001b[0m: Failed to create labels from the content of index 'label'. Are you sure this index contains labels?"],"ename":"DocumentStoreError","evalue":"Failed to create labels from the content of index 'label'. Are you sure this index contains labels?","output_type":"error"}]},{"cell_type":"code","source":"# feeder function that feeds each qa pair associated with product to evaluation pipeline\n# and tracks the correct retrievals in our pipe object\ndef run_pipeline(pipeline, top_k_retriever=10, top_k_reader=4):\n    for l in labels_agg:\n        _ = pipeline.pipeline.run(\n            query=l.question,\n            top_k_retriever=top_k_retriever,\n            top_k_reader=top_k_reader,\n            top_k_eval_documents=top_k_retriever,\n            labels=l,\n            filters={\"items_id\": [l.meta[\"item_id\"]], \"split\": [\"test\"]}\n        )\nrun_pipeline(pipe, top_k_retriever=3)\nprint(f\"Recall@3: {pipe.eval_retriever.recall:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:41:22.770261Z","iopub.execute_input":"2022-09-20T14:41:22.770631Z","iopub.status.idle":"2022-09-20T14:41:22.782328Z","shell.execute_reply.started":"2022-09-20T14:41:22.770599Z","shell.execute_reply":"2022-09-20T14:41:22.781361Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"markdown","source":"Notice that we've picked a specific value for top_k_reriever to specify number of documents to retrieve. In general, increasing this parameter will improve the recall, but at the expense of providing more documents to the reader and slowing down end-to-end pipeline","metadata":{}},{"cell_type":"code","source":"# create function that loops over several k and computes recall across whole test set of each k\ndef evaluate_retriever(retriever, topk_values=[1,3,5,10,20]):\n    topk_results = {}\n    \n    for topk in topk_values:\n        # create pipeline\n        p = EvalRetrieverPipeline(retriever)\n        # loop over each QA pair in test set\n        run_pipeline(p, top_k_retriever=topk)\n        # get metrics\n        topk_results[topk] = {\"recall\": p.eval_retriever.recall}\n    \n    return pd.DataFrame.from_dict(topk_results, orient=\"index\")\n\nes_topk_df = evaluate_retriever(es_retriever)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:41:02.078562Z","iopub.execute_input":"2022-09-20T14:41:02.079294Z","iopub.status.idle":"2022-09-20T14:41:02.085547Z","shell.execute_reply.started":"2022-09-20T14:41:02.079255Z","shell.execute_reply":"2022-09-20T14:41:02.084449Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"# plot results to see how recall improves as we increase k\n\ndef plot_retriever_eval(dfs, retriever_names):\n    fig, ax = plt.subplots()\n    for df, retriever_name in zip(dfs, retriever_names):\n        df.plot(y=\"recall\", ax=ax, label=retriever_name)\n    plt.xticks(df.index)\n    plt.ylabel(\"Top-k Recall\")\n    plt.xlabel(\"k\")\n    plt.show()\n    \nplot_retriever_eval([es_topk_df], [\"BM25\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:10:16.330925Z","iopub.status.idle":"2022-09-20T14:10:16.331738Z","shell.execute_reply.started":"2022-09-20T14:10:16.331437Z","shell.execute_reply":"2022-09-20T14:10:16.331461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Inflection point around k=5 and almost perfect recall from k=10 onwards.. Now, look at retrieving documents with dense vector trechniques.\n\n**Dense Passage Retrieval**\n\nSmaller k? Then less documents to reader and reduce latency of QA pipeline. Limitation of sparse retrievers like BM25 is that they can fail to capture relevant documents if user query has terms that don't exactly match those of review. \n\nAn alternative is dense embeddings to represent question and document. Current SOTA is *Dense Passage Retrieval (DPR)*. \n\nMain idea is to use two BERT models as encoders for question and passage, which map input text into d-dimensional vector representation of `[CLS]` token; then take dot product similarity of question vector with passage vector, then go into document store.\n\nIn Haystack, we can initialise a retriever for DPR in similar way we did for BM25. We also need to pick BERT encoders for question and passage. These encoders are trained by giving them questions with relevant (positive) passages and irrelevant (negative) passages, where the goal is to learn that relevant question-passage pairs have a higher similarity. For this, we'll use encoders that have been fine-tuned on NQ corpus in this way:","metadata":{}},{"cell_type":"code","source":"from haystack.retriever.dense import DensePassageRetriever\n\ndpr_retriever = DensePassageRetriever(\n    document_store=document_store,\n    query_embedding_model=\"facebook/dpr-question_encoder-single-nq-base\",\n    passage_embedding_model=\"facebook/dpr-ctx_encoder-single-nq-base\",\n    embed_title=False # false as concatenating doc title doesn't provide additional info\n    # as we filter per product\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:39:01.042255Z","iopub.execute_input":"2022-09-20T14:39:01.042648Z","iopub.status.idle":"2022-09-20T14:39:27.767086Z","shell.execute_reply.started":"2022-09-20T14:39:01.042613Z","shell.execute_reply":"2022-09-20T14:39:27.766081Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5e739d5a2a140f1a9a7f95f3e0b98d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"852ab9d300f840da91e3d6ef17c42ecc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8ec76d9916043d7958d01077dccf623"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/493 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7df7c5fdbcd24656acdb882658ff9545"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f310157fc4e43b68270e54d523b4d29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3a6580c63d456b8a324cc922994cf4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b99eee8e3a6a46348fc9a8996f181d50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de5dda5f9b6d4a7a93757a038cd5728f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/492 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"102b72808db44299946785743ee3fab0"}},"metadata":{}},{"name":"stderr","text":"The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \nThe tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \nThe class this function is called from is 'DPRContextEncoderTokenizerFast'.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/418M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f430ac7be4504bd2b7fbff117ea42f68"}},"metadata":{}}]},{"cell_type":"code","source":"# iterate over indexed documents in elasticsearch index\n# and apply encoders to update embedding representation\ndocument_store.update_embeddings(retriever=dpr_retriever)","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:40:08.166567Z","iopub.execute_input":"2022-09-20T14:40:08.167657Z","iopub.status.idle":"2022-09-20T14:40:33.751500Z","shell.execute_reply.started":"2022-09-20T14:40:08.167616Z","shell.execute_reply":"2022-09-20T14:40:33.750516Z"},"trusted":true},"execution_count":63,"outputs":[{"output_type":"display_data","data":{"text/plain":"Updating embeddings:   0%|          | 0/1615 [00:00<?, ? Docs/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f6a08094694393beaf4076cea52689"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Create embeddings:   0%|          | 0/1616 [00:00<?, ? Docs/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}]},{"cell_type":"code","source":"dpr_topk_df = evaluate_retriever(dpr_retriever)\nplot_retriever_eval([es_topk_df, dpr_topk_df, [\"BM25\", \"DPR\"]])","metadata":{"execution":{"iopub.status.busy":"2022-09-20T14:41:30.853600Z","iopub.execute_input":"2022-09-20T14:41:30.854318Z","iopub.status.idle":"2022-09-20T14:41:30.921907Z","shell.execute_reply.started":"2022-09-20T14:41:30.854279Z","shell.execute_reply":"2022-09-20T14:41:30.920536Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":68,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/3328203944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdpr_topk_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_retriever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdpr_retriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plot_retriever_eval([dpr_topk_df, [\"BM25\"]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/389939292.py\u001b[0m in \u001b[0;36mevaluate_retriever\u001b[0;34m(retriever, topk_values)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvalRetrieverPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# loop over each QA pair in test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_retriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;31m# get metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtopk_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"recall\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_retriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/4017734955.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(pipeline, top_k_retriever, top_k_reader)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# and tracks the correct retrievals in our pipe object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_retriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k_reader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels_agg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         _ = pipeline.pipeline.run(\n\u001b[1;32m      6\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'labels_agg' is not defined"],"ename":"NameError","evalue":"name 'labels_agg' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"DPR does not provide a boost in recall and saturates around k=3. \n\nSimilarity search can be sped up using FAISS as document store; and DPR performance can be improved by fine-tuning on target domain. Haystack has multiple tutorials for this.\n\nSo now we can look at evaluating the reader.\n\n### Evaluating the Reader\n\nTwo main metrics to evaluate readers:\n- *Exact Match (EM)*: A binary metric that gives EM = 1 if the characters are in the predicted and ground truth answers match exactly, and EM = 0 otherwise. If no answer expected, the model gets EM = 0 if it predicts any text at all.\n- *$F_1-score$*: Measures the harmonic mean of the precision and recall.","metadata":{}},{"cell_type":"code","source":"from farm.evaluation.squad_evaluation import compute_f1, compute_exact\n\npred = \"about 6000 hours\"\nlabel = \"6000 hours\"\nprint(f\"EM: {compute_exact(label, pred)}\")\nprint(f\"F1: {compute_f1(label, pred)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Under the hood, these functions first normalise the prediction and label by removing punctuation, fixing whitespace, and converting to lowercase. The normalised strings are tokenised as a bag-of-words before computing the metric at the token level.\n\nFrom this example we can see EM is a sricter metric than F1 score, adding a single token to prediction gives EM a zero; however, F1 can fail to catch truly incorrect answers. e.g.","metadata":{}},{"cell_type":"code","source":"pred = \"about 6000 dollars\"\nlabel = \"6000 hours\"\nprint(f\"EM: {compute_exact(label, pred)}\")\nprint(f\"F1: {compute_f1(label, pred)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So relying on F1 score is misleading, so tracking both metrics is a good strategy to balance trade-off between underestimating (EM) and overestimating (F1-score) model performance.\n\nGenerally, there are multiple valid answers per question, so metrics are calculated for each possibleQA pair in evaluation set and best score is selected over all possible answers. Overall EM and F1 scores are obtained by averaging over individual scores of each QA pair.\n\nTo evaluate reader, we will create a pipeline with two nodes: a reader node to evaluate the reader, will use EvalReader class that takes predictions from reader and computes corresponding EM and F1 scores. Tocompare with SQuAD evaluation, we'll take the best answers for each query with top_1_em and top_1_f1 metrics stored in EvalAnswers:","metadata":{}},{"cell_type":"code","source":"from haystack.eval import EvalAnswers\n\ndef evaluate_reader(reader):\n    score_keys = ['top_1_em', 'top_1_f1']\n    # false so retriever always passes context to reader (as in SQuAD evaluation)\n    eval_reader = EvalAnswers(skip_incorrect_retrieval=False)\n    pipe = Pipeline()\n    pipe.add_node(component=reader, name=\"QAReader\", inputs=[\"Query\"])\n    pipe.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\n    \n    for l in labels_agg:\n        doc = document_store.query(l.question,\n                                  filters={\"question_id\":[l.origin]})\n        _ = pipe.run(query=l.question, documents=doc, labels=l)\n        \n    return {k:v for k,v in eval_reader.__dict__.items() if k in score_keys}\n\nreader_eval = {}\nreader_eval[\"Fine-tune on SQuAD\"] = evaluate_reader(reader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print scores after running through every question\ndef plot_reader_eval(reader_eval):\n    fig, ax = plt.subplots()\n    df = pd.DataFrame.from_dict(reader_eval)\n    df.plot(kind=\"bar\", ylabel=\"Score\", rot=0, ax=ax)\n    ax.set_xticklabels([\"EM\", \"F1\"])\n    plt.legend(loc=\"upper left\")\n    plt.show() \n    \nplot_reader_eval(reader_eval)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Fine-tuned model performs significantly worse on SubjQA than on SQuAD 2.0, where MiniLM achievesEM and F1 of 76.1 and 79.5 respectively. \n\nOne reason for performance drop is that customer reviews are different from Wikipedia articles the SQuAD 2.0 dataset is generated from, and the language they use is often informal.\n\nAnother reason is likely the inherent subjectivity of our dataset, where both questions and answers differ from the factual information contained in Wikipedia. \n\nLook at how to fine-tune a model on a dataset to get better results with domain adaptation.\n\n### Domain Adaptation\n\nAlthough models fine-tuned on SQuAD generise well to other domains, we see for SubjQA the EM and F1 scores of our model were much worse than for SQuAD. We have observed this in other extractive QA datasets and is understood as evidence that transformer models are adept at overfitting to SQuAD.\n\nThe most straightforward way is to fine-tune MiniLM model further on SubjQA training set. The FARMReader has a train() method that is designed for this purpose and expecs the data to be in SQuAD JSON format, where all the QA pairs are grouped together for each item.\n\n> Data\n>> Paragraphs\n>>> Qas\n>>>> Answers\n\nQuite a complex data format, so we'll need a few functions and some Pandas to help do the conversion.","metadata":{}},{"cell_type":"code","source":"# implement a function that can create the paragraphs array associated \n# with each product ID. Each element in array contains a single context (review)\n# and a qas array of question-answer pairs.\ndef create_paragraphs(df):\n    paragraphs = []\n    id2context = dict(zip(df[\"review_id\"], df[\"context\"]))\n    for review_id, review in id2context.items():\n        qas = []\n        # Filter for all QA pairs about a specific content\n        review_df = df.query(f\"review_id == '{review_id}'\")\n        id2question = dict(zip(review[\"id\"], review_df[\"question\"]))\n        # build up qas array\n        for qid, question in id2question.items():\n            # filter for a single question id\n            question_df = df.query(f\"id == '{qid}'\").to_dict(orient=\"list\")\n            ans_start_idxs = question_df[\"answers.answer_start\"][0].tolist()\n            ans_text = question_df[\"answers.text\"][0].tolist()\n            # fill answerable questions\n            if len(ans_start_idxs):\n                answers = [\n                    {\"text\":text, \"answer_start\":answer_start} \n                    for text, answer_start in zip(ans_tex, ans_start_idxs)\n                ]\n                is impossible=False\n            else:\n                answers = []\n                is_impossible = True\n            # add QA pairs to qas\n            qas.append({\"question\": question, \"id\": qid, \n                        \"is_impossible\": is_impossible, \"answers\": answers})\n        # add context and QA pairs to paragraphs\n        paragraphs.append(\"qas\": qas, \"context\": review)\n    return paragraphs","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"product = dfs[\"train\"].query(\"title == ;B0001P4ZH\")\ncreate_paragraphs(product)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply function to each product ID in dataframe of each split\n# following func does this in electronics-{split}.json file\n\nimport json\n\ndef convert_to_squad(dfs):\n    for split, df in dfs.items():\n        subjqa_daa = {}\n        # create `paragraphs` or each product id\n        groups = (df.groupby(\"title\").apply(create_paragraphs).to_frame(name=\"paragraphs\").reset_index())\n        subjqa_data[\"data\"] = groups.to_dict(orient=\"records\")\n        # save result to disk\n        with open(f\"electronics-{split}.json\", \"w+\", encoding=\"utf-8\") as f:\n            json.dump(subjqa_data, f)\n    \nconvert_to_squad(dfs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splits in right format, we can fine-tune\n# first; specify location of train and dev splits and where to save fine-tuned model\n\ntrain_filename = \"electronics-train.json\"\ndev_filename = \"electronics-validation.json\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n            train_filename=train_filename, dev_filename=dev_filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare performance on test against baseline\nreader_eval[\"Fine-tune on SQuAD + SubjQA\"] = evaluate_reader(reader)\nplot_reader_eval(reader_eval)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Domain adaptation works! EM score increase 6x and F1 score doubles. Why not directly fine-tune on SubjQA train set? Because SubjQA has 1,295 examples while SQuAD has >100,000 so we may run into some overfitting. \n\nNonetheless, we can look at the results of native fine-tuning, using the same language model used for fine-tuning baseline on SQuAD.","metadata":{}},{"cell_type":"code","source":"# load model with FARMReader\nminilm_ckpt = \"microsoft/MiniLM-L12-H384-uncased\"\nminilm_reader = FARMReader(model_name_or_path=minilm_ckpt, progress_bar=False,\n                          max_seq_len=max_seq_length, doc_stride=doc_stride,\n                          return_no_answer=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# fine-tune for one epoch\nminilm_reader.train(data_dir=\".\", use_gpu=True, n_epochs=1, batch_size=16,\n                   train_filename=train_filename)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# include evaluation on test set\nreader_eval[\"Fine-tune on SubjQA\"] = evaluate_reader(minilm_reader)\nplot_reader_eval(reader_eval)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So fine-tuning on SubjQA results is much worse than fine-tuning on SQuAD and SubjQA.\n\nBest practice to use cross-validation when evaluating transformers as they can be prone to overfitting. There are examples of cross-validation with SQuAD-formatted datasets in FARM repository.\n\n## Evaluating Whole QA Pipeline\n\nNow we know how to evaluate the reader and retriever components individually, we can tie them together to measure the overall performance of our pipeline. We'll need to augment our retriever pipeline with nodes for the reader and its evaluation. \n\nWe've seen almost perfect recall at k=10, so we can fix this value and assess the impact it has on the reader's performance (as it will now receive multiple contexts per query compared to SQuAD-style evaluation):","metadata":{}},{"cell_type":"code","source":"# initialise retriever pipeline\npipe = EvalRetrieverPipeline(es_retriever)\n\n# add nodes for reader\neval_reader = EvalAnswers()\npipe.pipeline.add_node(component=reader, name=\"QAReader\", inputs=[\"EvalRetriever\"])\npipe.pipeline.add_node(component=eval_reader, name=\"EvalReader\", inputs=[\"QAReader\"])\n\n# Evaluate!\nrun_pipeline(pipe)\n\n# Extract metrics from reader\nreader_eval[\"QA Pipeline (top-1)\"] = {\n    k:v for k, v in eval_reader.__dict__.items() if k in [\"top_1_em\", \"top_1_fm\"]\n}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we can see how the retriever has an effect on overall performance. Overall there's a degradation compared to matching the question-context pairs as is done in SQuAD style evaluation. This can be circumvented by increasing the number of possible answers the reader is allowed to predict.\n\nUntil now, we have only extracted answer spans from the context, but in general it could be bits and pieces of the answer are scattered throughout the document. We'd like our model to synthesize these fragments into a single answer. Look at generative QA to succeed at this task.\n\n### Beyond Extractive QA\n\nAn alternative to extracting answers is to generate them with a pretrained language model. This is referred to as *abstractive* or *generative* QA; and has the potential to produce better-phrased answers that synthesize evidence across multiple passages. \n\nThis is less mature than extractive QA but an exciting field of research. Current SOTA is *retrieval-augmented generation* (RAG); extending the classic retriever-reader architecture by swapping the reader for a *generator* and using DPR as the retriever. The generator is a pretrained seq2seq transformer like T5 or BART that receives latent vectors of documents from DPR and iteratively generates an answer based on the query and documents. Since DPR and generator are differentiable, can be trained E2E.\n\nThere are two types of RAG models to choose from:\n- *RAG-Sequence*: Same retrieved document to generate a complete answer. In particular, the `top-k` documents from the retriever are fed to the generator, which produces an output sequence for each document and the result is marginalised to obtain the best answer\n- *RAG-Token*: Can use a different document to generate each token in the answer, allowing the generator to synthesize evidence from multiple documents.\n\nRAG-TOken models tend to perform better than RAG-Sequence, we can use token model fine-tuned on NQ as our generator.","metadata":{}},{"cell_type":"code","source":"from haystack.generator.transforms import RAGenerator\n\n# specify hyperparameters that control the text generation\ngenerator = RAGenerator(\n    model_name_or_path=\"facebook/rag-token-nq\",\n    embed_title=False, # don't embed titles since corpus is filtered per product id\n    num_beams=5) # beam search num","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from haystack.pipeline import GenerativeQAPipeline\n\n# tie retriever and generator with GenerativeQAPipeline\npipe = GenerativeQAPipeline(generator=generator, retriever=dpr-retriever)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In RAG, query encoder and generator are trained E2E while context encoder is frozen. In Haystack, the GenerativeQAPipeline uses the query encoder from RAGenerator and context encoder from DensePassageRetriever.","metadata":{}},{"cell_type":"code","source":"# try some queries on Amazon Fire tablet\ndef generate_answers(query, top_k_generator=3):\n    preds = pipe.run(\n        query=query, \n        top_k_generator=top_k_generator,\n        top_k_retriever=5, \n        filters={\"item_id\":[\"B0074BW614\"]}\n    )\n    print(f\"Question: {preds['query']} \\n\")\n    for idx in range(top_k_generator):\n        print(f\"Answer {idx+1}: {preds['answers'][idx]['answer']}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate(answers(query))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Does suggest the subjective nature of the question confuses the generator","metadata":{}},{"cell_type":"code","source":"generate_answers(\"What is the main drawback?\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see a more factual question yields more sensible results. To get better results we could fine-tune RAG E2E on SubjQA.\n\n## Conclusion\n\nTwo approaches to QA: Extractive and generative, and examine two retrieval algorithms: BM25 and DPR. Also domain adaptation can be a simple technique to boost QA system performance by a significant margin, as well as common metrics for evaluating such systems. We focused primarily on closed-domain QA, the techniques can be generalised to open-domain case. \n\nRecommend exploring generative QA once other approaches (search and extraction) have been exhausted, given its infancy. Also really think of what users need.\n\nOne exciting area is multimodal QA, QA over multiple modalities like text, tables and images. E.g. \"What was the famous painting with two touching fingers completed?\". Another area with practical business application is QA over knowledge graph, where nodes are real-world entities and edges are relationships, so one can use graph to answer questions about a missing element.\n\nIn order to successfully QA for real-world use cases, we need to apply tricks such as implementing a fast retrieval pipeline to make near real-time predictions. A few seconds of which can make a large difference in the customer experience.\n\nNext chapter, we'll look at how to accelerate model predictions further!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}