{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Text Summarisation","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"cnn_dailymail\", version=\"3.0.0\")\nprint(f\"Features: {dataset['train'].column_names}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = dataset[\"train\"][1]\nprint(f\"Article (excerpt of 500 characters, total length: {len(sample['article'])})\")\nprint(sample['article'][:500])\nprint(f\"\\nSummary (length: {len(sample['highlights'])})\")\nprint(sample['highlights'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Difficulty is the large number of tokens in long form; simple but crude way is to truncate texts beyond model's capacity context size. May be important information at end of text but for now live with this limitation of model architectures.\n\n## Text Summarisation Pipelines\n\nLook at some qualitative examples from different model architectures.","metadata":{}},{"cell_type":"code","source":"sample_text = dataset[\"train\"][1][\"article\"][:2000]\n# collect generated summaries of each model in a dictionary\nsummaries = {}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convention to separate sentences by newline; NLTK has a more sophisticated algo\n# that can differentiate end of sentence from punctuation that occurs in abbreviations\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download(\"punkt\")\n\nstring = \"The U.S. are a country. The U.N. is an organisation\"\nsent_tokenize(string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"If we run out of memory, we can replace models with smaller; like `gpt`, or `t5-small`.","metadata":{}},{"cell_type":"code","source":"# simple baseline is to take first three sentences of the article\ndef three_sentence_summary(text):\n    return \"\\n\".join(sent_tokenize(text)[:3])\n\nsummaries[\"baseline\"] = three_sentence_summary(sample_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# GPT2: can use to generate summaries by appending \"TL;DR\" at end of input text\n# create a text generation pipeline and load the large GPT-2 model\nfrom transformers import pipeline, set_seed\n\nset_seed(42)\npipe = pipeline(\"text-generation\", model=\"gpt2\")\ngpt2_query = sample_text + \"\\nTL;DR:\\n\"\npipe_out = pipe(gpt2_query, max_length=512, clean_up_tokenization_spaces=True)\n# slice off query input and keep result in python dict for later comparison\nsummaries[\"gpt2\"] = \"\\n\".join(\n    sent_tokenize(pipe_out[0][\"generated_text\"][len(gpt2_query):]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**T5**: Could create a universal transformer architecture by formulating all tasks as text-to-text tasks; reconstruct masked words, supervised data, summarisation etc.. And could perform summarisation withoutfine-tuning by using the same prompts during pretraining.\n\nThe input format to summarise is `\"summarize:<ARTICLE>\"` and for translate it looks like `\"translate English to German: <TEXT>\"` making T5 extremely versatile and can slve many tasks with a single model.","metadata":{}},{"cell_type":"code","source":"# can load T5 directly with pipeline() function; also taking care of formatting \n# inputs in text-to-text format so no need to prepend with \"summarize\"\npipe = pipeline(\"summarization\", model=\"t5-large\")\npipe_out = pipe(sample_text)\nsummaries[\"t5\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# BART: encoder-decoder trained to reconstruct corrupted inputs\n# can use facebook/bart-large-cnn checkpoint; specifically fine-tuned on CNN/DailyMail dataset\npipe = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\npipe_out = pipe(sample_text)\nsummaries[\"bart\"] = \"\\n\".join(sent_tokenize(pipe_out[0][\"summary_text\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PEGASUS** is encoder-decoder transformer, like BART. Pretraining objective is to predict masked sentences in multisentence texts. The closer the pretraining objective is to downstream task, the more effective it is. \n\nWith the aim of finding a pretraining objective more closer to summarization than general language modeling, they automatically identified, in a large corpus, sentences containing most of the content of their surrounding paragraphs (using summarization evaluation metrics as a heuristic for content overlap) and pretrained the PEGASUS model to reconstruct these sentences. Thereby achieving a SOTA model for text-summarization.","metadata":{}},{"cell_type":"code","source":"pipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\npipe_out = pipe(sample_text)\nsummaries[\"pegasus\"] = pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Compare Structures\n\nBear in mind: One model has not been trained on the dataset at all (GPT-2); one has been fine-tuned on the task among others (T5), and two have been exclusively fine-tuned on this task (BART and PEGASUS). Look at summaries:","metadata":{}},{"cell_type":"code","source":"print(\"GROUND TRUTH\")\nprint(dataset[\"train\"][1][\"highlights\"])\nprint(\"\")\n\nfor model_name in summaries:\n    print(model_name.upper())\n    print(summaries[model_name])\n    print(\"\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Things to note:\n- GPT2 is different from the others; it summarises the characters. Often GPT2 \"hallucinates\" or invents facts as it was not explicitly trained to generate truthful summaries\n- We see overlap between all the models; with PEGASUS's output bearing the most striking resemblance\n\nWe need a more systemic metric to measure results; define a metric that we can use to measure for all models on some benchmark dataset. This is not so easy as for each \"gold standard\" summary written by humans, there could be countless variations that are equally acceptable. Next we look at some common metrics for measuring the quality of generated text.\n\n## Measuring the Quality of Generated Text\n\nIf we have bad metrics, may be blind to model degredation. If misaligned with business goals then we may not create any value. Some options:\n- **BLEU**: Look at words or n-grams for matches. We count the number of words that occur in the generated text and divide by the length of a reference text. \n    - There is a problem if the same word is generated over and over again, until the length of the reference then we would get perfect precision\n    - So there's a modification, word counted as many times as it occurs in the reference; so has a more reasonable value\n    - Can also take n-grams also; with the counter in the numerater clipped, so we cap how many times the occurrence count of an n-gram appears in a reference sentence. Note that definition of sentence is not strict; if we had generated multiple sentences then we would treat it as one sentence.\n    - We sum over all samples in corpus C\n    - Because precision favours shorter sentences compared to longer, there is also a brevity penalty; so exponentially penalises generated text being smaller than generated text\n    - Preferable to look at precision as there may be multiple correct answers in translation; so want high precision and a generated text of similar length to the reference text\n    - Issue: Doesn't take synonyms into account; and derivation steps seem ad-hoc and like fragile heuristics\n    - Expects text to already be tokenised; so can cause errors if different tokenisation method. SacreBLEU metric addresses this by internalising tokenisation step; so is preferred for benchmarking","metadata":{}},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nbleu_metric = load_metric(\"sacrebleu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"`bleu_metric` is an instance of `Metric` class; works like an aggregator. Can add single instances with `add()` or whole batches with `add_batch()`; then can call `compute()` and the metric is calculated","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nbleu_metric.add(\n    prediction=\"the the the the the the\", reference = [\"the cat is on the mat\"])\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- BLEU score also works with multiple reference translations, thus why it is passed as a list\n    - Also has methods to modify precision calculation, like can add a constant to the numerator so that missing n-gram does not cause the score to go down to zero. Set smooth_value=0 for the purpose of explaining the values.\n    \n1-gram precision 2/6 and 2/3/4 grams are 0. Geometric mean is 0 and thus also the BLEU score. bp is brevity penalty.","metadata":{}},{"cell_type":"code","source":"# prediction where almost correct\nbleu_metric.add(\n    prediction=\"the cat is on mat\", reference=[\"the cat is on the mat\"])\nresults = bleu_metric.compute(smooth_method=\"floor\", smooth_value=0)\nresults[\"precisions\"] = [np.round(p, 2) for p in results[\"precisions\"]]\npd.DataFrame.from_dict(results, orient=\"index\", columns=[\"Value\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Precision scores are much better. 1-gram almost all match; last does not so precision of 0.5 for 4-gram.\n\nBLEU widely used for evaluating text, especially in machine translation since precise words favoured over translations that include all the possible and appropriate words. \n\nApplications also includes summarisation applications where the situation is difference; so want all the important information in thus favour high recall.Here, we would favour ROUGE score.\n\n- **ROUGE**: Specificaly developed for applications where high recall is more important than just precision. Similar to BLEU in that it compares generated and reference texts. Difference is that we check how many n-grams in reference text also occur in the generated text; rather than the other way around\n    - Such was the original proposal for ROUGE, then research found fully removing precision can have strong negative effects. BLEU without clipped counting means can measure precision also, then combine precision and recall ROGUE scores in harmonic mean to get F1-score. This is the metric commonly reported for ROGUE.\n    - There is a separate score in ROUGE to measure longest common substring (LCS) called ROUGE-L. We normalise by the length of text, otherwise the longer text would be at an advantage.\n    - Two variations; one calculates score per sentence and averages it for summaries (ROUGE-L); and he other calculates it directly over whole summary (ROUGE-Lsum)","metadata":{}},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rogue_metric = load_metric(\"rouge\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"reference = dataset[\"train\"][1][\"highlights\"]\nrecords = []\nrogue_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\n\nfor model in summaries:\n    rogue_metric.add(prediction=summaries[model], reference=reference)\n    score = rogue_metric.compute()\n    rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rogue_names)\n    records.append(rouge_dict)\npd.DataFrame.from_records(records, index=summaries.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"By default ROGUE calculates confidence intervals of 5th and 95th percentiles by default. Average score stored in attribute mid and interval can be retrieved with low and high. Unreliable results as we only looked at a single sample. But can compare quality of the summary for the one example.\n- Confirms that GPT2 performs worst, not surprising as not trained to explicitly summarise\n- Amazing to see how simple first three sentence baseline doesn't perform too badly compared to bn parameter scale transformers\n- PEGASUS and BART are best models (overall ROUGE scores are higher) \n- T5 slightly better on ROUGE-1 and LCS scores. \n\nTreat results with caution as only one example. Looking at PEGASUS paper, we would expect PEGASUS to out-perform T5 on CNN/DailyMail dataset.\n\nTry to reproduce those results with PEGASUS.\n\n## Evaluating PEGASUS on CNN/DailyMail Dataset","metadata":{}},{"cell_type":"code","source":"def evaluate_summaries_baseline(\n    dataset, metric, column_text=\"article\", column_summary=\"highlights\"):\n    summaries = [three_sentence_summary(text) for text in dataset[column_text]]\n    metric.add_batch(predictions=summaries, references=dataset[column_summary])\n    score = metric.compute()\n    return score","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Apply to a subset of the data, because generating summaries for all articles would take a lot of time. Every generated token requires a forward pass through the model, so generating 100 tokens for each sample would require 1 million forward passes (10,000 articls). With beam search, that is multiplied by the number of beams.\n\nSo just run evaluation on 100 samples..","metadata":{}},{"cell_type":"code","source":"test_sampled = dataset[\"test\"].shuffle(seed=42).select(range(100))\n\nscore = evaluate_summaries_baseline(test_sampled, rogue_metric)\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rogue_names)\npd.DataFrame.from_dict(rouge_dict, orient=\"index\", columns=[\"baseline\"]).T","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport torch\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef chunks(list_of_elements, batch_size):\n    \"\"\"Yield successive batch-size chunks from list_of_elements\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i: i+batch_size]\n        \ndef evaluate_summaries_pegasus(dataset, metric, model, tokenizer,\n                              batch_size=16, device=device, \n                              column_text=\"article\", \n                              column_summary=\"highlights\"):\n    # split dataset into smaller batches that we can process in parallel\n    article_batches = list(chunks(dataset[column_text], batch_size))\n    target_batches = list(chunks(datset[column_summary], batch_size))\n    \n    for article_batch, target_batch in tqdm(\n        zip(article_batches, target_batches), total=len(article_batches)):\n        \n        # tokenise input articles\n        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n                          padding=\"max_length\", return_tensors=\"pt\")\n        \n        # feed to generate to produce summaries using beam search\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n                                  attention_mask=inputs[\"attention_mask\"].to(device),\n                                  length_penalty=0.8, num_beams=8, max_length=128)\n        \n        # replace <n> and add decoded texts with reference to the metric\n        decoded_summaries = [tokenizer.decode(\n            s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n        decoded_summaries = [d.replace(\"<n>\", \" \") for d in decoded_summaries]\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n    \n    # compute metric and return ROUGE scores\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:51:23.161386Z","iopub.execute_input":"2022-09-10T07:51:23.162047Z","iopub.status.idle":"2022-09-10T07:51:23.173874Z","shell.execute_reply.started":"2022-09-10T07:51:23.162005Z","shell.execute_reply":"2022-09-10T07:51:23.172771Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"from datasets import load_metric\nrouge_metric = load_metric(\"rouge\")","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:53:51.901752Z","iopub.execute_input":"2022-09-10T07:53:51.902395Z","iopub.status.idle":"2022-09-10T07:53:52.915678Z","shell.execute_reply.started":"2022-09-10T07:53:51.902333Z","shell.execute_reply":"2022-09-10T07:53:52.914240Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nmodel_ckpt = \"google/pegasus-cnn_dailymail\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\nscore = evaluate_summaries_pegasus(test_sampled, rouge_metric,\n                                  model, tokenizer, batch_size=8)\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\npd.DataFrame(rouge_dict, index=[\"pegasus\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:54:29.573329Z","iopub.execute_input":"2022-09-10T07:54:29.574102Z","iopub.status.idle":"2022-09-10T07:54:45.947852Z","shell.execute_reply.started":"2022-09-10T07:54:29.574063Z","shell.execute_reply":"2022-09-10T07:54:45.946304Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Close to published results! One thing to note is that the loss and per-token accuracy are decoupled to some degree from ROUGE scores. The loss is independent of the decoding strategy, whereas the ROUGE score  is strongly coupled. \n\nROGUE and BLEU correlate better with human judgement than loss or accuracy, better to focus on them and choose a decoding strategy for text generation models, though these are far from perfect and one should also consider human judgements.\n\nOkay! Time to train our own model for summarisation now.\n\n## Training a Summarisation Model\n\nWe can refresh the notebook to clear previous memory and start from here. Here we will train a custom text summarisation model. Use the SAMSum dataset developed by Samsung, which has dialogues and brief summaries.\n\nIn enterprise setting, these may represent the interactions between customer and support centre, so generating accurate summaries can help improve customer service and detect common patterns among customer requests.","metadata":{}},{"cell_type":"code","source":"!pip install py7zr","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:45:12.777932Z","iopub.execute_input":"2022-09-10T07:45:12.778974Z","iopub.status.idle":"2022-09-10T07:45:25.749987Z","shell.execute_reply.started":"2022-09-10T07:45:12.778922Z","shell.execute_reply":"2022-09-10T07:45:25.748756Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset_samsum = load_dataset(\"samsum\")\nsplit_lengths = [len(dataset_samsum[split]) for split in dataset_samsum]\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(\"\\nDiaogue:\")\nprint(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(f\"\\nSummary:\")\nprint(dataset_samsum[\"test\"][0][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:45:25.752658Z","iopub.execute_input":"2022-09-10T07:45:25.753091Z","iopub.status.idle":"2022-09-10T07:45:34.067868Z","shell.execute_reply.started":"2022-09-10T07:45:25.753039Z","shell.execute_reply":"2022-09-10T07:45:34.066848Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Evaluating PEGASUS on SAMSUM","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline, set_seed\n\npipe = pipeline(\"summarization\", model=\"google/pegasus-cnn_dailymail\")\n\npipe_out = pipe(dataset_samsum[\"test\"][0][\"dialogue\"])\nprint(\"Summary:\")\nprint(pipe_out[0][\"summary_text\"].replace(\" .<n>\", \".\\n\"))","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:50:43.699241Z","iopub.execute_input":"2022-09-10T07:50:43.700002Z","iopub.status.idle":"2022-09-10T07:51:23.159243Z","shell.execute_reply.started":"2022-09-10T07:50:43.699954Z","shell.execute_reply":"2022-09-10T07:51:23.158094Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"score = evaluate_summaries_pegasus(dataset_samsum[\"test\"], rouge_metric, model,\n                                  tokenizer, column_text=\"dialogue\", \n                                  column_summary=\"summary\", batch_size=8)\n\nrouge_dict = dict((rn, score[rn].mod.fmeasure) for rn in rouge_names)\npd.DataFrame(rouge_dict, index=[\"pegasus\"])","metadata":{"execution":{"iopub.status.busy":"2022-09-10T07:54:45.948982Z","iopub.status.idle":"2022-09-10T07:54:45.950762Z","shell.execute_reply.started":"2022-09-10T07:54:45.950484Z","shell.execute_reply":"2022-09-10T07:54:45.950512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With this, we can:\n- directly measure the success of training with the metric and have a good baseline\n- Fine-tune on the new dataset should have immediate improvement in ROGUE metric, if not, we'll know we have something wrong with our training loop.\n\n### Fine-Tune PEGASUS","metadata":{}},{"cell_type":"code","source":"# have a quick look at length distribution of input and outputs\nd_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"dialogue\"]]\ns_len = [len(tokenizer.encode(s)) for s in dataset_samsum[\"train\"][\"summary\"]]\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 3.5) sharey=True)\n\naxes[0].hist(d_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[0].set_titple(\"Dialogue Token Length\")\naxes[0].set_xlabel(\"Length\")\naxes[0].set_ylabel(\"Count\")\n\naxes[1].hist(s_len, bins=20, color=\"C0\", edgecolor=\"C0\")\naxes[1].set_title(\"Summary Token Length\")\naxes[1].set_xlabel(\"Length\")\nplt.tight_layout()\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Most dialogues are shorter than CNN/DailyMail articles; with around 100-200 tokens per dialogue. Similarly, the summaries are much shorter, around 20-40 tokens (average tweet length).\n\nKeep observations in mind as we build data collator for trainer.\n\nFirst, need to tokenise dataset; set max length to 1024 and 128 for dialogues and summaries, respectively.","metadata":{}},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(\n        example_batch[\"dialogue\"], max_length=1024, truncation=True)\n    \n    # differentiate to let tokenizer know it is processing sequences for decoder\n    # and can handle accordingly\n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(\n            example_batch[\"summary\"], max_length=128, truncation=True)\n        \n    return {\n        \"input_ids\": input_encodings[\"input_ids\"],\n        \"attention_mask\": input_encodings[\"attention_mask\"],\n        \"labels\": target_encodings[\"input_ids\"]\n    }\n\ndataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched=True)\n\ncolumns = [\"input_ids\", \"labels\", \"attention_mask\"]\ndataset_samsum_pt.set_format(type=\"torch\", columns=columns)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create a data collator. This is called in Trainer just before batch is fed through the model. In most cases, can use the default collator, which collects all the tensors form the batch and simply stacks them.\n\nFor summarisation task, we need to not only stack the inputs but also prepare the targets on the decoder side. PEGASUS is an encoder-decoder transformer ths has classic seq2seq architecture. Common approach is \"teacher forcing\" in the decoder. With this strategy, the decoder receives input tokens that consists of labels shifted by one in addition to encoder output; so when making prediction for next token, the decoder gets the ground truth shifted by one as an input.\n\nShift by one so that decoder only sees previous ground truth labels and not current or future ones. Shifting suffices as decoder has masked self-attention and masks all inputs at present and in future. After, we pad tokens in the labels are ignored by the loss function by setting to -100. No need to do manually, since `DataCollatorForSeq2Seq` comes to the rescue and takes care of these steps for us.","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# set up Train Arguments as usual for training\ntraining_args = TrainingArguments(\n output_dir='pegasus-samsum', num_train_epochs=1, warmup_steps=500,\n per_device_train_batch_size=1, per_device_eval_batch_size=1,\n weight_decay=0.01, logging_steps=10, push_to_hub=True,\n evaluation_strategy='steps', eval_steps=500, save_steps=1e6,\n gradient_accumulation_steps=16)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Batch size=1 since model is big; but small batch size can hurt convergence. So we use a technique called *gradient accumulation*; which makes smaller batches and aggregates gradients instead of calculating gradients of full batch at once. Once computed enough gradients, we run the optimisation step. This is a bit slower than one pass but saves us a lot of GPU memory.","metadata":{}},{"cell_type":"code","source":"# logged into HuggingFace hub to push the model after training\nfrom huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# initialise trainer with model, tokenizer, train args and data collator\ntrainer = Trainer(model=model, args=training_args,\n tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n train_dataset=dataset_samsum_pt[\"train\"],\n eval_dataset=dataset_samsum_pt[\"validation\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note; we can also evaluate generations as part of training loop with extension of `TrainingArguments` called `Seq2Seq` Training Arguments and specify `predict_with_generate=True`; pass to dedicated Trainer called `Seq2SeqTrainer` which uses generate() fn instead of model's forward pass to create predictions.","metadata":{}},{"cell_type":"code","source":"# can directly run evaluation fn on test set after train to see how well model performs\ntrainer.train()\nscore = evaluate_summaries_pegasus(\n dataset_samsum[\"test\"], rouge_metric, trainer.model, tokenizer,\n batch_size=2, column_text=\"dialogue\", column_summary=\"summary\")\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\npd.DataFrame(rouge_dict, index=[f\"pegasus\"])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ROGUE scores improve considerably over model without fine-tuning!","metadata":{}},{"cell_type":"code","source":"# Push our model to the hub\ntrainer.push_to_hub(\"Training complete!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Generating Dialogue Summaries","metadata":{}},{"cell_type":"code","source":"gen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\npipe = pipeline(\"summarization\", model=\"transformersbook/pegasus-samsum\")\n\nprint(\"Dialogue:\")\nprint(sample_text)\nprint(\"\\nReference Summary:\")\nprint(reference)\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seems model has learned to synthesise the dialogue into a summary without just extracting passages. Now.. How well does the model work on a custom input?","metadata":{}},{"cell_type":"code","source":"custom_dialogue = \"\"\"\\\nThom: Hi guys, have you heard of transformers?\nLewis: Yes, I used them recently!\nLeandro: Indeed, there is a great library by Hugging Face.\nThom: I know, I helped build it ;)\nLewis: Cool, maybe we should write a book about it. What do you think?\nLeandro: Great idea, how hard can it be?!\nThom: I am in!\nLewis: Awesome, let's do it together!\n\"\"\"\nprint(pipe(custom_dialogue, **gen_kwargs)[0][\"summary_text\"])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It makes sense and summarises well! \n\n## Conclusion\n\nText summarisation poses unique challenges, as conventional metrics like accuracy do not reflect quality of generated text. So metrics such as BLEU or ROUGE can better evaluate generated texts; however, human judgement remains the best measure.\n\nA common question is how can we summarise documents where texts are longer than model's context length. This is still an open research question and not a single strategy at the moment. Recent work by OpenAI scales summarisation by applying it recursively to long documents and using human feedback in the loop.\n\nNext chapter is question answering, which provides an answer to a question based on a text passage. Here there are good strategies to deal with long or many documents and we'll learn to scale question answering to thousands of documents!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}