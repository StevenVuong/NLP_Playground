{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Own Language Model\n",
    "\n",
    "We will build autocomplete Python code model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Follow steps to get data onto device.\n",
    "\n",
    "Note: Data preparation is crucial and one should make an effort to clean up dataset as much as possible; e.g. remove duplicated code, consider copyright, investigate language in comments, docstrings etc.. and remove personal identifying information such as passwords or keys.\n",
    "\n",
    "Huggingface overcomes memory limitations for large dataset (200GB) by using the hard drive as a direct extension of RAM memory. Can load with DownloadConfig, setting delete_extracted=True so that we do not use up all our RAM.\n",
    "\n",
    "**Load Data From Disk**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DownloadConfig\n",
    "\n",
    "download_config = DownloadConfig(delete_extracted=True)\n",
    "dataset = load_dataset(\n",
    "    \"./codeparrot\", split=\"train\", download_config=download_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP data is lightweight to load in comparison to model processing computations, so won't be a bottleneck for I/O. Also zero-copy/zero-overhead uses Apache Arrow under the hood, making it efficient to access any element.\n",
    "\n",
    "Can be very fast from few tenths of GB/s to several GB/s. And if we don't have enough space on disk, we can always stream.\n",
    "\n",
    "**Streaming**\n",
    "\n",
    "Some datasets may be >1TB so hard to fit onto a standard hard drive; an alternative is to stream. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset directly from compressed JSON files instead of creating a cache file from them\n",
    "# is an iterable Dataset object; so cannot access random elements but need to read in order\n",
    "streamed_dataset = load_dataset(\"./codeparrot\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = iter(streamed_dataset)\n",
    "\n",
    "print(dataset[0] == next(iterator))\n",
    "print(dataset[1] == next(iterator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can reference the dataset on Hub and directly download samples without downloading raw files locally - a step further.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads examples on the fly\n",
    "remote_dataset = load_dataset(\n",
    "    \"transformersbook/codeparrot\", split=\"train\", streaming=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Data to Huggingface Hub\n",
    "- Easily access from training server\n",
    "- Share with community\n",
    "\n",
    "1. Create HF Repository\n",
    "2. Clone repository\n",
    "3. Copy data to it\n",
    "4. Push to hub\n",
    "    - `git add .` may take a while since a hash of all the files is computed\n",
    "    - Also good practice to add `README` cards with documentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Tokenizer\n",
    "\n",
    "When using a pretrained model, it's important to stick with the same preprocessing design choices selected for pretraining. Otherwise the model may be fed out-of-distribution patterns or unknown tokens.\n",
    "\n",
    "However, when training a new model, a tokenizer prepared for another dataset can be suboptimal as we may encounter the following problems:\n",
    "- T5 tokenizer trained on C4 corpus that had extensive stopword filtering, so has never seen common English words such as \"sex\".\n",
    "- CamemBERT tokenizer also trained on a very large corpus of text, but only French text. Therefore, is unaware of common English words such as \"being\".\n",
    "\n",
    "T5 and CamemBERT tokenizers instead split the words into subwords, which is inefficient as it also increases the sequence length of the model. **Lesson**: Be aware of the domain and dataset preprocessing used to train tokeniser; the tokeniser and model can encode bias that has a downstream impact on model behaviour. So to create an optimal tokenizer for our dataset, we need to train one for ourselves.\n",
    "\n",
    "> Note: Training a tokenizer does not involve backpropagation or weights, it involves creating an optimal mapping from a string of text to list of integers that can be ingested by the model. Today's tokenisers has a vocabulary consisting of a list of atomic strings and a method to convert, normalise, cut or map a text string into a list of indices with this vocabulary. This list of indices is then input for our neueral network.\n",
    "\n",
    "\n",
    "### The Tokenizer Model\n",
    "\n",
    "There are several variations of preprocessing and algorithms for creating tokenisers. BPE and Unigram have the most reasonable performance in most cases:\n",
    "- **BPE**: Starts from list of basic units (characters) and creates a vocabulary by a process of progressively creating new tokens formed by merging the most frequently co-occuring basic units and adding them to the vocabulary. This process is reiterated until a predefined vocabulary size is reached.\n",
    "- **Unigram**: Starts opposite to BPE. Initialising base vocabulary with all the words in the corpus, and potential subwords. Then progressively removes/splits less useful tokens to obtain a smaller and smaller vocabulary, until the target vocabulary size is reached. WordPiece is a predecessor of Unigram, and it's official implementation was never open-sourced by Google.\n",
    "\n",
    "Note: Superiority of performance for tokenising algorithm may depend on downstream task.\n",
    "\n",
    "We want to look at aspects to consider when evaluating.\n",
    "\n",
    "### Measuring Tokeniser Performance\n",
    "\n",
    "Some possible metrics to consider (for optimality and performance):\n",
    "- *Subword fertility*: Average number of subwords produced per tokeniser word.\n",
    "- *Proportion of continued words*: Proportion of tokenised words in a corpus split into at least two subtokens.\n",
    "- *Coverage metrics*: Such as the proportion of unknown words or rarely used tokens in a tokenised corpus.\n",
    "\n",
    "Also robustness, mispelling, noise, model performance on out of domain examples etc..\n",
    "\n",
    "Above provides views on tokeniser's performance but tends to ignore the interaction of tokeniser with the model. E.g. subword fertility can be minimised by including all possible words in the vocabulary but this produces a very large vocabulary. \n",
    "\n",
    "So performance of tokenisation generally best estimated by downstream performance of model as ultimate metric. E.g. BPE good performance demonstrated on machine translation tasks by models trained with these tokenisers and vocabularies instead of character/word based tokenisation.\n",
    "\n",
    "See how we can build our own tokenizer optimised for Python code.\n",
    "\n",
    "### A Tokenizer for Python\n",
    "\n",
    "Have to think about the semantics of tokeniser; eg. if we remove whitespaces we can lose a lot of meaning. And line breaks are not meaningful as they can be added/removed without impacting semantics. But splitting on underscores can break apart variables so would affect the running, so using natural language for tokenising code may be suboptimal.\n",
    "\n",
    "We want a tokeniser that preserves spaces, a good candidate could be a byte-level tokeniser; like GPT2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "python_code = r\"\"\"def say_hello():\n",
    "    print(\"Hello, world!\")\n",
    "\n",
    "# print it\n",
    "say_hello()\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "> Python has a built-in tokenize module that splits Python strings into meaningful units (code operation, comments, indent etc..). However, one issue is that this pretokeniser is Python based which is slow and limited by the GIL. Unlike Transformers which is coded in Rust, and Rust tokenizers are many orders of magnitude faster to train and use so more likely to use these given the size of our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT2 uses no normaliser!\n",
    "print(tokenizer.backend_tokenizer.normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works directly on raw unicode inputs without any normalisation steps. \n",
    "# look at pretokenisation\n",
    "print(tokenizer.backend_tokenizer.pre_tokenizer.pre_tokenize_str(python_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All operations on input string are tracked so we can know exactly what part of the input string a token after tokenization corresponds to. Just tracks which original string a token came from, this is called *offset tracking*. If some characters are removed in normalisation, we can still associate each token with the respective part in the original string.\n",
    "\n",
    "Odd-characters.. Byte-level; so tokenizer works on bytes instead of Unicode characters. Only 256 byte alphabet characters instead of 143,859 unicode characters in unicode alphabet; and you can express each unicode character as a sequence of bytes. Occupies far less characters, so our model embedding layer can be much smaller.\n",
    "\n",
    "Trade off is that input sequence is segmented into many small pieces with only 256 byte values. This requires more compute power to reconstruct unicode characters.\n",
    "\n",
    "Middle-ground solution: Construct medium-sized vocabulary by extending 256-word vocabulary with most common combination of bytes. Progressively construct a vocabulary of predefined size by creating new vocabulary tokens through iteratively combining most frequent co-occuring tokens in vocabulary. E.g. `t` + `h` + `e` for `the`; stringing together common tokens to form bigger elementary units.\n",
    "\n",
    "One issue with BPE algorithm: These are designed to work with clean Unicode string as inputs, not bytes and expects regular ASCII characters, without spaces or control characters. However there are many control characters (newline, tabs, escape etc..) that are nonprintable. \n",
    "\n",
    "GPT2 tokenizer first maps all 256 input bytes to unicode that can be easily digested by standard BPE algorithms. What's important is that we have 256 single values at the end, forming our base vocabulary and that these 256 values are correctly handled by our BPE algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could have used a more explicit conversion, like mapping newlines to a `NEWLINE` string, but BPE algorithms are typically designed to work on characters. So keeping Unicode character for each byte character is easier to handle with out-the-box BPE algorithm.\n",
    "\n",
    "We know that newlines are mapped to unicode characters; also:\n",
    "- Spaces, and consecutive spaces are conserved\n",
    "- Consecutive spaces are considered a single word\n",
    "- Each space preceding a word is attached and considered part of the subsequent word\n",
    "\n",
    "BPE model is in charge of splitting words into subunits until all subunits belong to the predefined vocabulary. \n",
    "\n",
    "Vocabulary of GPT-2 tokenizer comprises 50,257 words:\n",
    "- Base vocabulary with 256 values of the bytes\n",
    "- 50,000 additional tokens created by repeatedly merging most commonly co-occuring tokens\n",
    "- Special character added to vocabulary to represent document boundaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check length attribute of tokenizer\n",
    "print(f\"Size of the vocabulary: {len(tokenizer)}\")\n",
    "\n",
    "print(tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPE tokenizer keeps most of the words but split multiple spaces of indentation to several consecutive spaces. Happens because tokenizer not specifically trained on code, but on texts where consecutive spaces are rare. BPE model doesn't include a specific token in vocabulary for indentation, so we can see that the tokenizer is poorly suited for the dataset's domain.\n",
    "\n",
    "Solution is to retrain the tokenizer on the target's corpus, lets get to it!\n",
    "\n",
    "### Training a Tokenizer\n",
    "\n",
    "Retrain byte-level BPE tokenizer on a slice of our corpus to get a vocabulary better adapted to Python code. To retrain a tokenizer, we just need to:\n",
    "- Specify our target vocabulary size\n",
    "- Prepare an iterator to supply lists of input strings to process to train the tokenizer's model\n",
    "- Call the `train_new_from_iterator()` method\n",
    "\n",
    "Tokenizers are trained to extract the main statistics, unlike deep learning models which are often expected to memorise a lot of specific details from the training corpus. Effectively, tokenizers are trained to know which letter combinations are the most frequent in our corpus.\n",
    "\n",
    "Therefore, don't need a large corpus to train our tokenizer on, just needs to be representative of domain and big enough for tokenizer to extract statistically significant measures. \n",
    "\n",
    "Note: Depending on vocabulary size and exact texts in corpus, the tokenizer can end up storing unexpected words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access mapping\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode\n",
    "\n",
    "byte_to_unicode_map = bytes_to_unicode()\n",
    "unicode_to_byte_map = dict((v,k) for k,v in byte_to_unicode_map.items())\n",
    "base_vocab = list(unicode_to_byte_map.keys())\n",
    "\n",
    "print(f\"Size of our base vocabulary: {len(base_vocab)}\")\n",
    "print(f\"First element: `{base_vocab[0]}`, last element: `{base_vocab[-1]}`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each token, our model will have to learn an associated word embedding; and we don't want the embedding matrix to contain too many noisy words. Also unnecessary tokens are expensive in that they take up a vector in our vocabulary and increases total vocabulary size. \n",
    "\n",
    "To train a fresh tokenizer on our corpus and examine its learned vocabulary, we just need a corpus reasonably representative of our dataset statistics. Set about 1-2GB of data, or about 100,000 documents from our corupus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "length = 100_000\n",
    "dataset_name = \"transformersbook/codeparrot-train\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\", streaming=True)\n",
    "iter_dataset = iter(dataset)\n",
    "\n",
    "def batch_iterator(batch_size=10):\n",
    "    for _ in tqdm(range(0, length, batch_size)):\n",
    "        yield [next(iter_dataset)[\"content\"] for _ in range(batch_size)]\n",
    "\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=12_500, initial_alphabet=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip 256 byte tokens and look at first tokens added thereafter\n",
    "tokens = sorted(new_tokenizer.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f\"{tokenizer.convert_tokens_to_string(t)}\" for t, _ in tokens[257:280]]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can see last words\n",
    "print([f\"{new_tokenizer.convert_tokens_to_string(t)}\" for t, _ in tokens[-12:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how can tokenise example of Python code to see what it looks like\n",
    "print(new_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if Python reserved keywords are in vocabulary\n",
    "\n",
    "import keyword\n",
    "\n",
    "print(f\"There are in total {len(keyword.kwlist)} Python keywords.\")\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer.vocab:\n",
    "        print(f\"No, keyword `{keyw}` is not in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite a number of frequent keywords, like `finally` are not in the vocabulary. Try to build a larger vocabulary using a larger sample of our dataset; 32768 words and train the tokenizer on twice as large a slice of our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 200_000\n",
    "new_tokenizer_larger = tokenizer.train_new_from_iterator(\n",
    "    batch_iterator(), vocab_size=32768, initial_alphabet=base_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at last tokens\n",
    "tokens = sorted(new_tokenizer_larger.vocab.items(), key=lambda x: x[1], reverse=False)\n",
    "print([f\"{tokenizer.convert_tokens_to_string(t)}\" for t, _ in tokens[-12:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No regular programming words, which is promising. Try our code sample with larger tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_tokenizer_larger(python_code).tokens())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can see indents are conveniently kept in vocabulary, and common English words like `Hello`, `World` are included as single tokens. This is more in line with our expectations of the data and model we may likely encounter down-stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate common Python keywords\n",
    "for keyw in keyword.kwlist:\n",
    "    if keyw not in new_tokenizer_larger.vocab:\n",
    "        print(f\"No, keyword `{keyw}` is not in vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing `nonlocal` keyword but it's rarely used in practice as it makes the syntax more complex.. Keeping it out of the vocabulary seems reasonable. \n",
    "\n",
    "So from eyeballing our larger tokenizer seems better adapted for our task; though objectively evaluating the tokenizer performance is a challenging task without measuring the model's performance.\n",
    "\n",
    "Proceed with this tokeniser and train a model to see how well it works in practice.\n",
    "\n",
    "> Can verify the new tokenizer ~twice as efficient as standard GPT-2 tokenizer by comparing sequence lengths of tokenized code examples to the original. Our tokenizer uses approx half as many tokens as existing one to encode a text, giving us twice the effective model context for free. So training a model on a context window of size 1,024 is equivalent to training a model with the old tokenizer on a context window of size 2,048 with the advantage of being much faster and more memory efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a Custom Tokenizer On the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can push tokeniser as we already authenticated with huggingface-cli\n",
    "model_ckpt = \"codeparrot\"\n",
    "org = \"transformersbook\"\n",
    "new_tokenizer_larger.push_to_hub(model_ckpt) # create a repository in our namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anyone can load our tokenizer by running\n",
    "reloaded_tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "print(reloaded_tokenizer(python_code).tokens())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can save our smaller tokenizer as well\n",
    "new_tokenizer.push_to_hub(model_ckpt+\"-small-vocabulary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Model From Scratch\n",
    "\n",
    "Here we will:\n",
    "- Decide best architecture for the task\n",
    "- Initialise a fresh model without pretrained weights\n",
    "- Set up custom data loading class\n",
    "- Create a scalable training loop\n",
    "- Finally train small and large GPT-2 models with 111 million and 1.5bn parameters\n",
    "\n",
    "With such a large codebase with code snippets and an efficient tokeniser; we are able to tackle several tasks. The three common tasks:\n",
    "- **Causal Language Modeling**: Provide beginning of code sample and ask it to generate possible completions. This is self-supervised without any annotations. A decoder-only architecture such as GPT family is usually best suitited for this task\n",
    "- **Masked Language Modeling**: Provide a model with a noisy code sample and ask it to reconstruct the original clean sample; also a self-supervised training procedure. Denoising is generally a good pretraining task to learn general representations for later downstream tasks. Can combine with fine-tuning on downstream tasks with a limited number of examples.\n",
    "- **Sequence-to-Sequence Training**: Use heuristics like regular expressions to separate comments/docstrings from code and build a large-scale dataset of (code, comments) pairs that can be used as an annotated dataset. Training task is supervised training with (input, labels) or (comment, code) pairs. Can try to train a model that learns to transcript comments in code or vice-versa. A downstream task is documentation generation from code or code generation from documentation; depending on how we set our inputs/outputs.\n",
    "\n",
    "Our objective is a code autocompletion model, so we'll go ahead with first objective!\n",
    "\n",
    "> Note: Datasets and models are large so may require multiple GPUs and would run better with scripts than on notebook!\n",
    "\n",
    "### Initialising the Model\n",
    "\n",
    "Won't use `from_pretrained()`; instead load the configuration of `gpt2-xl` to utilise the hyperparameters and adapt the vocabulary size for the new tokenizer. Then initialise a new model with this configuration with `from_config()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "config = AutoConfig.from_pretrained(\"gpt2-xl\", vocab_size=len(tokenizer))\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how large the model actually is\n",
    "print(f\"GPT-2 (xl) size: {model_size(model)/1000*2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.5bn Model! A lot of capacity, we do have a large dataset.\n",
    "\n",
    "Generally, large language models are more efficient to train as long as the dataset is reasonably large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    return sum(t.numel() for t in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the newly initialised model in `models/` directory and push to hub\n",
    "# may take a few minutes given the size of the ckpt (>5gb)\n",
    "model.save_pretrained(\"models/\" + model_ckpt, push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a smaller version we can train to make sure everything works before scaling\n",
    "# take standard GPT-2 size as a base\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "config_small = AutoConfig.from_pretrained(\"gpt2\", vocab_small=len(tokenizer))\n",
    "model_small = AutoModelForCausalLM.from_config(config_small)\n",
    "\n",
    "print(f\"GPT-2 size: {model_size(model_small) / 1000 **2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also save to hub for easy sharing and reuse\n",
    "model.save_pretrained(\"models/\" + model_ckpt + \"-small\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with two models to train, we need to make sure we can feed them the input data efficiently during training.\n",
    "\n",
    "### Implementing the Dataloader\n",
    "\n",
    "For maximal efficiency, we want to supply our model with sequences filling its context; eg. if context length is 1,024; we wan tot provide 1,024-token sequences in training. But some code examples may be shorter or longer; so we should either drop the last incomplete sequence or pad it. However, this renders our training slightly less efficient and forces us to take care of padding and masking padded token labels.\n",
    "\n",
    "We are more compute than data-constrained, so we'll take the easy and efficienty way. We can tokenise several examples and then concatenate them, separated by the special end-of-sequence token, to get a very long sequence. Finally, split this sequence into equally sized chunks, so may lose at most a small fraction of data at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can make sure we have roughly one hundred full sequences in our tokenised examples by defining our input string character length as: \n",
    "\n",
    "`input_characters = number_of_sequences * sequence_length * characters_per_token`\n",
    "\n",
    "Where:\n",
    "- `input_characters`: Number of characters in string input to our tokenizer\n",
    "- `number_of_sequences`: Number of (truncated) sequences we'd like from our tokenizer (eg. 100)\n",
    "- `sequence_length`: Number of tokens per sequence returned by the tokenizer (eg. 1024)\n",
    "- `characters_per_token`: Avg. number of characters per output token that we first need to estimate\n",
    "\n",
    "If we input a string with input_characters characters we will get on average `number_of_sequences` output sequences, and can easily calculate how much input data we are losing by dropping the last sequence. \n",
    "\n",
    "If number_of_sequences=100 it means we stack roughly 100 sequences and at most lose the last element; which may be too short or too long; so lose at most 1% of our dataset. This approach ensures we don't introduce a bias by cutting off the majority of file endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first estimate the average character length per tokeniser\n",
    "examples, total_characters, total_tokens = 500, 0, 0\n",
    "dataset = load_dataset(\"transformersbook/codeparrot-train\", split=\"train\", streaming=True)\n",
    "\n",
    "for _, example in tqdm(zip(range(examples), iter(dataset)), total=examples):\n",
    "    total_characters += len(example['content'])\n",
    "    total_tokens += len(tokenizer(example['content']).tokens())\n",
    "\n",
    "characters_per_token = total_characters / total_tokens\n",
    "\n",
    "print(characters_per_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all that is needed to create our own `IterableDataset` (a helper class provided by PyTorch) for preparing constant-length inputs for the model. Just need to inherit `IterableDataset` and set up `__iter__()` function that yields the next element with the logic we walked through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset\n",
    "\n",
    "class ConstantLengthDataset(IterableDataset):\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer, dataset, seq_length=1024, num_of_sequences=1024, chars_per_token=3.6):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset = dataset\n",
    "        self.concat_token_id = tokenizer.eos_token_id\n",
    "        self.seq_length = seq_length\n",
    "        self.input_characters = seq_length * characters_per_token * num_of_sequences\n",
    "\n",
    "    def __iter__(self):\n",
    "        iterator = iter(self.dataset)\n",
    "        more_examples = True\n",
    "    \n",
    "        while more_examples:\n",
    "            # builds buffer of strings until it has enough characters\n",
    "            buffer, buffer_len = [], 0\n",
    "            while True:\n",
    "                if buffer_len >= self.input_characters:\n",
    "                    m = f\"Buffer full: {buffer_len}>={self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    break\n",
    "                try:\n",
    "                    m = f\"Fill buffer: {buffer_len}<{self.input_characters:.0f}\"\n",
    "                    print(m)\n",
    "                    buffer.append(next(iterator)[\"content\"])\n",
    "                    buffer_len += len(buffer[-1])\n",
    "                except StopIteration:\n",
    "                    iterator = iter(self.dataset)\n",
    "            \n",
    "            # tokenized and concat with EOS\n",
    "            all_token_ids = []\n",
    "            \n",
    "            tokenized_inputs = self.tokenizer(buffer, truncation=False)\n",
    "            for tokenized_input in tokenized_inputs[\"input_ids\"]:\n",
    "            # for tokenized_input in tokenized_inputs:\n",
    "                all_token_ids.extend(tokenized_input + [self.concat_token_id])\n",
    "\n",
    "            # chunked in seq_length-sized slices\n",
    "            # no need to pad as all sequences are maximal length so no need mask either\n",
    "            for i in range(0, len(all_token_ids), self.seq_length):\n",
    "                input_ids = all_token_ids[i:i + self.seq_length]\n",
    "                if len(input_ids) == self.seq_length:\n",
    "                    yield torch.tensor(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test iterable dataset\n",
    "\n",
    "## Note we shuffle raw dataset; since this is an iterable dataset\n",
    "# we cannot shuffle whole dataset; instead we shuffle elements in the buffer first\n",
    "shuffled_dataset = dataset.shuffle(buffer_size=100)\n",
    "constant_length_dataset = ConstantLengthDataset(\n",
    "    tokenizer, shuffled_dataset ,num_of_sequences=10)\n",
    "\n",
    "dataset_iterator = iter(constant_length_dataset)\n",
    "\n",
    "lengths = [len(b) for _, b in zip(range(5), dataset_iterator)]\n",
    "print(f\"Lengths of the sequences: {lengths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this works as intended and we get constant-length inputs for the model. With a reliable data source, we can build the actual training loop!\n",
    "\n",
    "### Defining the Training Loop\n",
    "\n",
    "An obvious limitation is the memory limit, even on a single GPU; so we would need to utilise several GPU's for training. Fortunately, we can use Accelerate to make our code scalable.\n",
    "\n",
    "Accelerate library is designed to make distributed training, and changing the underlying hardware for training easy. Accelerate gives us full control over the training loop, which is what we want to explore.\n",
    "\n",
    "HF Accelerate provides an API making training scripts run with mixed precision in any kind of distributed setting (single GPU, multi-GPU and TPI's). Then can run on local machine for debugging or beefy training cluster for final training run.\n",
    "\n",
    "Making changes to native PyTorch training loop goes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import datasets\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = torch.nn.Transformer()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "dataset = load_dataset(\"my_dataset\")\n",
    "data = torch.utils.data.DataLoader(dataset ,shuffle=True)\n",
    "# makes sure optimizers and dataloaders are prepared and distributed on infrastructure\n",
    "model, optimizer, data = accelerator.prepare(model, optimizer, data)\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(10):\n",
    "    for source, targets in data:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(source)\n",
    "        loss = F.cross_entropy(output, targets)\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So! We can define some heper functions; setting up the hyperparameters for training and wrap them in Namespace for easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "# commented parameters correspond well to the small modely\n",
    "config = {\n",
    "    \"train_batch_size\": 2, # 12,\n",
    "    \"valid_batch_size\": 2, # 12,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"shuffle_buffer\": 1000,\n",
    "    \"learning_rate\": 2e-4, # 5e-4\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_warmup_steps\": 750, # 2000\n",
    "    \"gradient_accumulation_steps\": 16, # 1\n",
    "    \"max_train_steps\": 50_000, #150_000\n",
    "    \"max_eval_steps\": -1,\n",
    "    \"seq_length\": 1024, \n",
    "    \"seed\": 1,\n",
    "    \"save_checkpoint_steps\": 50_000 #15_000\n",
    "}\n",
    "\n",
    "args = Namespace(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging for training; we want to make sure relevant information is stored and easily accessible. `setup_logging()` method sets up three levels of logging using standard Python Logger, TensorBoard and Weights & Biases. Depending on preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import logging\n",
    "import wandb\n",
    "\n",
    "def setup_logging(project_name):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s %(levelname)s %(name)s %(message)s',\n",
    "        datefmt='%Y-%m-%d %H:%M:%S', level=logging.INFO,\n",
    "        # each logger gets accelerator process index; log of each worker to file\n",
    "        handlers=[\n",
    "            logging.FileHandler(f\"log/debug_{accelerator.process_index}.log\"),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    # true only for main worker; so don't initialise TensorBoard and W&B several times\n",
    "    # as we decrease the logging levels for other workers\n",
    "    if accelerator.is_main_process: # only want to set up logging once\n",
    "        wandb.init(project=project_name, config=args)\n",
    "        # return autogenerated unique wandb.run.name we use later to name our expt branch on the hub\n",
    "        run_name = wandb.run.name\n",
    "        tb_writer = SummaryWriter()\n",
    "        tb_writer.add_hparams(vars(args), {'0': 0})\n",
    "        logger.setLevel(logging.INFO)\n",
    "        datasets.utils.logging.set_verbosity_debug()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        run_name = \"\"\n",
    "        logger.setLevel(logging.ERROR)\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "    return logger, tb_writer, run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_metrics(step, metrics):\n",
    "    # Fn to log metrics with TensorBoard and W&B\n",
    "    logger.info(f\"Step {step}: {metrics}\")\n",
    "    if accelerator.is_main_process:\n",
    "        wandb.log(metrics)\n",
    "        [tb_writer.add_scalar(k, v, step) for k, v in metrics.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "def create_dataloaders(dataset_name):\n",
    "    # create dataloader for train and validation sets \n",
    "    train_data = load_dataset(dataset_name+'-train', split=\"train\", streaming=True)\n",
    "    train_data = train_data.shuffle(buffer_size=args.shuffle_buffer, seed=args.seed)\n",
    "\n",
    "    valid_data = load_dataset(dataset_name+'-valid', split=\"validation\", streaming=True)\n",
    "\n",
    "    train_dataset = ConstantLengthDataset(tokenizer, train_data, seq_length=args.seq_length)\n",
    "    valid_dataset = ConstantLengthDataset(tokenizer, valid_data, seq_length=args.seq_length)\n",
    "\n",
    "    # wrap dataset in DataLoader; which takes care of batching\n",
    "    # HF Accelerate takes care of distributing the batches to each worker\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.train_batch_size)\n",
    "    eval_dataloader = DataLoader(valid_dataset, batch_size=args.valid_batch_size)\n",
    "\n",
    "    return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    # fn to differentiate parameters that should receive weight decay\n",
    "    # generally, biases and LayerNorm weights are not subject to weight decay\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {'params': params_with_wd, 'weight_decay': args.weight_decay},\n",
    "        {'params': params_without_wd, 'weight_decay': 0.0}\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    # eval fn that calculates loss and perplexity on evaluation set\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch, labels=batch)\n",
    "        loss = outputs.loss.repeat(args.valid_batch_size)\n",
    "        losses.append(accelerator.gather(loss))\n",
    "        if args.max_eval_steps > 0 and step >= args.max_eval_steps: break\n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = torch.tensor(float(\"inf\"))\n",
    "    return loss.item(), perplexity.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity measures how well the model's output probability distributions predict the target tokens. Lower perplexity -> better performance. Note: We can also compute perplexity by exponentiating cross-entropy loss that we get from model's output; especially at start of training when the loss is still high, we can get numerical outflow when calculating perplexity. We catch this error and set perplexity to infinity in these instances.\n",
    "\n",
    "One more thing: HF Hub can use Git under the hood to store and version models and datasets. Repository class from HF Hub allow us to programmatically access the repository and pull, branch, commit or push. Use in our script to continuously push checkpoints to Hub during training.\n",
    "\n",
    "With everything in place; we can write the heart of the training script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# Accelerator\n",
    "accelerator = Accelerator()\n",
    "samples_per_step = accelerator.state.num_processes * args.train_batch_size\n",
    "\n",
    "# logging\n",
    "logger, tb_writer, run_name = setup_logging(project_name.split(\"/\")[1])\n",
    "logger.info(accelerator.state)\n",
    "\n",
    "# Load model and tokenizer\n",
    "if accelerator.is_main_process:\n",
    "    hf_repo = Repository(\"./\", clone_from=project_name, revision=run_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./\", gradient_checkpointing=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./\")\n",
    "\n",
    "# Load dataset and dataloader\n",
    "train_dataloader, eval_dataloader = create_dataloaders(dataset_name)\n",
    "\n",
    "# Prepare optimizer and LR scheduler\n",
    "optimizer = AdamW(get_grouped_params(model), lr=args.learning_rate)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=args.lr_scheduler_type, optimizer=optimizer,\n",
    "    num_warmup_steps=args.num_warmup_steps, \n",
    "    num_training_steps=args.max_train_steps\n",
    ")\n",
    "\n",
    "def get_lr(): return optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "# prepare everything with our `accelerator` (order of args not important)\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "# Train model\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for step, batch in enumerate(train_dataloader, start=1):\n",
    "    loss = model(batch, labels=batch).loss\n",
    "    log_metrics(\n",
    "        step, \n",
    "        {\"lr\": get_lr(), \"samples\": step*samples_per_step, \"steps\": completed_steps, \"loss/train\": loss.item()})\n",
    "    loss = loss / args.gradient_accumulation_steps\n",
    "    accelerator.backward(loss) \n",
    "\n",
    "    if step % args.gradient_accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        completed_steps += 1\n",
    "\n",
    "    if step % args.save_checkpoint_steps == 0:\n",
    "        logger.info(\"Evaluating and saving model checkpoint\")\n",
    "        eval_loss, perplexity = evaluate()\n",
    "        log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "        accelerator.wait_for_everyone()\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        if accelerator.is_main_process:\n",
    "            unwrapped_model.save_pretrained(\"./\")\n",
    "            hf_repo.push_to_hub(commit_message=f\"step {step}\")\n",
    "        model.train()\n",
    "    \n",
    "    if completed_steps >= args.max_train_steps:\n",
    "        break\n",
    "\n",
    "# Evaluate and save the last checkpoint\n",
    "logger.info(\"Evaluatiing and saving model after training)\")\n",
    "eval_loss, perplexity = evaluate()\n",
    "log_metrics(step, {\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "accelerator.wait_for_everyone()\n",
    "unwrapped_model = accelerator.unwrap_model(model)\n",
    "if accelerator.is_main_process:\n",
    "    unwrapped_model.save_pretrained(\"./\")\n",
    "    hf_repo.push_to_hub(commit_message=f\"final model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is all we need to train a large language model on distributed infrastructure! Deconstructing the script a bit:\n",
    "\n",
    "- *Model Saving*: Run script from model repository, and at the start check out a new branch named after the `run_name` we get from Weights & Bisaes. Later, we commit the model at each checkpoint and push it to the Hub. So each experiment is on a new branch and each commit represents a model checkpoint. Note we need to call `wait_for_everyone()` and `unwrap_model()` to make sure the model is properly synchronised before we store it.\n",
    "- *Optimization*: Use `AdamW` with cosine learning rate schedule after linear warmup period. Hyper-parameters closely follow GPT-3 parameters for similar sized models\n",
    "- *Evaluation*: Evaluate every time we save, so every `save_checkpoint_steps` and after training. We log validation perplexity as well as validation loss.\n",
    "- *Gradient accumulation and checkpointing*: Gradient accumulation means the batch sizes can fit into GPU memory; rather than gatherig graidients over several backpasses and optimises once enough gradients are accumulated. Using a method called *gradient checkpointing*, we can trade some of the memory footprint for approx 20% training slowdown; allowing us to fit even the large model in a single GPU.\n",
    "\n",
    "\n",
    "For training multiple GPUs, there are several approaches depending on model and data volume. HF Accelerate uses `DataDistributedParallelism`; allowing one to train models faster with larger batch sizes that wouldn't fit into any single GPU. Step by step:\n",
    "\n",
    "1. Each worker consists of a GPU; in HF Accelerate, there is a dataloader on the main process that prepares batches of data and sends them to all the workers.\n",
    "2. Each GPU receives a batch of data and calculates the loss and accumulated gradients from forward and backward passes with a local copy of the model.\n",
    "3. The gradients from each node are averaged with a *reduce* pattern, and the averaged gradients are sent back to each worker.\n",
    "4. The gradients are applied using the optimizer on each node individually. Might seem like redundant work, but avoids transferring copies of the large models between nodes. Trade-off is that other nodes would need to wait until they received the updated version.\n",
    "5. With the models updated, we start all over again, with the main worker preparing new batches.\n",
    "\n",
    "Simple pattern allows us to train large models extremely fast by scaling up to the number of available GPUs without much additional logic. Sometimes, however, this is not enough. And we may need more sophisticated parallelism strategies.\n",
    "\n",
    "Though now, with everything required, we can launch a job!\n",
    "\n",
    "### The Training Run\n",
    "\n",
    "Put training script in a python file, then add requirements.txt file and push to github. To spin up the training script, we can run the commands:\n",
    "\n",
    "```sh\n",
    "git clone https://huggingface.co/transformersbook/codeparrot\n",
    "cd codeparrot\n",
    "pip install -r requirements.txt\n",
    "wandb login\n",
    "accelerate config\n",
    "accelerate launch codeparrot_training.py\n",
    "```\n",
    "\n",
    "And our model will start training! wandb will prompt a login; and `accelerate config` will guide us through setting up the infrastructure. The smaller model takes about 24 hrs to train and the large 7 days. We can use the smaller as a testing ground for larger models.\n",
    "\n",
    "After training is completed; we can merge the experiment branch on the Hub back into the main branch with the commands:\n",
    "\n",
    "```sh\n",
    "git checkout main\n",
    "git merge <RUN_NAME>\n",
    "git push\n",
    "```\n",
    "\n",
    "And now we can look at how to investigate the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "We see the training loss and validation perplexity go down continuously; and the loss curve looks almost linear on log-log scale. Large model converges faster in terms of processed tokens, though overall training takes longer.\n",
    "\n",
    "Types of analyses: Qualitative and quantitative. In the former, we look at concrete examples to better understand which cases the model succeeds and where it fails. In the latter, we look at model's performance statistically on a large set of test cases.\n",
    "\n",
    "First, we wrap the small model in a pipeline and use it to continue some code inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "# wrap code in a pipeline\n",
    "model_ckpt = \"tansformersbook/codeparrot-small\"\n",
    "generation = pipeline(\"text-generation\", model=model_ckpt, device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generation pipeline to generate candidate completions from a prompt. By default, pipeline will generate code until a predefined maximum length, and the output could contain multiple funcitons or classes.\n",
    "\n",
    "To keep outputs concise, we'll implement a `first_block()` function that uses regex to extract the first occurrence of a function or class. The `complete_code()` function below applies this logic to print out the completions generated by CodeParrot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import set_seed\n",
    "\n",
    "def first_block(string):\n",
    "    return re.split(\"\\nclass|\\ndef|\\ndef|\\n#|\\n@|\\nprint|\\nif\", string)[0].rstrip()\n",
    "\n",
    "def complete_code(pipe, prompt, max_lenght=64, num_completions=4, seed=1):\n",
    "    set_seed(seed)\n",
    "    gen_kwargs = {\"temperature\":0.4, \"top_p\":0.95, \"top_k\":0, \"num_beams\":1, \"do_sample\": True}\n",
    "    code_gens = generation(\n",
    "        prompt, num_return_sequences=num_completions, max_length=max_length, **gen_kwargs)\n",
    "    code_strings = []\n",
    "    for code in code_gens:\n",
    "        generated_code = first_block(code_gen[\"generated_text\"][len(prompt):])\n",
    "        code_strings.append(generated_code)\n",
    "    print((\"\\n\"+\"=\"*80 + \"\\n\").join(code_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test; have the model write a function that calculates the area of a rectangle\n",
    "prompt = \"\"\"def area_of_rectangle(a: float, b: float):\n",
    "    # return the area of the rectangle\n",
    "\"\"\"\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract urls from a HTML string\n",
    "promt = \"\"\"def get_urls_from_html(html):\n",
    "# Get all embedded URLs in a HTML string.\n",
    "\"\"\"\n",
    "complete_code(generation, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test function on HF home page\n",
    "import requests\n",
    "\n",
    "def get_urls_from_html(html):\n",
    "    return [url for url in re.findall(r'<a href=\"(.*?)\"', html) if url]\n",
    "\n",
    "print(\" | \".join(get_urls_from_html(requests.get(\"https://hf.co/\").text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load large model and see if we can translate a function from pure Python to numpy\n",
    "\n",
    "prompt = \"\"\"# a function in native python\n",
    "def mean(a):\n",
    "    return sum(a) / len(a)\n",
    "    \n",
    "# the same function using numpy:\n",
    "import numpy as np\n",
    "def mean(a):\"\"\"\n",
    "complete_code(generation, prompt, max_lenth=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use to build a Scikit-learn model\n",
    "prompt = '''X = np.random.randn(100, 100)\n",
    "y = np.random.randint(0, 1, 100)\n",
    "# fit random forest classifier with 20 estimators'''\n",
    "complete_code(generation, prompt, max_length=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BLEU metric is bad here as it measures overlap of text; and in code there is a lot of freedom in terms of variables na dclasses; and the success of a program does not depend on the naming convention, as long as it is consistent. However, BLEU would punish a name which deviates from the reference, even though it may be impossible to predict.\n",
    "\n",
    "Better ways to measure code, such as performance in unit tests; and fraction that pass through tests. Or other evaluation regimen!\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "We build a code autocomplete function for Python! We also built:\n",
    "- Large scale dataset suitable for pretraining a large language model\n",
    "- Created a custom tokenizer able to efficiently encode Python code with that dataset\n",
    "- Wrote a training script with HF transfoermers to train small and large versions of GPT-2 model from scratch on a multi-GPU instance\n",
    "\n",
    "We saw reasonable code continuations and discussed areas of improvement!\n",
    "\n",
    "Go forth and conquer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
