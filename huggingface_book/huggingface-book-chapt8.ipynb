{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Chapt 8: Models in Production","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-27T13:03:06.096184Z","iopub.execute_input":"2022-09-27T13:03:06.096946Z","iopub.status.idle":"2022-09-27T13:03:17.611715Z","shell.execute_reply.started":"2022-09-27T13:03:06.096785Z","shell.execute_reply":"2022-09-27T13:03:17.610601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=bert_ckpt)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:17.617345Z","iopub.execute_input":"2022-09-27T13:03:17.620240Z","iopub.status.idle":"2022-09-27T13:03:46.369394Z","shell.execute_reply.started":"2022-09-27T13:03:17.620201Z","shell.execute_reply":"2022-09-27T13:03:46.368224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"\"\"Hey, I'd like to rent a vehicle from Nov 1st to Nov 15th in Paris and I need a 15 passenger van\"\"\"\npipe(query)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:46.371143Z","iopub.execute_input":"2022-09-27T13:03:46.371501Z","iopub.status.idle":"2022-09-27T13:03:46.589736Z","shell.execute_reply.started":"2022-09-27T13:03:46.371461Z","shell.execute_reply":"2022-09-27T13:03:46.588850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Must address:\n- Model performance: How well does model do on test set that reflects production data\n- Latency: How fast can deliver predictions?\n- Memory: Especially when no access to cloud server; can fit on edge device?","metadata":{}},{"cell_type":"code","source":"class PerformanceBenchmark:\n    def __init__(self, pipeline, dataset, optim_type=\"BERT baseline\"):\n        self.pipeline = pipeline\n        self.dataset = dataset\n        self.optim_type = optim_type\n        \n    def compute_accuracy(self):\n        # We'll define this later\n        pass\n\n    def compute_size(self):\n        # We'll define this later\n        pass\n    \n    def time_pipeline(self):\n        # define this later\n        pass\n    \n    def run_benchmark(self):\n        \"\"\"Collect all metrics in a dictionary.\"\"\"\n        metrics = {}\n        # keep track of different optimisation techniques\n        metrics[self.optim_type] = self.compute_size()\n        metrics[self.optim_type].update(self.time_pipeline())\n        metrics[self.optim_type].update(self.compute_accuracy())\n        return metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:46.592625Z","iopub.execute_input":"2022-09-27T13:03:46.595632Z","iopub.status.idle":"2022-09-27T13:03:46.605890Z","shell.execute_reply.started":"2022-09-27T13:03:46.595593Z","shell.execute_reply":"2022-09-27T13:03:46.604919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\n# load CLINC50 dataset from hub; plus config is out of scope training examples\nclinc = load_dataset(\"clinc_oos\", \"plus\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:46.607296Z","iopub.execute_input":"2022-09-27T13:03:46.607968Z","iopub.status.idle":"2022-09-27T13:03:49.714520Z","shell.execute_reply.started":"2022-09-27T13:03:46.607927Z","shell.execute_reply":"2022-09-27T13:03:49.713497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample = clinc[\"test\"][42]\nsample","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:49.716015Z","iopub.execute_input":"2022-09-27T13:03:49.717072Z","iopub.status.idle":"2022-09-27T13:03:49.729630Z","shell.execute_reply.started":"2022-09-27T13:03:49.717032Z","shell.execute_reply":"2022-09-27T13:03:49.728673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"intents = clinc[\"test\"].features[\"intent\"] # provided as IDs\nintents.int2str(sample[\"intent\"]) # map to strings","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:49.731100Z","iopub.execute_input":"2022-09-27T13:03:49.732192Z","iopub.status.idle":"2022-09-27T13:03:51.271855Z","shell.execute_reply.started":"2022-09-27T13:03:49.732150Z","shell.execute_reply":"2022-09-27T13:03:51.270719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Establish a Benchmark","metadata":{}},{"cell_type":"code","source":"from datasets import load_metric\n\naccuracy_score = load_metric(\"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:51.273201Z","iopub.execute_input":"2022-09-27T13:03:51.273603Z","iopub.status.idle":"2022-09-27T13:03:51.677816Z","shell.execute_reply.started":"2022-09-27T13:03:51.273568Z","shell.execute_reply":"2022-09-27T13:03:51.676820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_accuracy(self):\n    \"\"\"Overrides PerformanceBenchmark.compute_accuracy() method.\n    Expects the predictions and references (ground truth) to be integers.\n    Use the pipeline to extract the predictions from the text and then str2int() method\n    to map prediction to corresponding ID.\n    \n    Collects predictions and labels in lists before returnign accuracy on the dataset\n    \"\"\"\n    preds, labels = [], []\n    for example in self.dataset:\n        pred = self.pipeline(example[\"text\"])[0][\"label\"]\n        label = example[\"intent\"]\n        preds.append(intents.str2int(pred))\n        labels.append(label)\n    accuracy = accuracy_score.compute(predictions=preds, references=labels)\n    print(f\"Accuracy on test set - {accuracy['accuracy']:.3f}\")\n    return accuracy\n\nPerformanceBenchmark.compute_accuracy = compute_accuracy","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:51.679314Z","iopub.execute_input":"2022-09-27T13:03:51.680339Z","iopub.status.idle":"2022-09-27T13:03:51.688276Z","shell.execute_reply.started":"2022-09-27T13:03:51.680300Z","shell.execute_reply":"2022-09-27T13:03:51.687245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute size of model with torch.save(); uses Pickle module under the hood\n# can see weights and biases under the hood; each key/value corresponds to layer and tensor\nlist(pipe.model.state_dict().items())[42]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:51.693132Z","iopub.execute_input":"2022-09-27T13:03:51.694228Z","iopub.status.idle":"2022-09-27T13:03:51.710618Z","shell.execute_reply.started":"2022-09-27T13:03:51.694192Z","shell.execute_reply":"2022-09-27T13:03:51.709574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.save(pipe.model.state_dict(), \"model.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:51.712152Z","iopub.execute_input":"2022-09-27T13:03:51.713049Z","iopub.status.idle":"2022-09-27T13:03:52.281219Z","shell.execute_reply.started":"2022-09-27T13:03:51.713012Z","shell.execute_reply":"2022-09-27T13:03:52.280234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get model size in bytes\n\nfrom pathlib import Path\n\ndef compute_size(self):\n    \"\"\"Overrides PerformanceBenchmark.compute_size() method\"\"\"\n    state_dict = self.pipeline.model.state_dict()\n    tmp_path = Path(\"model.pt\")\n    torch.save(state_dict, tmp_path)\n    # calc size in mb\n    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n    # delete temporary file\n    tmp_path.unlink()\n    print(f\"Model size (MB) - {size_mb:.2f}\")\n    return {\"size_mb\": size_mb}\n\nPerformanceBenchmark.compute_size = compute_size","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:52.282864Z","iopub.execute_input":"2022-09-27T13:03:52.283277Z","iopub.status.idle":"2022-09-27T13:03:52.290259Z","shell.execute_reply.started":"2022-09-27T13:03:52.283237Z","shell.execute_reply":"2022-09-27T13:03:52.289142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"latency: time takes to feed query text and return predicted intent from model.","metadata":{}},{"cell_type":"code","source":"# time average latency per query\n\nfrom time import perf_counter\n\nfor _ in range(3):\n    start_time = perf_counter()\n    _ = pipe(query)\n    latency = perf_counter() - start_time\n    print(f\"Latency (ms) - {1000 * latency:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:52.291793Z","iopub.execute_input":"2022-09-27T13:03:52.292431Z","iopub.status.idle":"2022-09-27T13:03:52.651629Z","shell.execute_reply.started":"2022-09-27T13:03:52.292374Z","shell.execute_reply":"2022-09-27T13:03:52.650203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef time_pipeline(self, query=\"What is the pin number for my account?\"):\n    \"\"\"Overrides the PerformanceBenchmark.time_pipeline() method.\n    Performance varies depending on hardware, what's important is relative diff between \n    runs (consistency).\"\"\"\n    latencies = []\n    # Warmup\n    for _ in range(10):\n        _ = self.pipeline(query)\n    # timed run\n    for _ in range(100):\n        start_time = perf_counter()\n        _ = self.pipeline(query)\n        latency = perf_counter() - start_time\n        latencies.append(latency)\n    # compute run statistics\n    time_avg_ms = 1000 * np.mean(latencies)\n    time_std_ms = 1000 * np.std(latencies)\n    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n\nPerformanceBenchmark.time_pipeline = time_pipeline","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:52.653276Z","iopub.execute_input":"2022-09-27T13:03:52.653882Z","iopub.status.idle":"2022-09-27T13:03:52.663038Z","shell.execute_reply.started":"2022-09-27T13:03:52.653824Z","shell.execute_reply":"2022-09-27T13:03:52.661698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# benchmark BERT baseline\npb = PerformanceBenchmark(pipe, clinc[\"test\"])\nperf_metrics = pb.run_benchmark()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:03:52.665790Z","iopub.execute_input":"2022-09-27T13:03:52.666792Z","iopub.status.idle":"2022-09-27T13:13:03.612065Z","shell.execute_reply.started":"2022-09-27T13:03:52.666764Z","shell.execute_reply":"2022-09-27T13:13:03.609835Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Knowledge Distillation","metadata":{}},{"cell_type":"markdown","source":"Train a smaller student model to mimic a slower, larger better-performing teacher. We typically scale the probabilities with a temperature hyperparameter *T* before applying softmax to produce a softer probability distributon over classes and reveal more information about decision boundary that the teacher has learned. *T* = 1 recovers the original softmax distribution. \n\nWe can use the **Kullback-Leibler (KL)** Divergence to measure the difference between two probability distributions. We can approximate how much is lost when we approximate the probability distribution of the teacher with the student. So we get a knowledge distillation loss:\n\n$L_{KD} = T^2D_{KL}$\n\nWhere $T^2$ is a normalisation factor to counter the fact that the magnitude of the gradients produced by soft labels scale as $1/T^2$. For classification tasks, the student loss is then a weighted average of the distillation loss with the usual cross-entropy loss $L_{CE}$ of the ground truth labels. Where the weighting parameter $\\alpha$ is how much to weigh the distillation loss vs the cross entropy loss.\n\n**Pretraining**: Can be used during pretraining to create a general-purpose student to be later fine-tuned. Ex. in DistilBERT, the loss includes a cosine embedding loss to align the directions of the hidden state vectors between the teacher and student.","metadata":{}},{"cell_type":"markdown","source":"### Creating a Knowledge Distillation Trainer\n\nSome things to add to Trainer base class:\n- Hyper-parameters $\\alpha$ and *T* which control relative weight of distillation loss and how much softmax probability should be smoothened\n- Fine-tuned teacher model, which is BERT-base in our case\n- New loss function that combines the cross-entropy loss with knowledge distillation loss","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nclass DistillationTrainingArguments(TrainingArguments):\n    def __init__(self, *args, alpha=0.5, temperature=2.0, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.alpha = alpha\n        self.temperature = temperature","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:09.125386Z","iopub.execute_input":"2022-09-27T13:13:09.125748Z","iopub.status.idle":"2022-09-27T13:13:09.131483Z","shell.execute_reply.started":"2022-09-27T13:13:09.125718Z","shell.execute_reply":"2022-09-27T13:13:09.130508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# subclass Trainer and override compute_loss() to include knowledge distillation loss L_kd\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import Trainer\n\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        \n    def compute_loss(self, model, inputs, return_outputs=False):\n        outputs_stu = model(**inputs)\n        # Extract cross-entropy loss and logits from student\n        loss_ce = outputs_stu.loss\n        logits_stu = outputs_stu.logits\n        # Extract logits from teacher\n        with torch.no_grad():\n            outputs_tea = self.teacher_model(**inputs)\n            logits_tea = outputs_tea.logits\n        # soften probabilities and compute distillation loss\n        loss_fct = nn.KLDivLoss(reduction=\"batchmean\") # average loss over batch dim\n        loss_kd = self.args.temperature ** 2 * loss_fct(\n            # inputs as log prob.\n            F.log_softmax(logits_stu / self.args.temperature, dim=-1),\n            # labels as normal prob.\n            F.softmax(logits_tea / self.args.temperature, dim=-1)\n        )\n        # return weighted student loss\n        loss = self.args.alpha * loss_ce + (1. - self.args.alpha) * loss_kd\n        return (loss, outputs_stu) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:09.306488Z","iopub.execute_input":"2022-09-27T13:13:09.306858Z","iopub.status.idle":"2022-09-27T13:13:09.428561Z","shell.execute_reply.started":"2022-09-27T13:13:09.306827Z","shell.execute_reply":"2022-09-27T13:13:09.427562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Choosing a Good Student Initialisation","metadata":{}},{"cell_type":"markdown","source":"Smaller model in general for the student to reduce the latency and memory footprint. Rule of thumb from literature: Works best when teacher and student are of the same model type. Possibly because there are diferent output embedding spaces, hindering the student's ability to mimic the teacher.","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# instantiate tokenizer from DistilBERT\nstudent_ckpt = \"distilbert-base-uncased\"\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_ckpt)\n\ndef tokenize_text(batch):\n    return student_tokenizer(batch[\"text\"], truncation=True)\n\n# remove text column as we no longer need\nclinc_enc = clinc.map(tokenize_text, batched=True, remove_columns=[\"text\"])\n# rename intent to labels\nclinc_enc = clinc_enc.rename_column(\"intent\", \"labels\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:09.767191Z","iopub.execute_input":"2022-09-27T13:13:09.767930Z","iopub.status.idle":"2022-09-27T13:13:12.922366Z","shell.execute_reply.started":"2022-09-27T13:13:09.767891Z","shell.execute_reply":"2022-09-27T13:13:12.921484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:12.924312Z","iopub.execute_input":"2022-09-27T13:13:12.924759Z","iopub.status.idle":"2022-09-27T13:13:12.969048Z","shell.execute_reply.started":"2022-09-27T13:13:12.924721Z","shell.execute_reply":"2022-09-27T13:13:12.968125Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we need to define hyper-parameters and `compute_metrics()` function for DistillationTrainer.","metadata":{}},{"cell_type":"code","source":"def compute_metrics(pred):\n    predictions, labels = pred\n    # convert logits to most probable prediction using argmax\n    predictions = np.argmax(predictions, axis=1)\n    # can use accuracy() score fn defined previously\n    return accuracy_score.compute(predictions=predictions, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:12.970809Z","iopub.execute_input":"2022-09-27T13:13:12.971509Z","iopub.status.idle":"2022-09-27T13:13:12.978299Z","shell.execute_reply.started":"2022-09-27T13:13:12.971473Z","shell.execute_reply":"2022-09-27T13:13:12.977222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 48\n\nfinetuned_ckpt = \"distilbert-base-uncased-finetuned-clinc\"\nstudent_training_args = DistillationTrainingArguments(\n    output_dir=finetuned_ckpt, evaluation_strategy = \"epoch\",\n    num_train_epochs=5, learning_rate=2e-5,\n    per_device_train_batch_size = batch_size,\n    per_device_eval_batch_size = batch_size,\n    alpha=1, weight_decay=0.01,\n    push_to_hub=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:12.981104Z","iopub.execute_input":"2022-09-27T13:13:12.982274Z","iopub.status.idle":"2022-09-27T13:13:12.996172Z","shell.execute_reply.started":"2022-09-27T13:13:12.982236Z","shell.execute_reply":"2022-09-27T13:13:12.994930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mappings between each intent and label ID; can be obtained from BERT base model\nid2label = pipe.model.config.id2label\nlabel2id = pipe.model.config.label2id","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:12.997780Z","iopub.execute_input":"2022-09-27T13:13:12.998506Z","iopub.status.idle":"2022-09-27T13:13:13.003959Z","shell.execute_reply.started":"2022-09-27T13:13:12.998472Z","shell.execute_reply":"2022-09-27T13:13:13.002887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoConfig\n\n# configuration for student with information about label mappings\n# also specify number of classes our model should expect\nnum_labels = intents.num_classes\nstudent_config = (AutoConfig.from_pretrained(\n    student_ckpt, num_labels=num_labels, id2label=id2label, label2id=label2id\n))","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:13.005688Z","iopub.execute_input":"2022-09-27T13:13:13.006806Z","iopub.status.idle":"2022-09-27T13:13:13.113008Z","shell.execute_reply.started":"2022-09-27T13:13:13.006771Z","shell.execute_reply":"2022-09-27T13:13:13.111973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef student_init():\n    # can provide config in .from_pretrained\n    return (AutoModelForSequenceClassification.from_pretrained(student_ckpt, config=student_config).to(device))","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:17.957275Z","iopub.execute_input":"2022-09-27T13:13:17.957637Z","iopub.status.idle":"2022-09-27T13:13:17.964010Z","shell.execute_reply.started":"2022-09-27T13:13:17.957606Z","shell.execute_reply":"2022-09-27T13:13:17.962618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above is everything needed for the distillation trainer (distilBERT). Now we can load the teacher and fine-tune.","metadata":{}},{"cell_type":"code","source":"!sudo apt-get install git-lfs","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:19.455947Z","iopub.execute_input":"2022-09-27T13:13:19.456930Z","iopub.status.idle":"2022-09-27T13:13:27.361243Z","shell.execute_reply.started":"2022-09-27T13:13:19.456889Z","shell.execute_reply":"2022-09-27T13:13:27.360011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:20:54.122152Z","iopub.execute_input":"2022-09-27T13:20:54.122578Z","iopub.status.idle":"2022-09-27T13:20:54.468331Z","shell.execute_reply.started":"2022-09-27T13:20:54.122547Z","shell.execute_reply":"2022-09-27T13:20:54.467274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"teacher_ckpt = \"transformersbook/bert-base-uncased-finetuned-clinc\"\nteacher_model = (AutoModelForSequenceClassification.from_pretrained(\n    teacher_ckpt, num_labels=num_labels).to(device))\n\ndistilbert_trainer = DistillationTrainer(\n    model_init=student_init, teacher_model=teacher_model, args=student_training_args,\n    train_dataset=clinc_enc[\"train\"], eval_dataset=clinc_enc[\"validation\"],\n    compute_metrics=compute_metrics, tokenizer=student_tokenizer\n)\n\ndistilbert_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:20:55.594934Z","iopub.execute_input":"2022-09-27T13:20:55.595735Z","iopub.status.idle":"2022-09-27T13:20:57.527030Z","shell.execute_reply.started":"2022-09-27T13:20:55.595690Z","shell.execute_reply":"2022-09-27T13:20:57.525197Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# push model to hub for later re-use\ndistilbert_trainer.push_to_hub(\"Training completed!\")","metadata":{"execution":{"iopub.status.busy":"2022-09-27T13:13:04.380061Z","iopub.status.idle":"2022-09-27T13:13:04.380460Z","shell.execute_reply.started":"2022-09-27T13:13:04.380281Z","shell.execute_reply":"2022-09-27T13:13:04.380300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use model in a pipeline for our performance benchmark\nfinetuned_ckpt = \"stevevee0101/distilbert-base-uncased-finetuned-clinc\"\npipe = pipeline(\"text-classification\", model=finetuned_ckpt)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pass to PerformanceBenchmark to compute metrics\noptim_type = \"DistilBERT\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Scatterplot accuracy vs latency, with point radius being the model size on disk. ","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndef plot_metrics(perf_metrics, current_optim_type):\n    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n    \n    for idx in df.index:\n        df_opt = df.loc[idx]\n        # add dashed circle around current optimisation type\n        if idx == current_optim_type:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                       alpha=0.5, s=df_opt[\"size_mb\"], label=idx,\n                       marker=\"$\\u25CC$\")\n        else:\n            plt.scatter(df_opt[\"time_avg_ms\"], df_opt[\"accuracy\"] * 100,\n                       s=df_opt[\"size_mb\"], label=idx, alpha=0.5)\n        \n    legend = plt.legend(bbox_to_anchor=(1,1))\n    for handle in legend.legendHandles:\n        handle.set_sizes([20])\n        \n    plt.ylim(80, 90)\n    \n    # use slowest model to define x-axis range\n    xlim = int(perf_metrics[\"BERT baseline\"][\"time_avg_ms\"] + 3)\n    plt.xlim(1, xlim)\n    plt.ylabel(\"Accuracy (%)\")\n    plt.xlabel(\"Average latency (ms)\")\n    plt.show()\n    \nplot_metrics(perf_metrics, optim_type)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Smaller model significantly decreases average latency; with only 1% reduction in accuracy. Try to close accuracy gap with distillation loss of teacher and find good values for $\\alpha$ and $T$.","metadata":{}},{"cell_type":"markdown","source":"### Find Good Hyperparameters with Optuna","metadata":{}},{"cell_type":"markdown","source":"Could do a grid-search.. A better alternativ eis to use *Optuna*, an optimisation framework. We find the minimum of $f(x,y)$ by defining an objective() function that returns the alue of $f(x,y)$.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # specifies parameter ranges to sample uniformly from\n    x = trial.suggest_float(\"x\", -2, 2)\n    y = trial.suggest_float(\"y\", -2, 2)\n    return (1 - x) ** 2 + 100 * (y - x ** 2) ** 2","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna \n\nstudy = optuna.create_study()\n# collects multiple trials as a study\nstudy.optimize(objective, n_trials=1000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get best parameters once study is completed\nstudy.best_params","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Finds values reasonably close to the global minimum (1, 1). Follow similar logic, define hyper-parameter space we wish to optimise over.","metadata":{}},{"cell_type":"code","source":"# include number of training epochs\n\ndef hp_space(trial):\n    return {\n        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 5, 10),\n        \"alpha\": trial.suggest_float(\"alpha\", 0, 1),\n        \"temperature\": trial.suggest_int(\"temperature\", 2, 20)\n    }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_run = distilbert_trainer.hyperparameter_search(\n    # specify 'maximize' as we want the best accuracy possible\n    n_trials=20, direction=\"maximize\", hp_space=hp_space\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(best_run)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Alpha being 0.12 tells us most of the training signal is coming from the knowledge distillation term instead of cross-entropy loss.","metadata":{}},{"cell_type":"code","source":"# update training arguments with these values and run final training run\nfor k, v in best_run.hyperparameters.items():\n    setattr(student_training_args, k, v)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define a new repository to store our distilled model\ndistilled_ckpt = \"distilbert-base-uncased-distilled-clinc\"\nstudent_training_args.output_dir = distilled_ckpt\n\n# create a new trainer with optimal parameters\ndistil_trainer = DistillationTrainer(\n    model_init=student_init, teacher_model=teacher_model,\n    args=student_training_args, train_dataset=clinc_enc[\"train\"], \n    eval_dataset=clinc_enc[\"validation\"], compute_metrics=compute_metrics,\n    tokenizer=student_tokenizer\n)\n\ndistil_trainer.train();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# here the student matches the accuracy of the teacher despite being half the size!\n# push to hub for future use\ndistil_trainer.push_to_hub(\"Training complete\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Benchmarking our Distilled Model","metadata":{}},{"cell_type":"code","source":"# redo benchmark\ndistilled_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\npipe = pipeline(\"text-classification\", model=distilled_ckpt)\noptim_type = \"Distillation\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Accuracy surpassed teacher! Possibly because teacher has not been fine-tuned as systematically as the student. We can compress our model even further with Quantisation.","metadata":{}},{"cell_type":"markdown","source":"### Quantisation of our Model","metadata":{}},{"cell_type":"markdown","source":"Reduce precision of weights and activation eg. to 8bit instead of usual 32bit to require less memory storage; which can be done with little to no loss in accuracy. This is as once a model is trained, we only need forward pass to run inference so can reduce the precision type without impacting accuracy too much. We can control the range and precision of a fixed-point number by adjusting the scaling factor.\n\nCan map the range to a smaller one and linearly distribute the values in-between. Values outside range get clamped; when reverting, dequantisation gives the nearest fixed-point number.\n\nTransformers and DNN are good candidates for quantisation as the weights and activations take values in relatively small ranges. So we don't have to squeeze a huge range into the 256 numbers of INT8.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nstate_dict = pipe.model.state_dict()\nweights = state_dict[\"distilbert.transformer.layer.0.attention.out_lin.weight\"]\nplt.hist(weights.flatten().numpy(), bins=250, range=(-0.3, 0.3), edgecolor=\"C0\")\nplt.show();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weights are distributed in range [-0.1, 0.1] around zero. If we want to quantise as 8-bit integer, the range of vaues would be $[q_{min}, q_{max}] = [-128, 127]$. Zero points of FP32 and 8-bit coincide. Scale factor is:\n\n$f = (\\frac{f_{max}-f_{min}}{q_{max} - q_{min}})(q-Z)$","metadata":{}},{"cell_type":"code","source":"zero_point = 0\nscale = (weights.max() - weights.min()) / (127 - (-128))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To obtain quantised tensor, we need to invert mapping $q=f/S + Z$, clamp the values, round to the nearest integer and represent the result in torch.int8 data type using Tensor.char() function.","metadata":{}},{"cell_type":"code","source":"(weights / scale + zero_point).clamp(-128, 127).round().char()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So we just quantised our first tensor! In Pytorch, we can simplify with `quantize_per_tensor()` function with quantised data type `torch.qint`, optimised for integer arithmetic operations.","metadata":{}},{"cell_type":"code","source":"from torch import quantize_per_tensor\n\ndtype = torch.qint8\nquantized_weights = quantize_per_tensor(weights, scale, zero_point, dtype)\nquantized_weights.int_repr()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\nweights @ weights","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use QFunctional wrapper so we can perform operations with torch.qint8 data type\nfrom torch.nn.quantized import QFunctional\n\nq_fn = QFunctional()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%timeit\n_fn.mul(quantized_weights, quantized_weights)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Almost 100x faster! Even faster with dedicated backends for running quantised operators efficiently.\n\nAlso reduces memory storage by factor of 4! Test with example.","metadata":{}},{"cell_type":"code","source":"import sys\n\nsys.getsizeof(weights.storage()) / sys.getsizeof(quantized_weights.storage())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Trade-off: Changing precision at each layer introduces small disturbances which can compound and affect the model's performance. Three (of many) typical ways to quantise:\n- *Dynamic Quantization*: Nothing changes during training and adapt during inference. Also model's activations are quantised; happens on the fly. However, activations are written and read to memory in floating-point format, this conversion between integer and float can be a performance bottleneck.\n- *Static QUantisation*: Precompute quantisation scheme. Calculate and save ahead of time. However, requires access to good data sample to determine a good quantisation scheme. But des not address precision discrepancy leading to performance drop in metrics.\n- *Quantisation aware*: Train by rounding FP32 to mimic quantisation effect in both forward and backward pass. Improves performance in model metrics over static and dynamic quantisation.\n\nBiggest bottleneck is compute and memory bandwidth associated with enormous numbers of weights in models. So dynamic quantisation is best for transformer-based models in NLP. In smaller models, the limiting factor is memory bandwidth of the activations, so static quantisation is generally used. ","metadata":{}},{"cell_type":"code","source":"# simple to implement dynamic quantisation and can be done with a single line\n\nfrom torch.quantization import quantize_dynamic\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\nmodel = (AutoModelForSequenceClassification.from_pretrained(model_ckpt).to(\"cpu\"))\n\n# specify classes we wish to quantise. See how much int8 impacts accuracy\nmodel_quantized = quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# benchmark quantised model\npipe = pipeline(\"text-classification\", model=model_quantized, tokenizer=tokenizer)\noptim_type = \"Distillation + quantization\"\npb = PerformanceBenchmark(pipe, clinc[\"test\"], optim_type=optim_type)\nperf_metrics.update(pb.run_benchmark())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Quantised model almost half the size of distilled and even slight accuracy gain! Push to limit with framework called ONNX Runtime.\n\n## Optimizing Inference with ONNX and ONNX Runtime","metadata":{}},{"cell_type":"markdown","source":"ONNX optimises computation by converting a NN to a computation graph and can run on heavily optimised accelerators; going down a layer of abstraction for increased speed. Must convert to ONNX format for this, and can achieve with the following steps:\n1. Initialise model as pipeline\n2. Run placeholder inputs through pipeline so ONNX can record the computational graph\n3. Define dynamic axes to handle dynamic sequence lengths\n4. Save graph with network parameters","metadata":{}},{"cell_type":"code","source":"import os\nfrom psutil import cpu_count\n\n# must set some OpenMP environment variables for ONNX\nos.environ[\"OMP_NUM_THREADS\"] = f\"{cpu_count()}\"\nos.environ[\"OMP_WAIT_POLICY\"] = \"ACTIVE\" # specifies waiting threads should be acive","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"OpenMP is designed for developing highly parallelized applications.","metadata":{}},{"cell_type":"code","source":"from transformers.convert_graph_to_onnx import convert\n\nmodel_ckpt = \"transformersbook/distilbert-base-uncased-distilled-clinc\"\nonnx_model_path = Paht(\"onnx/model.onnx\")\n# wrap model in a transformers pipeline() function during conversion\n# also pass tokenizer to initialise pipeline\nconvert(framework=\"pt\", model=model_ckpt, tokenizer=tokenizer,\n       output=onnx_model_path, opset=12, pipeline_name=\"text-classification\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ONNX uses *operator sets* to group together immutable operator specifications, so opset=12 corresponds to specific version of ONNX library.","metadata":{}},{"cell_type":"code","source":"from onnxruntime import (\n    GraphOptimizationLevel, InferenceSession, SessionOptions\n)\ndef create_model_for_provider(model_path, provider=\"CPUExecutionProvider\"):\n    options = SessionOptions()\n    options.intra_op_num_threads = 1\n    options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n    # create inference session to feed inputs to model\n    session = InferenceSession(str(model_path), options, providers=[provider])\n    session.disable_fallback()\n    return session","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = clinc_enc[\"test\"][:1]\ndel inputs[\"labels\"] # requires input_ids and attention_mask as inputs; so drop labels\nlogits_onnx = onnx_model.run(None, inputs)[0] \nlogits_onnx.shape # can get class logits","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.argmax(logits_onnx)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clinc_enc[\"test\"][0][\"labels\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ONNX model is not compatible with text-classification pipeline, so create our own class that mimics the core behaviour.","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\n\nclass OnnxPipeline:\n    def __init__(self, model, tokenizer):\n        self.model = model\n        self.tokenizer = tokenizer\n        \n    def __call__(self, query):\n        model_inputs = self.tokenizer(query, return_tensors=\"pt\")\n        inputs_onnx = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n        logits = self.model.run(None, inputs_onnx)[0][0, :]\n        probs = softmax(logits)\n        pred_idx = np.argmax(probs).item()\n        return [{\"label\": intents.int2str(pred_idx), \"score\": probs[pred_idx]}]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pipe = OnnxPipeline(onnx_model, tokenizer)\npipe(query)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now create a Performance Benchmark. Override previous class' `compute_size()` because we cannot rely on `state_dict` and `torch.save()` to measure a model's size since `onnx_model` is technically an ONNX InferenceSession object that doesn't have access to attributes of PyTorch's nn.Module.","metadata":{}},{"cell_type":"code","source":"class OnnxPerformanceBenchmark(PerformanceBenchmark):\n    def __init__(self, *args, model_path, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.model_path = model_path\n        \n    def compute_size(self):\n        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n        print(f\"Model size (MB) - {size_mb:.2f}\")\n        return {\"size_mb\": size_mb}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# now see how our distilled model compares with ONNX format\noptim_type = \"Distillation + ORT\"\npb = OnnxPerformanceBenchmark(pipe, clinc[\"test\"], optim_type, model_path=\"onnx/model.onnx\")\nperf_metrics.update(pb.run_benchmark())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Has improved latency! ORT offers the three ways to quantise a model as below. We'll apply dynamic quantisation to our distilled model.\n\nIn ORT, the quantisation is applied through `quantize_dynamic()` function, which requires a path to the ONNX model to quantize, a target path to save the quantized model to, and the data type to reduce the weights to.","metadata":{}},{"cell_type":"code","source":"from onnxruntime.quantization import quantize_dynamic, QuantType\n\nmodel_input = \"onnx/model.onnx\"\nmodel_output = \"onnx/model.quant.onnx\"\nquantize_dynamic(model_input, model_output, weight_type=QuantType.QInt8)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"onnx_quantized_model = create_model_for_provider(model_output)\npipe = OnnxPipeline(onnx_quantized_model, tokenizer)\noptim_type = \"Distillation + ORT (quantized)\"\npb = OnnxPerformanceBenchmark(pipe,clinc[\"test\"], optim_type, model_path=model_output)\nperf_metrics.update(pb.run_benchmark())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_metrics(perf_metrics, optim_type)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Reduced latency by ~30% compared to PyTorch's quantization. One reason is because PyTorch only optimizes `nn.Linear` modules whereas ONNX quantises embedding layer also. Almost a 3x gain compared to BERT baseline.\n\nAnother strategy to reduce the size is remove some weights altogether, this is called *weight pruning*.","metadata":{}},{"cell_type":"markdown","source":"## Weight Pruning: Making Models Sparser","metadata":{}},{"cell_type":"markdown","source":"Basically gradually remove weight connections during training so our model becomes progressively sparser. The resulting model has a small number of nonzero parameters, which can then be stored in a compact sparse matrix format. \n\nMathematically, the way most weight pruning methods works is to calculate matrix S of *importance scores* then select top *k* percent of weights by importance.","metadata":{}},{"cell_type":"code","source":"!pip install latexify-py","metadata":{"execution":{"iopub.status.busy":"2022-09-29T15:02:33.130823Z","iopub.execute_input":"2022-09-29T15:02:33.131332Z","iopub.status.idle":"2022-09-29T15:02:46.317037Z","shell.execute_reply.started":"2022-09-29T15:02:33.131228Z","shell.execute_reply":"2022-09-29T15:02:46.315820Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting latexify-py\n  Downloading latexify_py-0.0.7-py3-none-any.whl (9.0 kB)\nRequirement already satisfied: dill>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from latexify-py) (0.3.5.1)\nInstalling collected packages: latexify-py\nSuccessfully installed latexify-py-0.0.7\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import math\nimport latexify\n\n@latexify.with_latex\ndef topk(s_ij, top_k_pct):\n    if s_ij in top_k_pct:\n        return 1\n    else:\n        return 0\n\ntopk","metadata":{"execution":{"iopub.status.busy":"2022-09-29T15:08:57.152787Z","iopub.execute_input":"2022-09-29T15:08:57.153214Z","iopub.status.idle":"2022-09-29T15:08:57.163016Z","shell.execute_reply.started":"2022-09-29T15:08:57.153181Z","shell.execute_reply":"2022-09-29T15:08:57.161757Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<latexify.core.with_latex.<locals>._LatexifiedFunction at 0x7f191479b450>","text/latex":"$$ \\displaystyle \\mathrm{topk}(s_ij, top_k_pct)\\triangleq \\left\\{ \\begin{array}{ll} 1, & \\mathrm{if} \\ \\mathrm{unknown\\_comparator}(s_ij, top_k_pct) \\\\ 0, & \\mathrm{otherwise} \\end{array} \\right. $$"},"metadata":{}}]},{"cell_type":"markdown","source":"so k is a new hyperparameter to control the amount of sparsity in the model. We can then define a *mask matrix* **M** that masks the weights $W_{ij}$ during the forward pass with some input $x_i$ and creates a sparse network of activations.\n\nConsider:\n- Which weights to eliminate\n- How to adjust remaining weights for best performance\n- How to eliminate computationally efficiently\n\n**Magnitude Pruning**: Keep most important until desired sparsity is reached. However, computationally demanding, need to train to convergence at each step. So better to gradually increase initial sparsity. We can have most pruning initially and it tapers off. Update binary masks to allow masked weights to reactivate during training and recover crom potential accuracy loss by tuning. Though only designed for pure supervised learning and can make finetuning difficult as may remove important connections.\n\n**Movement Pruning**: Gradually remove weights during fine-tuning so model becomes progressively sparser. Scores increase as weights move away from zero, so most important weights are furthest from zero.\n\nThough not supported by corrent hardware for sparse matrix operations.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}