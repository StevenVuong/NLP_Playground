{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xtOBKRW5xWbb"
   },
   "source": [
    "# ScrabbleGAN DEMO (TF 2.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7coV-TRwfts7"
   },
   "source": [
    "more information: https://github.com/Nikolai10/scrabble-gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqBDOXCrf6Xe"
   },
   "source": [
    "## Enabling and testing the GPU\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Editâ†’Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Next, we'll confirm that we can connect to the GPU with tensorflow:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FGoPsiIifz0q"
   },
   "source": [
    "## Download Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2V7M2tfS3LG"
   },
   "outputs": [],
   "source": [
    "# download project\n",
    "!git clone -b dev https://github.com/Nikolai10/scrabble-gan.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gin-config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znuBc1LYf-sx"
   },
   "source": [
    "## Download Dataset (For Demonstration Purpose Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5q7ZT5OYsgK"
   },
   "outputs": [],
   "source": [
    "# external users: manually download https://drive.google.com/file/d/1duoY9gBmx6quHNGWDlQGKIYO2ubsVZ-y/view?usp=sharing\n",
    "# place files as described in https://github.com/Nikolai10/scrabble-gan (Setup)\n",
    "!mkdir -p /content/scrabble-gan/res/data/iamDB\n",
    "!unzip /content/drive/MyDrive/data.zip -d /content/scrabble-gan/res/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkhWPioXgGnf"
   },
   "source": [
    "## Import Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2CjPJyCpVyJY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XMMjKdbWVAzP"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.extend(['./scrabble-gan'])\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import gin\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_docs.vis.embed as embed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.bigacgan.arch_ops import spectral_norm\n",
    "from src.bigacgan.data_utils import load_prepare_data, train, make_gif, load_random_word_list\n",
    "from src.bigacgan.net_architecture import make_generator, make_discriminator, make_recognizer, make_gan\n",
    "from src.bigacgan.net_loss import hinge, not_saturating\n",
    "\n",
    "gin.external_configurable(hinge)\n",
    "gin.external_configurable(not_saturating)\n",
    "gin.external_configurable(spectral_norm)\n",
    "\n",
    "from src.dinterface.dinterface import init_reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_-_JVScgJ2H"
   },
   "source": [
    "## Init Config Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loqTm1kTVXzQ"
   },
   "outputs": [],
   "source": [
    "@gin.configurable\n",
    "def setup_optimizer(g_lr, d_lr, r_lr, beta_1, beta_2, loss_fn, disc_iters):\n",
    "    generator_optimizer = tf.keras.optimizers.Adam(learning_rate=g_lr, beta_1=beta_1, beta_2=beta_2)\n",
    "    discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=d_lr, beta_1=beta_1, beta_2=beta_2)\n",
    "    recognizer_optimizer = tf.keras.optimizers.Adam(learning_rate=r_lr, beta_1=beta_1, beta_2=beta_2)\n",
    "    return generator_optimizer, discriminator_optimizer, recognizer_optimizer, loss_fn, disc_iters\n",
    "\n",
    "\n",
    "@gin.configurable('shared_specs')\n",
    "def get_shared_specs(epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention):\n",
    "    return epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention\n",
    "\n",
    "\n",
    "@gin.configurable('io')\n",
    "def setup_io(base_path, checkpoint_dir, gen_imgs_dir, model_dir, raw_dir, read_dir, input_dim, buf_size, n_classes,\n",
    "             seq_len, char_vec, bucket_size):\n",
    "    base_path='./scrabble-gan'\n",
    "    gen_path = base_path + gen_imgs_dir\n",
    "    ckpt_path = base_path + checkpoint_dir\n",
    "    m_path = base_path + model_dir\n",
    "    raw_dir = base_path + raw_dir\n",
    "    read_dir = base_path + read_dir\n",
    "    return input_dim, buf_size, n_classes, seq_len, bucket_size, ckpt_path, gen_path, m_path, raw_dir, read_dir, char_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1YZUdRCVnfZ"
   },
   "outputs": [],
   "source": [
    "# init params\n",
    "gin.parse_config_file('./scrabble-gan/src/scrabble_gan.gin')\n",
    "epochs, batch_size, latent_dim, embed_y, num_gen, kernel_reg, g_bw_attention, d_bw_attention = get_shared_specs()\n",
    "in_dim, buf_size, n_classes, seq_len, bucket_size, ckpt_path, gen_path, m_path, raw_dir, read_dir, char_vec = setup_io()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrEfyRihgNk-"
   },
   "source": [
    "## Load and Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VsboZhAIWYBz"
   },
   "outputs": [],
   "source": [
    "# convert IAM Handwriting dataset (words) to GAN format\n",
    "if not os.path.exists(read_dir):\n",
    "      print('converting iamDB-Dataset to GAN format...')\n",
    "      init_reading(raw_dir, read_dir, in_dim, bucket_size)\n",
    "\n",
    "# load random words into memory (used for word generation by G)\n",
    "random_words = load_random_word_list(read_dir, bucket_size, char_vec)\n",
    "\n",
    "# load and preprocess dataset (python generator)\n",
    "train_dataset = load_prepare_data(in_dim, batch_size, read_dir, char_vec, bucket_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim, buf_size, n_classes, seq_len, bucket_size, ckpt_path, gen_path, m_path, raw_dir, read_dir, char_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_files = 0\n",
    "for i in range(1, 11):\n",
    "    img_dir = os.path.join(read_dir, str(i))\n",
    "    img_files = os.listdir(img_dir)\n",
    "    \n",
    "    for img_file in img_files:\n",
    "        if img_file.endswith('.txt'):\n",
    "            num_files+=1\n",
    "print(num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for img_dir in os.listdir(read_dir):\n",
    "    print(img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-8KMEzLgQCb"
   },
   "source": [
    "## Build Composite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9-nGgp-VsOv",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# init generator, discriminator and recognizer\n",
    "generator = make_generator(latent_dim, in_dim, embed_y, gen_path, kernel_reg, g_bw_attention, n_classes)\n",
    "discriminator = make_discriminator(gen_path, in_dim, kernel_reg, d_bw_attention)\n",
    "recognizer = make_recognizer(in_dim, seq_len, n_classes + 1, gen_path)\n",
    "\n",
    "# build composite model (update G through composite model)\n",
    "gan = make_gan(generator, discriminator, recognizer, gen_path)\n",
    "\n",
    "# init optimizer for both generator, discriminator and recognizer\n",
    "generator_optimizer, discriminator_optimizer, recognizer_optimizer, loss_fn, disc_iters = setup_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJ08-2b6gUgu"
   },
   "source": [
    "## Define Optimizers + Checkpoint-Saver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BroxWH3ZV-5y"
   },
   "outputs": [],
   "source": [
    "# purpose: save and restore models\n",
    "checkpoint_prefix = os.path.join(ckpt_path, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                     discriminator_optimizer=discriminator_optimizer,\n",
    "                                     recognizer_optimizer=recognizer_optimizer,\n",
    "                                     generator=generator,\n",
    "                                     discriminator=discriminator,\n",
    "                                     recognizer=recognizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0coxgiyge4C"
   },
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kgn4kkPZaKbc"
   },
   "source": [
    "**note:** If you use the [free Colab version](https://colab.research.google.com/signup), you should first reduce the number of epochs (e.g. epochs=5) to not exceed the 12h time limit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ST8zVGl_WLGi"
   },
   "outputs": [],
   "source": [
    "# reuse this seed + labels overtime to visualize progress in the animated GIF\n",
    "seed = tf.random.normal([num_gen, latent_dim])\n",
    "random_bucket_idx = random.randint(4, bucket_size - 1)\n",
    "labels = np.array([random.choice(random_words[random_bucket_idx]) for _ in range(num_gen)], np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_bucket_idx, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I2jzbQxuWQKd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=15\n",
    "# start training\n",
    "train(train_dataset, generator, discriminator, recognizer, gan, checkpoint, checkpoint_prefix, generator_optimizer,\n",
    "          discriminator_optimizer, recognizer_optimizer, [seed, labels], buf_size, batch_size, epochs, m_path,\n",
    "          latent_dim, gen_path, loss_fn, disc_iters, random_words, bucket_size, char_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9v_53fDrCrJJ"
   },
   "outputs": [],
   "source": [
    "# use imageio to create an animated gif using the images saved during training.\n",
    "make_gif(gen_path)\n",
    "embed.embed_file(gen_path + 'biggan.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtZAMYUzlMR7"
   },
   "source": [
    "## Run Inference On Your Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXzp579rlOe5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_to_saved_model = './scrabble-gan/res/out/big_ac_gan/model/generator_' + str(epochs)\n",
    "\n",
    "# number of samples to generate\n",
    "n_samples = 10\n",
    "# your sample string\n",
    "sample_string = 'April'\n",
    "\n",
    "\n",
    "# load trained model\n",
    "imported_model = tf.saved_model.load(path_to_saved_model)\n",
    "\n",
    "# inference loop\n",
    "for idx in range(1):\n",
    "    fake_labels = []\n",
    "    words = [sample_string] * 10\n",
    "    noise = tf.random.normal([n_samples, latent_dim])\n",
    "\n",
    "    # encode words\n",
    "    for word in words:\n",
    "        fake_labels.append([char_vec.index(char) for char in word])\n",
    "    fake_labels = np.array(fake_labels, np.int32)\n",
    "\n",
    "    # run inference process\n",
    "    predictions = imported_model([noise, fake_labels], training=False)\n",
    "    # transform values into range [0, 1]\n",
    "    predictions = (predictions + 1) / 2.0\n",
    "\n",
    "    # plot results\n",
    "    for i in range(predictions.shape[0]):\n",
    "        plt.subplot(10, 1, i + 1)\n",
    "        plt.imshow(predictions[i, :, :, 0], cmap='gray')\n",
    "        # plt.text(0, -1, \"\".join([char_vec[label] for label in fake_labels[i]]))\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGyxKq6aXEg0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ScrabbleGAN_DEMO.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-7.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-7:m87"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
