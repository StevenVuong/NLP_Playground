{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow BERT Classification FineTune.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[Original Post](https://medium.com/swlh/fine-tuning-bert-for-text-classification-and-question-answering-using-tensorflow-framework-4d09daeb3330#id_token=eyJhbGciOiJSUzI1NiIsImtpZCI6IjE4MmU0NTBhMzVhMjA4MWZhYTFkOWFlMWQyZDc1YTBmMjNkOTFkZjgiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJuYmYiOjE2NDQyOTkwOTAsImF1ZCI6IjIxNjI5NjAzNTgzNC1rMWs2cWUwNjBzMnRwMmEyamFtNGxqZGNtczAwc3R0Zy5hcHBzLmdvb2dsZXVzZXJjb250ZW50LmNvbSIsInN1YiI6IjExMjk2MzczMjI5MjcwMzU1ODc4OCIsImVtYWlsIjoic3RldmVudnVvbmc5NkBnbWFpbC5jb20iLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXpwIjoiMjE2Mjk2MDM1ODM0LWsxazZxZTA2MHMydHAyYTJqYW00bGpkY21zMDBzdHRnLmFwcHMuZ29vZ2xldXNlcmNvbnRlbnQuY29tIiwibmFtZSI6IlN0ZXZlbiBWdW9uZyIsInBpY3R1cmUiOiJodHRwczovL2xoMy5nb29nbGV1c2VyY29udGVudC5jb20vYS9BQVRYQUp3N0phNE5VU1hvOW9lMkpiRlp1cENMdkxsam8yRmFKRUxTOTlNWT1zOTYtYyIsImdpdmVuX25hbWUiOiJTdGV2ZW4iLCJmYW1pbHlfbmFtZSI6IlZ1b25nIiwiaWF0IjoxNjQ0Mjk5MzkwLCJleHAiOjE2NDQzMDI5OTAsImp0aSI6ImI3ODRmMDJkZmM5ZTNhMmRhZGU4YTNhYWQ2NGUwYjJkMzNjYzIwMmYifQ.hAbZE_wmOwt8TTBPQk-8lH1Ll6BVtLOmKo_QQHZ2Dg2Ha0vjzoYyfEFO-O0Pco57G8Exz_QViLvl8OkEkBHNo-3sGp2NhYxsxQW_zeACtv3Z063vve2AGw_zX2kG7TNYL4vS5kQ8QrLjgpf6sT4gR96f-07aC-yqC-KdDJgGwe-iKcR7ZrwGIpUNXLrxKAahp2jo7jRXQJPvKpqEFwmLCJHYMhj_imWmupR2zG9Y2j69O26s9ipRWa0w7_miMPIM2gqbScICGbpj3lai1A619xkSnjgIoM_mUhEZeCEupH8Bk_ZFBjzbWGfNjrWHVQyC9ASzDSQMjR8J7ZzJao9EOg)"
      ],
      "metadata": {
        "id": "lqRTEZyM3d9u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-models-official"
      ],
      "metadata": {
        "id": "cDLWvbW45XfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import official.nlp.optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_hub as hub\n",
        "from official import nlp\n",
        "from official.nlp.bert import tokenization"
      ],
      "metadata": {
        "id": "XzlfZ5wF920j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "jMuwSv5zE6w0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "IxGfHDiPFBgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Defining functions to preprocess our data\n",
        "\n",
        "def encode_sentence(sentence, tokenizer):\n",
        "\n",
        "    # tokenise sentence\n",
        "    tokens = list(tokenizer.tokenize(sentence))\n",
        "\n",
        "    # each sentence should end with a [SEP] (separator) token\n",
        "    tokens.append('[SEP]')\n",
        "\n",
        "    # then convert tokens to ids\n",
        "    return tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "\n",
        "def bert_encode(glue_dict, tokenizer, max_seq_len):\n",
        "\n",
        "    # start by encoding all the sentences and packing them into ragged-tensors\n",
        "    # ragged tensor can be created from nested python list\n",
        "    sentence1 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence1\"])])\n",
        "    sentence2 = tf.ragged.constant([encode_sentence(s, tokenizer) for s in np.array(glue_dict[\"sentence2\"])])\n",
        "\n",
        "    # now prepend a [CLS] token, of length of num sentences and concatenate the ragged tensors \n",
        "    # to form a single input_word_ids tensor for each example. Remember each starts with CLS\n",
        "    cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n",
        "    input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "\n",
        "    # the mask allows the model to cleanly differentiate between the content and the padding\n",
        "    # it has the same shape as the input_word_ids, and contains a 1 anywhere the input_word_ids \n",
        "    input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "    type_cls = tf.zeros_like(cls)\n",
        "    type_s1 = tf.zeros_like(sentence1)\n",
        "    type_s2 = tf.ones_like(sentence2)\n",
        "\n",
        "    # input_type_ids tensor also has the same shape, but inside the non-padded region \n",
        "    # it contains a 0 or a 1 indicating which sentence the token is a part of\n",
        "    input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "\n",
        "    # zero pad all\n",
        "    input_word_ids = tf.keras.preprocessing.sequence.pad_sequences(input_word_ids.to_tensor(), maxlen=max_seq_len, padding='post',truncating='post')\n",
        "    input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=max_seq_len, padding='post',truncating='post')\n",
        "    input_type_ids = tf.keras.preprocessing.sequence.pad_sequences(input_type_ids, maxlen=max_seq_len, padding='post',truncating='post')\n",
        "\n",
        "    # convert to tensor and return\n",
        "    inputs = {\n",
        "        'input_word_ids': tf.convert_to_tensor(input_word_ids),\n",
        "        'input_mask': tf.convert_to_tensor(input_mask),\n",
        "        'input_type_ids': tf.convert_to_tensor(input_type_ids)\n",
        "        }\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "hbygncg83a4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialising our model\n",
        "\n",
        "# define max sequence length\n",
        "max_seq_length = 128\n",
        "\n",
        "# initialise input word ids, mask and type ids\n",
        "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
        "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
        "input_type_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name='input_type_ids')\n",
        "\n",
        "# load bert layer and pass inputs to get the pooled layer\n",
        "bert_inputs = {'input_word_ids': input_word_ids, 'input_mask': input_mask, 'input_type_ids': input_type_ids}\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\", trainable=True)\n",
        "pooled_output, _ = bert_layer([input_word_ids, input_mask, input_type_ids])\n",
        "\n",
        "# add dropout layer; and then output Dense layer with Truncated Norm Initializer\n",
        "output = tf.keras.layers.Dropout(rate=0.2)(pooled_output)\n",
        "initializer = tf.keras.initializers.TruncatedNormal(stddev=0.02)\n",
        "bert_output = tf.keras.layers.Dense(2, kernel_initializer=initializer, name='output')(output)\n",
        "\n",
        "# create model; set input expectation (dict) and output \n",
        "model = tf.keras.models.Model(inputs=bert_inputs, outputs=bert_output)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "GHwCJgQc3uwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy() # vocabulary file?\n",
        "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy() # if lowercase\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case) # full tokenizer.. From VocabFile and lowercase (true)\n"
      ],
      "metadata": {
        "id": "Mr-1ZAwK7Yld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download dataset and apply bert encoding with our tokenizer\n",
        "glue, info = tfds.load('glue/mrpc', with_info=True, batch_size=-1)\n",
        "glue_train = bert_encode(glue['train'], tokenizer, max_seq_length)\n",
        "glue_train_labels = glue['train']['label']\n",
        "\n",
        "glue_validation = bert_encode(glue['validation'], tokenizer, max_seq_length)\n",
        "glue_validation_labels = glue['validation']['label']"
      ],
      "metadata": {
        "id": "Ve3w9cni6Nxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "## Sample breakdown of 'bert_encode' function; see step by step\n",
        "sample_idx = 2\n",
        "\n",
        "# load some sample sentences\n",
        "sample_sentence_1 = glue['train']['sentence1'][sample_idx]\n",
        "sample_sentence_2 = glue['train']['sentence2'][sample_idx]\n",
        "sample_label = glue['train']['label'][sample_idx]\n",
        "\n",
        "print('Sample Index:', glue['train']['idx'][sample_idx], '\\n', \n",
        "      'Sample Label:', sample_label,'\\n', \n",
        "      'Train Example:',sample_sentence_1, '\\n',\n",
        "      'Test Example:',sample_sentence_2, '\\n'\n",
        "      )\n",
        "\n",
        "# split sample into individual tokens\n",
        "sample_tokneized = list(tokenizer.tokenize(sample_sentence_1.numpy()))\n",
        "print('Sample after tokenization', sample_tokneized[:5])\n",
        "\n",
        "sample_tokneized.append('[SEP]')\n",
        "print('Converting SEP to ID', tokenizer.convert_tokens_to_ids(['[SEP]']))\n",
        "print('End of tokenized:', sample_tokneized[-5:])\n",
        "\n",
        "# convert to ids\n",
        "sample_tokenized_ids = tokenizer.convert_tokens_to_ids(sample_tokneized)\n",
        "print('Tokenized and turn to ids', sample_tokenized_ids[:5])\n",
        "\n",
        "# do for sent1 and 2; turn into ragged tensors of ids\n",
        "sentence1 = tf.ragged.constant([encode_sentence(sample_sentence_1.numpy(), tokenizer)])\n",
        "sentence2 = tf.ragged.constant([encode_sentence(sample_sentence_2.numpy(), tokenizer)])\n",
        "print('\\n', 'Ragged Tensor of sentence 2', sentence2)\n",
        "\n",
        "# tokenize CLS then have as many as there are sentences (to split sentences)\n",
        "cls = [tokenizer.convert_tokens_to_ids(['[CLS]'])] * sentence1.shape[0]\n",
        "print('\\n', 'CLS token ID', cls)\n",
        "\n",
        "# create input tensor; can see we start with CLS token, then two SEP; one inbetween and one at end\n",
        "input_word_ids = tf.concat([cls, sentence1, sentence2], axis=-1)\n",
        "print('\\n', 'Ragged tensior of word 1 and 2', input_word_ids)\n",
        "\n",
        "# Create input mask, cls, s1 and s2 tensors.\n",
        "input_mask = tf.ones_like(input_word_ids).to_tensor()\n",
        "\n",
        "# create input type id's, concat all of the following\n",
        "type_cls = tf.zeros_like(cls) # shape (n, 1) after CLS\n",
        "type_s1 = tf.zeros_like(sentence1) # shape of (n, sentences1); label as 0's (input types)\n",
        "type_s2 = tf.ones_like(sentence2) # shape of (n, sentences2); label this as 1's (input types)\n",
        "input_type_ids = tf.concat([type_cls, type_s1, type_s2], axis=-1).to_tensor()\n",
        "print('\\n Input Type IDs: \\n', input_type_ids) # shape of sentence 1\n",
        "\n",
        "# pad all\n",
        "input_word_ids = tf.keras.preprocessing.sequence.pad_sequences(input_word_ids.to_tensor(), maxlen=128, padding='post',truncating='post')\n",
        "input_mask = tf.keras.preprocessing.sequence.pad_sequences(input_mask, maxlen=128, padding='post',truncating='post')\n",
        "input_type_ids = tf.keras.preprocessing.sequence.pad_sequences(input_type_ids, maxlen=128, padding='post',truncating='post')\n",
        "\n",
        "# create inputs dict\n",
        "inputs = {\n",
        "    'input_word_ids': input_word_ids,\n",
        "    'input_mask': input_mask,\n",
        "    'input_type_ids': input_type_ids\n",
        "    }\n",
        "print('\\n BERT Inputs', inputs)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "AWHDZT3e6YOW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Compile model\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "# define sizes\n",
        "batch_size = 32\n",
        "eval_batch_size = 32\n",
        "train_data_size = len(glue_train_labels)\n",
        "\n",
        "# define steps; include warmup\n",
        "steps_per_epoch = int(train_data_size / batch_size)\n",
        "num_train_steps = steps_per_epoch * epochs\n",
        "warmup_steps = int(epochs * train_data_size * 0.1 / batch_size)\n",
        "\n",
        "# define optimizer; metrics and loss. Sparse Categorical CrossEntropy\n",
        "optimizer = nlp.optimization.create_optimizer(2e-5, num_train_steps=num_train_steps, num_warmup_steps=warmup_steps)\n",
        "metrics = [tf.keras.metrics.SparseCategoricalAccuracy('accuracy', dtype=tf.float32)]\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
        "\n",
        "model.fit(\n",
        "    glue_train, glue_train_labels,\n",
        "    validation_data= (glue_validation, glue_validation_labels),\n",
        "    batch_size=batch_size,\n",
        "    validation_batch_size=eval_batch_size,\n",
        "    epochs=epochs\n",
        "    )"
      ],
      "metadata": {
        "id": "Pea-9WXP7koB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights\n",
        "model.save_weights(\"./weights.h5\")\n",
        "# build some examples with our tokenizer\n",
        "my_examples = bert_encode(\n",
        "    glue_dict={\n",
        "        'sentence1': [\n",
        "            'The rain in Spain falls mainly on the plain.',\n",
        "            'Look I fine tuned BERT.'],\n",
        "        'sentence2': [\n",
        "            'It mostly rains on the flat lands of Spain.',\n",
        "            'Is it working? This does not match.']\n",
        "    },\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_len=max_seq_length\n",
        "    )\n",
        "\n",
        "# get the result\n",
        "result = model.predict(my_examples)\n",
        "print(result)\n",
        "\n",
        "result = tf.argmax(result).numpy()\n",
        "print(result)\n",
        "print(np.array(info.features['label'].names)[result])"
      ],
      "metadata": {
        "id": "gwj-rioxwEi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info"
      ],
      "metadata": {
        "id": "HkmhFAdF6DMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SyHp4mLeDkJd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}